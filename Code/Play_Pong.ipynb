{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-25 22:27:54,254] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize OpenAI for Pong\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# Hyper parameters for policy gradient model\n",
    "global num_actions, discount\n",
    "num_actions = 3 \n",
    "discount = 0.99\n",
    "\n",
    "# Hyper parameters for Network\n",
    "FC_param = {'FC_1':200}\n",
    "\n",
    "# Initialize parameter for policy gradient model \n",
    "observation = env.reset()\n",
    "image_old  = None\n",
    "images, fake_labels, rewards_std, action_hist, reward_hist, reward_runn = [], [], [], [], [], []\n",
    "reward_episode = 0\n",
    "\n",
    "# Build network\n",
    "def Network(image_, FC_param):\n",
    "    \n",
    "    FC_tmp = tf.contrib.layers.flatten(image_, scope='Flatten')\n",
    "    \n",
    "    for key,value in FC_param.items():\n",
    "        FC_tmp = tf.layers.dense(inputs=FC_tmp, \n",
    "                                 units=value,\n",
    "                                 kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                     stddev=1./np.sqrt(5000), \n",
    "                                                                                     dtype=tf.float32),\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 use_bias=False,\n",
    "                                 name=key)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=FC_tmp, \n",
    "                             units=num_actions, \n",
    "                             kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                 stddev=1./np.sqrt(500), \n",
    "                                                                                 dtype=tf.float32),\n",
    "                             use_bias=False,\n",
    "                             name='Logits')\n",
    "    \n",
    "    action_probs = tf.nn.softmax(logits, name='SoftMax')\n",
    "    \n",
    "    return action_probs\n",
    "\n",
    "# Get discounted reward and normalize it\n",
    "def discount_norm(rew):\n",
    "    rew_func = lambda a, v: a*discount + v # Reward function\n",
    "    \n",
    "    rew_reverse = tf.scan(rew_func, tf.reverse(rew,[True, False]))\n",
    "    discounted_rew = tf.reverse(rew_reverse,[True, False])\n",
    "    \n",
    "    mean, variance= tf.nn.moments(discounted_rew, [0])\n",
    "    discounted_rew -= mean\n",
    "    discounted_rew /= tf.sqrt(variance + 1e-6)\n",
    "    \n",
    "    return discounted_rew\n",
    "\n",
    "# Process image by cropping and binarizing\n",
    "def process_obs(obs):\n",
    "    obs = obs[35:195]\n",
    "    obs = obs[::2,::2,0]\n",
    "    obs[obs == 144] = 0\n",
    "    obs[obs == 109] = 0\n",
    "    obs[obs != 0] = 1 \n",
    "    return obs.astype(np.float)\n",
    "\n",
    "# Build model\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        image_ = tf.placeholder(dtype=tf.float32, shape=[None, 80,80,1],name=\"image\")\n",
    "        fake_label_ = tf.placeholder(dtype=tf.float32, shape=[None, num_actions],name=\"fake_label\")\n",
    "        reward_ = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"reward\")\n",
    "\n",
    "        discounted_epr = discount_norm(reward_) # Get policy gradient\n",
    "\n",
    "        tf_aprob = Network(image_,FC_param)\n",
    "\n",
    "# Main program\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    episode_number = pickle.load(open('PG-Pong-ckpt-1/last-episode.p','rb'))\n",
    "    saver.restore(sess, 'PG-Pong-ckpt-1\\\\Pong_PG.ckpt-'+str(episode_number))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        image_proc = process_obs(observation)\n",
    "        image_diff = image_proc - image_old if image_old is not None else np.zeros((80,80))\n",
    "\n",
    "        image_old = image_proc\n",
    "\n",
    "        # Uniformly pick an action\n",
    "        feed_step = {image_: np.reshape(image_diff, (1,80,80,1))}\n",
    "        aprob = sess.run(tf_aprob,feed_step) ; aprob = aprob[0,:]\n",
    "        action = np.random.choice(num_actions, p=aprob)\n",
    "        label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "        # Input action to OpenAI and get feedback\n",
    "        observation, reward, done, info = env.step(action+1)\n",
    "        reward_episode += reward\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
