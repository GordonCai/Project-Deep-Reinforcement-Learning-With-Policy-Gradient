{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-18 11:18:48,894] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        #with tf.variable_scope(\"state_processor\"), tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\"):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "       \n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 84, 84 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        \n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        \n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        # input 84x84x4, 32 filters, 8x8 kernel_size, stride = 4\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # 2nd: 64 filters, 4x4 kernel_size, stride = 2  \n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        # 3rd: 64 filters, 3x3 kernel_size, stride = 1\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        \n",
    "        # fc: 512 rectifier units \n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        loss = sess.run([self.loss], feed_dict)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    jList = []\n",
    "    rList = []\n",
    "    total_t = 0\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    #checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    #checkpoint_path = os.path.join(experiment_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "#     latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "#     if latest_checkpoint:\n",
    "#         print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "#         saver.restore(sess, latest_checkpoint)\n",
    "    print(\"Import saved model\")\n",
    "    new_saver = tf.train.import_meta_graph('model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Get the current time step\n",
    "    #total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "    #for i in range(300):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    \n",
    "    #env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    " \n",
    "    \n",
    "    for i_episode in range(1):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        #saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        saver.save(tf.get_default_session(), 'model')\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "    \n",
    "        r_episode = 0\n",
    "        j_episode = 0\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "          \n",
    "            # Add epsilon to Tensorboard\n",
    "            #episode_summary = tf.Summary()\n",
    "            #episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            #q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            r_episode += reward\n",
    "            print('reward:', r_episode)\n",
    "            j_episode += t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "           \n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            #tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "            #print()\n",
    "\n",
    "            #print('reward', reward_batch)\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            \n",
    "            if done:\n",
    "                targets_batch = reward_batch \n",
    "            else: \n",
    "                targets_batch = reward_batch + discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            #print('state batch:', len(states_batch))\n",
    "            states_batch = np.array(states_batch)\n",
    "            #print('np.array:', len(states_batch))\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                print('done:', done)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        #episode_summary = tf.Summary()\n",
    "        #episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        #episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        #q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        #q_estimator.summary_writer.flush()\n",
    "        jList.append(j_episode)\n",
    "        rList.append(r_episode)\n",
    "        print(\"\\nEpisode Reward: {}\".format(r_episode))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes=10000\n",
    "replay_memory_size=500000\n",
    "replay_memory_init_size=50000\n",
    "update_target_estimator_every=10000\n",
    "epsilon_start=1.0\n",
    "epsilon_end=0.1\n",
    "epsilon_decay_steps=500000\n",
    "discount_factor=0.99\n",
    "batch_size=32\n",
    "record_video_every=1\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dqn/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import saved model\n",
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (0) @ Episode 1/10000, loss: Nonereward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 1 (1) @ Episode 1/10000, loss: [0.0071780412]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 2 (2) @ Episode 1/10000, loss: [0.04476155]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 3 (3) @ Episode 1/10000, loss: [0.0064100055]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 4 (4) @ Episode 1/10000, loss: [0.0077073202]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 5 (5) @ Episode 1/10000, loss: [0.04544811]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 6 (6) @ Episode 1/10000, loss: [0.0077916943]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 7 (7) @ Episode 1/10000, loss: [0.007124166]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 8 (8) @ Episode 1/10000, loss: [0.0085838828]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 9 (9) @ Episode 1/10000, loss: [0.0076811351]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 10 (10) @ Episode 1/10000, loss: [0.0089405514]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 11 (11) @ Episode 1/10000, loss: [0.0086020436]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 12 (12) @ Episode 1/10000, loss: [0.0080145206]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 13 (13) @ Episode 1/10000, loss: [0.0092973765]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 14 (14) @ Episode 1/10000, loss: [0.0086899661]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 15 (15) @ Episode 1/10000, loss: [0.0063114767]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 16 (16) @ Episode 1/10000, loss: [0.045138959]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 17 (17) @ Episode 1/10000, loss: [0.045540135]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 18 (18) @ Episode 1/10000, loss: [0.0081588533]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 19 (19) @ Episode 1/10000, loss: [0.039473556]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 20 (20) @ Episode 1/10000, loss: [0.0078904331]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 21 (21) @ Episode 1/10000, loss: [0.008553708]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 22 (22) @ Episode 1/10000, loss: [0.0085021276]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 23 (23) @ Episode 1/10000, loss: [0.0080490261]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 24 (24) @ Episode 1/10000, loss: [0.0069364477]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 25 (25) @ Episode 1/10000, loss: [0.0070888]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 26 (26) @ Episode 1/10000, loss: [0.0091542341]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 27 (27) @ Episode 1/10000, loss: [0.00829532]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 28 (28) @ Episode 1/10000, loss: [0.0083082076]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 29 (29) @ Episode 1/10000, loss: [0.0075395983]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 30 (30) @ Episode 1/10000, loss: [0.007739719]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 31 (31) @ Episode 1/10000, loss: [0.0067168251]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 32 (32) @ Episode 1/10000, loss: [0.0074414769]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 33 (33) @ Episode 1/10000, loss: [0.0087780813]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 34 (34) @ Episode 1/10000, loss: [0.0078054839]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 35 (35) @ Episode 1/10000, loss: [0.007388073]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 36 (36) @ Episode 1/10000, loss: [0.0081991293]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 37 (37) @ Episode 1/10000, loss: [0.0075513213]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 38 (38) @ Episode 1/10000, loss: [0.007260459]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 39 (39) @ Episode 1/10000, loss: [0.0062624216]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 40 (40) @ Episode 1/10000, loss: [0.0084397746]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 41 (41) @ Episode 1/10000, loss: [0.0083070938]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 42 (42) @ Episode 1/10000, loss: [0.008417679]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 43 (43) @ Episode 1/10000, loss: [0.0066743749]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 44 (44) @ Episode 1/10000, loss: [0.006593674]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 45 (45) @ Episode 1/10000, loss: [0.0075134262]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 46 (46) @ Episode 1/10000, loss: [0.045905553]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 47 (47) @ Episode 1/10000, loss: [0.0057675699]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 48 (48) @ Episode 1/10000, loss: [0.0065091276]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 49 (49) @ Episode 1/10000, loss: [0.047402781]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 50 (50) @ Episode 1/10000, loss: [0.0077416203]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 51 (51) @ Episode 1/10000, loss: [0.0085295457]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 52 (52) @ Episode 1/10000, loss: [0.008944653]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 53 (53) @ Episode 1/10000, loss: [0.0073968507]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 54 (54) @ Episode 1/10000, loss: [0.0074981698]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 55 (55) @ Episode 1/10000, loss: [0.0076608676]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 56 (56) @ Episode 1/10000, loss: [0.0088260313]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 57 (57) @ Episode 1/10000, loss: [0.008015573]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 58 (58) @ Episode 1/10000, loss: [0.0082266126]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 59 (59) @ Episode 1/10000, loss: [0.0088194739]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 60 (60) @ Episode 1/10000, loss: [0.0074771969]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 61 (61) @ Episode 1/10000, loss: [0.0073428396]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 62 (62) @ Episode 1/10000, loss: [0.0073264418]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 63 (63) @ Episode 1/10000, loss: [0.045665592]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 64 (64) @ Episode 1/10000, loss: [0.046956774]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 65 (65) @ Episode 1/10000, loss: [0.0082027707]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 66 (66) @ Episode 1/10000, loss: [0.045580298]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 67 (67) @ Episode 1/10000, loss: [0.039707892]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 68 (68) @ Episode 1/10000, loss: [0.044624627]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 69 (69) @ Episode 1/10000, loss: [0.045404434]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 70 (70) @ Episode 1/10000, loss: [0.0064989049]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 71 (71) @ Episode 1/10000, loss: [0.0083416607]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 72 (72) @ Episode 1/10000, loss: [0.04546449]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 73 (73) @ Episode 1/10000, loss: [0.0073802201]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 74 (74) @ Episode 1/10000, loss: [0.0084495433]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 75 (75) @ Episode 1/10000, loss: [0.0083213793]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 76 (76) @ Episode 1/10000, loss: [0.0076735923]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 77 (77) @ Episode 1/10000, loss: [0.0076841945]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 78 (78) @ Episode 1/10000, loss: [0.0082252035]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 79 (79) @ Episode 1/10000, loss: [0.0070380475]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 80 (80) @ Episode 1/10000, loss: [0.0087852143]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 81 (81) @ Episode 1/10000, loss: [0.0079255216]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 82 (82) @ Episode 1/10000, loss: [0.008344519]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 83 (83) @ Episode 1/10000, loss: [0.0077333977]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 84 (84) @ Episode 1/10000, loss: [0.0070884367]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 85 (85) @ Episode 1/10000, loss: [0.007985835]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 86 (86) @ Episode 1/10000, loss: [0.0088227624]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 87 (87) @ Episode 1/10000, loss: [0.0082316492]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 88 (88) @ Episode 1/10000, loss: [0.0073906323]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 89 (89) @ Episode 1/10000, loss: [0.0082931491]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 90 (90) @ Episode 1/10000, loss: [0.0071019758]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 91 (91) @ Episode 1/10000, loss: [0.0070080282]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 92 (92) @ Episode 1/10000, loss: [0.0080636833]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 93 (93) @ Episode 1/10000, loss: [0.0076853093]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 94 (94) @ Episode 1/10000, loss: [0.0088632926]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 95 (95) @ Episode 1/10000, loss: [0.0081427079]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 96 (96) @ Episode 1/10000, loss: [0.045145348]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 97 (97) @ Episode 1/10000, loss: [0.0078383526]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 98 (98) @ Episode 1/10000, loss: [0.0077060983]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 99 (99) @ Episode 1/10000, loss: [0.0074537122]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 100 (100) @ Episode 1/10000, loss: [0.0087949019]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 101 (101) @ Episode 1/10000, loss: [0.006563643]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 102 (102) @ Episode 1/10000, loss: [0.0080184452]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 103 (103) @ Episode 1/10000, loss: [0.045686543]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 104 (104) @ Episode 1/10000, loss: [0.045233868]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 105 (105) @ Episode 1/10000, loss: [0.046085283]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 106 (106) @ Episode 1/10000, loss: [0.038162928]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 107 (107) @ Episode 1/10000, loss: [0.0068652676]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 108 (108) @ Episode 1/10000, loss: [0.0079214312]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 109 (109) @ Episode 1/10000, loss: [0.0067508556]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 110 (110) @ Episode 1/10000, loss: [0.0078390269]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 111 (111) @ Episode 1/10000, loss: [0.008663401]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 112 (112) @ Episode 1/10000, loss: [0.008572096]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 113 (113) @ Episode 1/10000, loss: [0.0080001382]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 114 (114) @ Episode 1/10000, loss: [0.038449485]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 115 (115) @ Episode 1/10000, loss: [0.0074248295]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 116 (116) @ Episode 1/10000, loss: [0.0066920193]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 117 (117) @ Episode 1/10000, loss: [0.0065862429]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 118 (118) @ Episode 1/10000, loss: [0.0078309681]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 119 (119) @ Episode 1/10000, loss: [0.0096690748]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 120 (120) @ Episode 1/10000, loss: [0.0077575608]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 121 (121) @ Episode 1/10000, loss: [0.0081848912]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 122 (122) @ Episode 1/10000, loss: [0.0077228234]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 123 (123) @ Episode 1/10000, loss: [0.0098294793]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 124 (124) @ Episode 1/10000, loss: [0.0073516238]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 125 (125) @ Episode 1/10000, loss: [0.0078851189]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 126 (126) @ Episode 1/10000, loss: [0.045525823]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 127 (127) @ Episode 1/10000, loss: [0.0069529675]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 128 (128) @ Episode 1/10000, loss: [0.0090725757]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 129 (129) @ Episode 1/10000, loss: [0.007813585]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 130 (130) @ Episode 1/10000, loss: [0.0098946728]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 131 (131) @ Episode 1/10000, loss: [0.0077586193]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 132 (132) @ Episode 1/10000, loss: [0.0069025187]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 133 (133) @ Episode 1/10000, loss: [0.0071299803]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 134 (134) @ Episode 1/10000, loss: [0.0081280814]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 135 (135) @ Episode 1/10000, loss: [0.081870347]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 136 (136) @ Episode 1/10000, loss: [0.008859803]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 137 (137) @ Episode 1/10000, loss: [0.037060488]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 138 (138) @ Episode 1/10000, loss: [0.0087745143]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 139 (139) @ Episode 1/10000, loss: [0.0072721872]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 140 (140) @ Episode 1/10000, loss: [0.0087577961]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 141 (141) @ Episode 1/10000, loss: [0.0075894343]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 142 (142) @ Episode 1/10000, loss: [0.0079145627]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 143 (143) @ Episode 1/10000, loss: [0.038132641]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 144 (144) @ Episode 1/10000, loss: [0.007893011]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 145 (145) @ Episode 1/10000, loss: [0.0068001319]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 146 (146) @ Episode 1/10000, loss: [0.0069552572]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 147 (147) @ Episode 1/10000, loss: [0.0069291526]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 148 (148) @ Episode 1/10000, loss: [0.0076220664]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 149 (149) @ Episode 1/10000, loss: [0.0063024652]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 150 (150) @ Episode 1/10000, loss: [0.0091128815]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 151 (151) @ Episode 1/10000, loss: [0.0078942617]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 152 (152) @ Episode 1/10000, loss: [0.007949207]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 153 (153) @ Episode 1/10000, loss: [0.0088524325]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 154 (154) @ Episode 1/10000, loss: [0.0090641174]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 155 (155) @ Episode 1/10000, loss: [0.0071035968]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 156 (156) @ Episode 1/10000, loss: [0.0071641114]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 157 (157) @ Episode 1/10000, loss: [0.0081180539]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 158 (158) @ Episode 1/10000, loss: [0.0066174646]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 159 (159) @ Episode 1/10000, loss: [0.008324774]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 160 (160) @ Episode 1/10000, loss: [0.0073007066]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 161 (161) @ Episode 1/10000, loss: [0.045231722]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 162 (162) @ Episode 1/10000, loss: [0.045816187]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 163 (163) @ Episode 1/10000, loss: [0.007706319]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 164 (164) @ Episode 1/10000, loss: [0.0091768317]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 165 (165) @ Episode 1/10000, loss: [0.0091894493]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 166 (166) @ Episode 1/10000, loss: [0.0060956832]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 167 (167) @ Episode 1/10000, loss: [0.0065928465]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 168 (168) @ Episode 1/10000, loss: [0.0088510709]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 169 (169) @ Episode 1/10000, loss: [0.00894765]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 170 (170) @ Episode 1/10000, loss: [0.0095813992]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 171 (171) @ Episode 1/10000, loss: [0.0079126488]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 172 (172) @ Episode 1/10000, loss: [0.046217125]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 173 (173) @ Episode 1/10000, loss: [0.0065335212]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 174 (174) @ Episode 1/10000, loss: [0.0072510755]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 175 (175) @ Episode 1/10000, loss: [0.045093823]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 176 (176) @ Episode 1/10000, loss: [0.009247357]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 177 (177) @ Episode 1/10000, loss: [0.0075947964]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 178 (178) @ Episode 1/10000, loss: [0.0078564081]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 179 (179) @ Episode 1/10000, loss: [0.046433929]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 180 (180) @ Episode 1/10000, loss: [0.0057417778]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 181 (181) @ Episode 1/10000, loss: [0.0077213617]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 182 (182) @ Episode 1/10000, loss: [0.043058231]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 183 (183) @ Episode 1/10000, loss: [0.0077491915]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 184 (184) @ Episode 1/10000, loss: [0.0079812333]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 185 (185) @ Episode 1/10000, loss: [0.0094633]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 186 (186) @ Episode 1/10000, loss: [0.0081683397]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 187 (187) @ Episode 1/10000, loss: [0.0070067039]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 188 (188) @ Episode 1/10000, loss: [0.0071225511]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 189 (189) @ Episode 1/10000, loss: [0.007117549]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 190 (190) @ Episode 1/10000, loss: [0.03780321]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 191 (191) @ Episode 1/10000, loss: [0.0082006743]reward: 0.0\n",
      "reward_total: 0.0\n",
      "done: True\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (191) @ Episode 2/10000, loss: Nonereward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 1 (192) @ Episode 2/10000, loss: [0.0064684702]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 2 (193) @ Episode 2/10000, loss: [0.0072990814]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 3 (194) @ Episode 2/10000, loss: [0.0085425973]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 4 (195) @ Episode 2/10000, loss: [0.04515988]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 5 (196) @ Episode 2/10000, loss: [0.046709616]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 6 (197) @ Episode 2/10000, loss: [0.0067760288]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 7 (198) @ Episode 2/10000, loss: [0.0099529698]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 8 (199) @ Episode 2/10000, loss: [0.0073385006]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 9 (200) @ Episode 2/10000, loss: [0.0077215917]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 10 (201) @ Episode 2/10000, loss: [0.0077063134]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 11 (202) @ Episode 2/10000, loss: [0.0071846368]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 12 (203) @ Episode 2/10000, loss: [0.0073716175]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 13 (204) @ Episode 2/10000, loss: [0.0078756288]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 14 (205) @ Episode 2/10000, loss: [0.0081825871]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 15 (206) @ Episode 2/10000, loss: [0.0094105639]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 16 (207) @ Episode 2/10000, loss: [0.007044855]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 17 (208) @ Episode 2/10000, loss: [0.0069104764]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 18 (209) @ Episode 2/10000, loss: [0.0080798427]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 19 (210) @ Episode 2/10000, loss: [0.0089102648]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 20 (211) @ Episode 2/10000, loss: [0.0082137398]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 21 (212) @ Episode 2/10000, loss: [0.0085413065]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 22 (213) @ Episode 2/10000, loss: [0.0094695687]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 23 (214) @ Episode 2/10000, loss: [0.0085949823]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 24 (215) @ Episode 2/10000, loss: [0.082425714]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 25 (216) @ Episode 2/10000, loss: [0.0088876393]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 26 (217) @ Episode 2/10000, loss: [0.0083684744]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 27 (218) @ Episode 2/10000, loss: [0.0076011796]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 28 (219) @ Episode 2/10000, loss: [0.008011898]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 29 (220) @ Episode 2/10000, loss: [0.0084186802]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 30 (221) @ Episode 2/10000, loss: [0.0070280335]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 31 (222) @ Episode 2/10000, loss: [0.0079386244]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 32 (223) @ Episode 2/10000, loss: [0.0072911764]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 33 (224) @ Episode 2/10000, loss: [0.007073083]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 34 (225) @ Episode 2/10000, loss: [0.0071140672]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 35 (226) @ Episode 2/10000, loss: [0.008405216]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 36 (227) @ Episode 2/10000, loss: [0.0065423353]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 37 (228) @ Episode 2/10000, loss: [0.0083714444]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 38 (229) @ Episode 2/10000, loss: [0.0090265712]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 39 (230) @ Episode 2/10000, loss: [0.0078256298]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 40 (231) @ Episode 2/10000, loss: [0.0081642382]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 41 (232) @ Episode 2/10000, loss: [0.0080969129]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 42 (233) @ Episode 2/10000, loss: [0.007841723]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 43 (234) @ Episode 2/10000, loss: [0.045371115]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 44 (235) @ Episode 2/10000, loss: [0.0082098581]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 45 (236) @ Episode 2/10000, loss: [0.0080433544]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 46 (237) @ Episode 2/10000, loss: [0.0085465759]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 47 (238) @ Episode 2/10000, loss: [0.0085368762]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 48 (239) @ Episode 2/10000, loss: [0.0080936868]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 49 (240) @ Episode 2/10000, loss: [0.007896468]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 50 (241) @ Episode 2/10000, loss: [0.0087120542]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 51 (242) @ Episode 2/10000, loss: [0.0083317207]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 52 (243) @ Episode 2/10000, loss: [0.0083586192]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 53 (244) @ Episode 2/10000, loss: [0.0075910715]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 54 (245) @ Episode 2/10000, loss: [0.0090826023]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 55 (246) @ Episode 2/10000, loss: [0.0072494699]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 56 (247) @ Episode 2/10000, loss: [0.0088217463]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 57 (248) @ Episode 2/10000, loss: [0.0088591892]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 58 (249) @ Episode 2/10000, loss: [0.0084777977]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 59 (250) @ Episode 2/10000, loss: [0.0073950416]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 60 (251) @ Episode 2/10000, loss: [0.045423497]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 61 (252) @ Episode 2/10000, loss: [0.0077002589]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 62 (253) @ Episode 2/10000, loss: [0.045553364]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 63 (254) @ Episode 2/10000, loss: [0.039882246]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 64 (255) @ Episode 2/10000, loss: [0.038249247]reward: 1.0\n",
      "reward_total: 1.0\n",
      "Step 65 (256) @ Episode 2/10000, loss: [0.0076617673]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 66 (257) @ Episode 2/10000, loss: [0.007390873]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 67 (258) @ Episode 2/10000, loss: [0.0083163641]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 68 (259) @ Episode 2/10000, loss: [0.0079751108]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 69 (260) @ Episode 2/10000, loss: [0.047573194]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 70 (261) @ Episode 2/10000, loss: [0.0062920833]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 71 (262) @ Episode 2/10000, loss: [0.0074704364]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 72 (263) @ Episode 2/10000, loss: [0.0079989117]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 73 (264) @ Episode 2/10000, loss: [0.0091702789]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 74 (265) @ Episode 2/10000, loss: [0.0088587143]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 75 (266) @ Episode 2/10000, loss: [0.0074669085]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 76 (267) @ Episode 2/10000, loss: [0.0060554454]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 77 (268) @ Episode 2/10000, loss: [0.0097336806]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 78 (269) @ Episode 2/10000, loss: [0.0094927428]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 79 (270) @ Episode 2/10000, loss: [0.0071629253]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 80 (271) @ Episode 2/10000, loss: [0.0089092571]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 81 (272) @ Episode 2/10000, loss: [0.0069400268]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 82 (273) @ Episode 2/10000, loss: [0.0096679237]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 83 (274) @ Episode 2/10000, loss: [0.0076402426]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 84 (275) @ Episode 2/10000, loss: [0.0080000404]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 85 (276) @ Episode 2/10000, loss: [0.0080185216]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 86 (277) @ Episode 2/10000, loss: [0.0069460468]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 87 (278) @ Episode 2/10000, loss: [0.0076556522]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 88 (279) @ Episode 2/10000, loss: [0.0077529452]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 89 (280) @ Episode 2/10000, loss: [0.0078021176]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 90 (281) @ Episode 2/10000, loss: [0.0062124887]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 91 (282) @ Episode 2/10000, loss: [0.0088270446]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 92 (283) @ Episode 2/10000, loss: [0.0085757384]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 93 (284) @ Episode 2/10000, loss: [0.0080335848]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 94 (285) @ Episode 2/10000, loss: [0.0069561275]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 95 (286) @ Episode 2/10000, loss: [0.0080315406]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 96 (287) @ Episode 2/10000, loss: [0.0084245652]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 97 (288) @ Episode 2/10000, loss: [0.0084636398]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 98 (289) @ Episode 2/10000, loss: [0.0078120949]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 99 (290) @ Episode 2/10000, loss: [0.0084913988]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 100 (291) @ Episode 2/10000, loss: [0.0090272175]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 101 (292) @ Episode 2/10000, loss: [0.0067584054]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 102 (293) @ Episode 2/10000, loss: [0.0086846873]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 103 (294) @ Episode 2/10000, loss: [0.0085533019]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 104 (295) @ Episode 2/10000, loss: [0.0079186568]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 105 (296) @ Episode 2/10000, loss: [0.0072934409]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 106 (297) @ Episode 2/10000, loss: [0.0091334246]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 107 (298) @ Episode 2/10000, loss: [0.045154095]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 108 (299) @ Episode 2/10000, loss: [0.0065940716]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 109 (300) @ Episode 2/10000, loss: [0.0079590958]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 110 (301) @ Episode 2/10000, loss: [0.0080559552]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 111 (302) @ Episode 2/10000, loss: [0.0088249892]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 112 (303) @ Episode 2/10000, loss: [0.045184661]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 113 (304) @ Episode 2/10000, loss: [0.0068616727]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 114 (305) @ Episode 2/10000, loss: [0.0070964694]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 115 (306) @ Episode 2/10000, loss: [0.0081747007]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 116 (307) @ Episode 2/10000, loss: [0.0073593035]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 117 (308) @ Episode 2/10000, loss: [0.0078343153]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 118 (309) @ Episode 2/10000, loss: [0.0086427387]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 119 (310) @ Episode 2/10000, loss: [0.0087724859]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 120 (311) @ Episode 2/10000, loss: [0.039016262]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 121 (312) @ Episode 2/10000, loss: [0.045099687]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 122 (313) @ Episode 2/10000, loss: [0.0069465009]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 123 (314) @ Episode 2/10000, loss: [0.009165816]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 124 (315) @ Episode 2/10000, loss: [0.044867661]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 125 (316) @ Episode 2/10000, loss: [0.0081876284]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 126 (317) @ Episode 2/10000, loss: [0.0079698786]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 127 (318) @ Episode 2/10000, loss: [0.0080695571]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 128 (319) @ Episode 2/10000, loss: [0.0064446819]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 129 (320) @ Episode 2/10000, loss: [0.0088626016]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 130 (321) @ Episode 2/10000, loss: [0.0080929799]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 131 (322) @ Episode 2/10000, loss: [0.0082342215]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 132 (323) @ Episode 2/10000, loss: [0.0084025813]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 133 (324) @ Episode 2/10000, loss: [0.0077350447]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 134 (325) @ Episode 2/10000, loss: [0.007504872]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 135 (326) @ Episode 2/10000, loss: [0.0071825483]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 136 (327) @ Episode 2/10000, loss: [0.043845717]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 137 (328) @ Episode 2/10000, loss: [0.0082655307]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 138 (329) @ Episode 2/10000, loss: [0.0074978028]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 139 (330) @ Episode 2/10000, loss: [0.045185801]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 140 (331) @ Episode 2/10000, loss: [0.0068846573]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 141 (332) @ Episode 2/10000, loss: [0.0083281789]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 142 (333) @ Episode 2/10000, loss: [0.040297225]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 143 (334) @ Episode 2/10000, loss: [0.0083878879]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 144 (335) @ Episode 2/10000, loss: [0.0088566784]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 145 (336) @ Episode 2/10000, loss: [0.0074743568]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 146 (337) @ Episode 2/10000, loss: [0.0071183131]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 147 (338) @ Episode 2/10000, loss: [0.008529136]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 148 (339) @ Episode 2/10000, loss: [0.0084308852]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 149 (340) @ Episode 2/10000, loss: [0.044742316]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 150 (341) @ Episode 2/10000, loss: [0.038072862]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 151 (342) @ Episode 2/10000, loss: [0.04688983]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 152 (343) @ Episode 2/10000, loss: [0.0071975347]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 153 (344) @ Episode 2/10000, loss: [0.0065708188]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 154 (345) @ Episode 2/10000, loss: [0.0082737925]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 155 (346) @ Episode 2/10000, loss: [0.0083585912]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 156 (347) @ Episode 2/10000, loss: [0.039600093]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 157 (348) @ Episode 2/10000, loss: [0.0082990155]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 158 (349) @ Episode 2/10000, loss: [0.009517597]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 159 (350) @ Episode 2/10000, loss: [0.0083740782]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 160 (351) @ Episode 2/10000, loss: [0.0085633192]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 161 (352) @ Episode 2/10000, loss: [0.0084229149]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 162 (353) @ Episode 2/10000, loss: [0.0084432587]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 163 (354) @ Episode 2/10000, loss: [0.0075898287]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 164 (355) @ Episode 2/10000, loss: [0.0095175831]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 165 (356) @ Episode 2/10000, loss: [0.0069975387]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 166 (357) @ Episode 2/10000, loss: [0.076463938]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 167 (358) @ Episode 2/10000, loss: [0.0080257859]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 168 (359) @ Episode 2/10000, loss: [0.077031508]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 169 (360) @ Episode 2/10000, loss: [0.010125007]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 170 (361) @ Episode 2/10000, loss: [0.0089989882]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 171 (362) @ Episode 2/10000, loss: [0.0070042005]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 172 (363) @ Episode 2/10000, loss: [0.0077540716]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 173 (364) @ Episode 2/10000, loss: [0.0081572849]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 174 (365) @ Episode 2/10000, loss: [0.008052568]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 175 (366) @ Episode 2/10000, loss: [0.0079017868]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 176 (367) @ Episode 2/10000, loss: [0.0082202256]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 177 (368) @ Episode 2/10000, loss: [0.0074590296]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 178 (369) @ Episode 2/10000, loss: [0.0071929209]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 179 (370) @ Episode 2/10000, loss: [0.0083791288]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 180 (371) @ Episode 2/10000, loss: [0.0083603654]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 181 (372) @ Episode 2/10000, loss: [0.0069742743]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 182 (373) @ Episode 2/10000, loss: [0.0065261791]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 183 (374) @ Episode 2/10000, loss: [0.0077413958]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 184 (375) @ Episode 2/10000, loss: [0.0088682622]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 185 (376) @ Episode 2/10000, loss: [0.0073628686]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 186 (377) @ Episode 2/10000, loss: [0.0078008948]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 187 (378) @ Episode 2/10000, loss: [0.0068719061]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 188 (379) @ Episode 2/10000, loss: [0.0074897353]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 189 (380) @ Episode 2/10000, loss: [0.0074370108]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 190 (381) @ Episode 2/10000, loss: [0.039000951]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 191 (382) @ Episode 2/10000, loss: [0.039187077]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 192 (383) @ Episode 2/10000, loss: [0.045495033]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 193 (384) @ Episode 2/10000, loss: [0.00823138]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 194 (385) @ Episode 2/10000, loss: [0.008910506]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 195 (386) @ Episode 2/10000, loss: [0.0081994226]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 196 (387) @ Episode 2/10000, loss: [0.0081211301]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 197 (388) @ Episode 2/10000, loss: [0.0096412599]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 198 (389) @ Episode 2/10000, loss: [0.0072526075]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 199 (390) @ Episode 2/10000, loss: [0.0067053991]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 200 (391) @ Episode 2/10000, loss: [0.047014933]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 201 (392) @ Episode 2/10000, loss: [0.0091476552]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 202 (393) @ Episode 2/10000, loss: [0.0092701251]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 203 (394) @ Episode 2/10000, loss: [0.044263944]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 204 (395) @ Episode 2/10000, loss: [0.0078407247]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 205 (396) @ Episode 2/10000, loss: [0.0085485652]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 206 (397) @ Episode 2/10000, loss: [0.0074687982]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 207 (398) @ Episode 2/10000, loss: [0.0072936402]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 208 (399) @ Episode 2/10000, loss: [0.0085899169]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 209 (400) @ Episode 2/10000, loss: [0.0074846409]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 210 (401) @ Episode 2/10000, loss: [0.0073928786]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 211 (402) @ Episode 2/10000, loss: [0.0067465743]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 212 (403) @ Episode 2/10000, loss: [0.0089503638]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 213 (404) @ Episode 2/10000, loss: [0.0076500466]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 214 (405) @ Episode 2/10000, loss: [0.0069601298]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 215 (406) @ Episode 2/10000, loss: [0.0070213857]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 216 (407) @ Episode 2/10000, loss: [0.0076878853]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 217 (408) @ Episode 2/10000, loss: [0.0082038976]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 218 (409) @ Episode 2/10000, loss: [0.0082786353]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 219 (410) @ Episode 2/10000, loss: [0.0071377074]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 220 (411) @ Episode 2/10000, loss: [0.009427458]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 221 (412) @ Episode 2/10000, loss: [0.0065151067]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 222 (413) @ Episode 2/10000, loss: [0.007620703]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 223 (414) @ Episode 2/10000, loss: [0.0084558111]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 224 (415) @ Episode 2/10000, loss: [0.0090836976]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 225 (416) @ Episode 2/10000, loss: [0.006831822]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 226 (417) @ Episode 2/10000, loss: [0.0084241163]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 227 (418) @ Episode 2/10000, loss: [0.0091225272]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 228 (419) @ Episode 2/10000, loss: [0.045623615]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 229 (420) @ Episode 2/10000, loss: [0.0069541624]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 230 (421) @ Episode 2/10000, loss: [0.0075939177]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 231 (422) @ Episode 2/10000, loss: [0.0091892583]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 232 (423) @ Episode 2/10000, loss: [0.0073456918]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 233 (424) @ Episode 2/10000, loss: [0.0072733806]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 234 (425) @ Episode 2/10000, loss: [0.0087363645]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 235 (426) @ Episode 2/10000, loss: [0.0085622072]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 236 (427) @ Episode 2/10000, loss: [0.03821063]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 237 (428) @ Episode 2/10000, loss: [0.044917565]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 238 (429) @ Episode 2/10000, loss: [0.0080129746]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 239 (430) @ Episode 2/10000, loss: [0.0077382219]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 240 (431) @ Episode 2/10000, loss: [0.0071519674]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 241 (432) @ Episode 2/10000, loss: [0.0073468629]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 242 (433) @ Episode 2/10000, loss: [0.0084997118]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 243 (434) @ Episode 2/10000, loss: [0.0080188066]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 244 (435) @ Episode 2/10000, loss: [0.007713113]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 245 (436) @ Episode 2/10000, loss: [0.0078971013]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 246 (437) @ Episode 2/10000, loss: [0.0093624108]reward: 0.0\n",
      "reward_total: 1.0\n",
      "done: True\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 0 (437) @ Episode 3/10000, loss: Nonereward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 1 (438) @ Episode 3/10000, loss: [0.0083990619]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 2 (439) @ Episode 3/10000, loss: [0.0092242453]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 3 (440) @ Episode 3/10000, loss: [0.0083097536]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 4 (441) @ Episode 3/10000, loss: [0.0073884805]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 5 (442) @ Episode 3/10000, loss: [0.008231096]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 6 (443) @ Episode 3/10000, loss: [0.0068573514]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 7 (444) @ Episode 3/10000, loss: [0.0077734916]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 8 (445) @ Episode 3/10000, loss: [0.0085262619]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 9 (446) @ Episode 3/10000, loss: [0.0080220504]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 10 (447) @ Episode 3/10000, loss: [0.0085553061]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 11 (448) @ Episode 3/10000, loss: [0.0075727864]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 12 (449) @ Episode 3/10000, loss: [0.0082846843]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 13 (450) @ Episode 3/10000, loss: [0.044905771]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 14 (451) @ Episode 3/10000, loss: [0.0089505585]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 15 (452) @ Episode 3/10000, loss: [0.0072602713]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 16 (453) @ Episode 3/10000, loss: [0.0073566115]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 17 (454) @ Episode 3/10000, loss: [0.0082021635]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 18 (455) @ Episode 3/10000, loss: [0.0066099907]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 19 (456) @ Episode 3/10000, loss: [0.0077603362]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 20 (457) @ Episode 3/10000, loss: [0.0080118682]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 21 (458) @ Episode 3/10000, loss: [0.0077029569]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 22 (459) @ Episode 3/10000, loss: [0.0066517722]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 23 (460) @ Episode 3/10000, loss: [0.0069992412]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 24 (461) @ Episode 3/10000, loss: [0.0081457878]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 25 (462) @ Episode 3/10000, loss: [0.0089133047]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 26 (463) @ Episode 3/10000, loss: [0.0080793044]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 27 (464) @ Episode 3/10000, loss: [0.0074104615]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 28 (465) @ Episode 3/10000, loss: [0.0073315729]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 29 (466) @ Episode 3/10000, loss: [0.0084805535]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 30 (467) @ Episode 3/10000, loss: [0.007659982]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 31 (468) @ Episode 3/10000, loss: [0.044738106]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 32 (469) @ Episode 3/10000, loss: [0.0099192774]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 33 (470) @ Episode 3/10000, loss: [0.0064752148]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 34 (471) @ Episode 3/10000, loss: [0.0090908762]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 35 (472) @ Episode 3/10000, loss: [0.0090414248]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 36 (473) @ Episode 3/10000, loss: [0.0083781919]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 37 (474) @ Episode 3/10000, loss: [0.0083336048]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 38 (475) @ Episode 3/10000, loss: [0.0079988278]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 39 (476) @ Episode 3/10000, loss: [0.0086022057]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 40 (477) @ Episode 3/10000, loss: [0.0080620116]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 41 (478) @ Episode 3/10000, loss: [0.046127465]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 42 (479) @ Episode 3/10000, loss: [0.0081756674]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 43 (480) @ Episode 3/10000, loss: [0.008275304]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 44 (481) @ Episode 3/10000, loss: [0.0079677813]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 45 (482) @ Episode 3/10000, loss: [0.045976032]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 46 (483) @ Episode 3/10000, loss: [0.0068054465]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 47 (484) @ Episode 3/10000, loss: [0.0062033162]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 48 (485) @ Episode 3/10000, loss: [0.0087362304]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 49 (486) @ Episode 3/10000, loss: [0.0073559787]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 50 (487) @ Episode 3/10000, loss: [0.0084203314]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 51 (488) @ Episode 3/10000, loss: [0.083422624]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 52 (489) @ Episode 3/10000, loss: [0.0070644561]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 53 (490) @ Episode 3/10000, loss: [0.0070623066]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 54 (491) @ Episode 3/10000, loss: [0.0086661875]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 55 (492) @ Episode 3/10000, loss: [0.045357414]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 56 (493) @ Episode 3/10000, loss: [0.04521393]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 57 (494) @ Episode 3/10000, loss: [0.0088158967]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 58 (495) @ Episode 3/10000, loss: [0.038036205]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 59 (496) @ Episode 3/10000, loss: [0.042414386]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 60 (497) @ Episode 3/10000, loss: [0.0084061623]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 61 (498) @ Episode 3/10000, loss: [0.0081616081]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 62 (499) @ Episode 3/10000, loss: [0.007637383]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 63 (500) @ Episode 3/10000, loss: [0.0082773622]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 64 (501) @ Episode 3/10000, loss: [0.0073761721]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 65 (502) @ Episode 3/10000, loss: [0.0092828767]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 66 (503) @ Episode 3/10000, loss: [0.0071119992]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 67 (504) @ Episode 3/10000, loss: [0.0082943626]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 68 (505) @ Episode 3/10000, loss: [0.0083151497]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 69 (506) @ Episode 3/10000, loss: [0.0076131751]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 70 (507) @ Episode 3/10000, loss: [0.0077855]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 71 (508) @ Episode 3/10000, loss: [0.0079520913]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 72 (509) @ Episode 3/10000, loss: [0.0083215348]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 73 (510) @ Episode 3/10000, loss: [0.0062000863]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 74 (511) @ Episode 3/10000, loss: [0.0065211984]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 75 (512) @ Episode 3/10000, loss: [0.046708774]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 76 (513) @ Episode 3/10000, loss: [0.0091085806]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 77 (514) @ Episode 3/10000, loss: [0.045985937]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 78 (515) @ Episode 3/10000, loss: [0.0083399545]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 79 (516) @ Episode 3/10000, loss: [0.0087641161]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 80 (517) @ Episode 3/10000, loss: [0.0084933946]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 81 (518) @ Episode 3/10000, loss: [0.0069398824]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 82 (519) @ Episode 3/10000, loss: [0.0085990531]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 83 (520) @ Episode 3/10000, loss: [0.045678657]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 84 (521) @ Episode 3/10000, loss: [0.0080233067]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 85 (522) @ Episode 3/10000, loss: [0.0093359174]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 86 (523) @ Episode 3/10000, loss: [0.0090906695]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 87 (524) @ Episode 3/10000, loss: [0.0077067525]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 88 (525) @ Episode 3/10000, loss: [0.0097331097]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 89 (526) @ Episode 3/10000, loss: [0.0087935068]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 90 (527) @ Episode 3/10000, loss: [0.0088906623]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 91 (528) @ Episode 3/10000, loss: [0.008279982]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 92 (529) @ Episode 3/10000, loss: [0.007686872]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 93 (530) @ Episode 3/10000, loss: [0.0078312345]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 94 (531) @ Episode 3/10000, loss: [0.0093764579]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 95 (532) @ Episode 3/10000, loss: [0.0078399582]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 96 (533) @ Episode 3/10000, loss: [0.0079119392]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 97 (534) @ Episode 3/10000, loss: [0.0077969804]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 98 (535) @ Episode 3/10000, loss: [0.0088816844]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 99 (536) @ Episode 3/10000, loss: [0.046373978]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 100 (537) @ Episode 3/10000, loss: [0.0085780956]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 101 (538) @ Episode 3/10000, loss: [0.0081584034]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 102 (539) @ Episode 3/10000, loss: [0.0070620957]reward: 1.0\n",
      "reward_total: 1.0\n",
      "Step 103 (540) @ Episode 3/10000, loss: [0.0083591063]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 104 (541) @ Episode 3/10000, loss: [0.0079015391]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 105 (542) @ Episode 3/10000, loss: [0.0078258598]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 106 (543) @ Episode 3/10000, loss: [0.0078813378]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 107 (544) @ Episode 3/10000, loss: [0.0079205558]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 108 (545) @ Episode 3/10000, loss: [0.0058793789]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 109 (546) @ Episode 3/10000, loss: [0.0092630256]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 110 (547) @ Episode 3/10000, loss: [0.0089019667]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 111 (548) @ Episode 3/10000, loss: [0.0068444083]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 112 (549) @ Episode 3/10000, loss: [0.0073379921]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 113 (550) @ Episode 3/10000, loss: [0.0066367332]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 114 (551) @ Episode 3/10000, loss: [0.0075323014]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 115 (552) @ Episode 3/10000, loss: [0.045970298]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 116 (553) @ Episode 3/10000, loss: [0.04477879]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 117 (554) @ Episode 3/10000, loss: [0.0073506236]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 118 (555) @ Episode 3/10000, loss: [0.0068453276]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 119 (556) @ Episode 3/10000, loss: [0.0074832877]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 120 (557) @ Episode 3/10000, loss: [0.006363011]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 121 (558) @ Episode 3/10000, loss: [0.0072594904]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 122 (559) @ Episode 3/10000, loss: [0.039622396]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 123 (560) @ Episode 3/10000, loss: [0.0081603732]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 124 (561) @ Episode 3/10000, loss: [0.0081663486]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 125 (562) @ Episode 3/10000, loss: [0.0069416682]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 126 (563) @ Episode 3/10000, loss: [0.0078661963]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 127 (564) @ Episode 3/10000, loss: [0.0071463003]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 128 (565) @ Episode 3/10000, loss: [0.0083689876]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 129 (566) @ Episode 3/10000, loss: [0.0079735667]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 130 (567) @ Episode 3/10000, loss: [0.0074106669]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 131 (568) @ Episode 3/10000, loss: [0.0085146707]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 132 (569) @ Episode 3/10000, loss: [0.045010731]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 133 (570) @ Episode 3/10000, loss: [0.0082783885]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 134 (571) @ Episode 3/10000, loss: [0.0073522041]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 135 (572) @ Episode 3/10000, loss: [0.0087449299]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 136 (573) @ Episode 3/10000, loss: [0.0078726606]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 137 (574) @ Episode 3/10000, loss: [0.0069946628]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 138 (575) @ Episode 3/10000, loss: [0.00896433]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 139 (576) @ Episode 3/10000, loss: [0.0086367708]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 140 (577) @ Episode 3/10000, loss: [0.037819795]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 141 (578) @ Episode 3/10000, loss: [0.007452454]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 142 (579) @ Episode 3/10000, loss: [0.0084277224]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 143 (580) @ Episode 3/10000, loss: [0.04616737]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 144 (581) @ Episode 3/10000, loss: [0.0084939469]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 145 (582) @ Episode 3/10000, loss: [0.0081178695]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 146 (583) @ Episode 3/10000, loss: [0.0079469644]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 147 (584) @ Episode 3/10000, loss: [0.007290503]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 148 (585) @ Episode 3/10000, loss: [0.0082450621]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 149 (586) @ Episode 3/10000, loss: [0.0074741677]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 150 (587) @ Episode 3/10000, loss: [0.0077349311]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 151 (588) @ Episode 3/10000, loss: [0.0067128167]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 152 (589) @ Episode 3/10000, loss: [0.0085927621]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 153 (590) @ Episode 3/10000, loss: [0.0075906664]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 154 (591) @ Episode 3/10000, loss: [0.046238147]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 155 (592) @ Episode 3/10000, loss: [0.0089969775]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 156 (593) @ Episode 3/10000, loss: [0.0074989875]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 157 (594) @ Episode 3/10000, loss: [0.039025977]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 158 (595) @ Episode 3/10000, loss: [0.0071404171]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 159 (596) @ Episode 3/10000, loss: [0.076642975]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 160 (597) @ Episode 3/10000, loss: [0.0072931806]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 161 (598) @ Episode 3/10000, loss: [0.008238297]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 162 (599) @ Episode 3/10000, loss: [0.046094511]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 163 (600) @ Episode 3/10000, loss: [0.0072980365]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 164 (601) @ Episode 3/10000, loss: [0.046842672]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 165 (602) @ Episode 3/10000, loss: [0.0070712445]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 166 (603) @ Episode 3/10000, loss: [0.0070042615]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 167 (604) @ Episode 3/10000, loss: [0.007317882]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 168 (605) @ Episode 3/10000, loss: [0.0079515269]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 169 (606) @ Episode 3/10000, loss: [0.0069946935]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 170 (607) @ Episode 3/10000, loss: [0.0083260052]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 171 (608) @ Episode 3/10000, loss: [0.0076391767]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 172 (609) @ Episode 3/10000, loss: [0.0072147036]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 173 (610) @ Episode 3/10000, loss: [0.0056390464]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 174 (611) @ Episode 3/10000, loss: [0.0059125926]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 175 (612) @ Episode 3/10000, loss: [0.0087477975]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 176 (613) @ Episode 3/10000, loss: [0.006790367]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 177 (614) @ Episode 3/10000, loss: [0.046932042]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 178 (615) @ Episode 3/10000, loss: [0.00807874]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 179 (616) @ Episode 3/10000, loss: [0.0084597636]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 180 (617) @ Episode 3/10000, loss: [0.0063093808]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 181 (618) @ Episode 3/10000, loss: [0.047425393]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 182 (619) @ Episode 3/10000, loss: [0.045711178]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 183 (620) @ Episode 3/10000, loss: [0.046009004]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 184 (621) @ Episode 3/10000, loss: [0.0077018123]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 185 (622) @ Episode 3/10000, loss: [0.03708604]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 186 (623) @ Episode 3/10000, loss: [0.0080740843]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 187 (624) @ Episode 3/10000, loss: [0.0072235498]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 188 (625) @ Episode 3/10000, loss: [0.0075345216]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 189 (626) @ Episode 3/10000, loss: [0.0079025505]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 190 (627) @ Episode 3/10000, loss: [0.0071208812]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 191 (628) @ Episode 3/10000, loss: [0.0075020245]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 192 (629) @ Episode 3/10000, loss: [0.0078280326]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 193 (630) @ Episode 3/10000, loss: [0.0071299593]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 194 (631) @ Episode 3/10000, loss: [0.0065632556]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 195 (632) @ Episode 3/10000, loss: [0.007331227]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 196 (633) @ Episode 3/10000, loss: [0.0075501474]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 197 (634) @ Episode 3/10000, loss: [0.0067654424]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 198 (635) @ Episode 3/10000, loss: [0.083263032]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 199 (636) @ Episode 3/10000, loss: [0.007122694]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 200 (637) @ Episode 3/10000, loss: [0.046902113]reward: 1.0\n",
      "reward_total: 2.0\n",
      "Step 201 (638) @ Episode 3/10000, loss: [0.0087199174]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 202 (639) @ Episode 3/10000, loss: [0.04580259]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 203 (640) @ Episode 3/10000, loss: [0.0080332747]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 204 (641) @ Episode 3/10000, loss: [0.043327164]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 205 (642) @ Episode 3/10000, loss: [0.047325637]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 206 (643) @ Episode 3/10000, loss: [0.0093115903]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 207 (644) @ Episode 3/10000, loss: [0.009463815]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 208 (645) @ Episode 3/10000, loss: [0.0088525377]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 209 (646) @ Episode 3/10000, loss: [0.0089910161]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 210 (647) @ Episode 3/10000, loss: [0.044646442]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 211 (648) @ Episode 3/10000, loss: [0.0071162917]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 212 (649) @ Episode 3/10000, loss: [0.0075107673]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 213 (650) @ Episode 3/10000, loss: [0.0080562159]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 214 (651) @ Episode 3/10000, loss: [0.0075972066]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 215 (652) @ Episode 3/10000, loss: [0.007600659]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 216 (653) @ Episode 3/10000, loss: [0.0074403011]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 217 (654) @ Episode 3/10000, loss: [0.0080489237]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 218 (655) @ Episode 3/10000, loss: [0.044843044]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 219 (656) @ Episode 3/10000, loss: [0.044799432]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 220 (657) @ Episode 3/10000, loss: [0.0087409355]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 221 (658) @ Episode 3/10000, loss: [0.008829847]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 222 (659) @ Episode 3/10000, loss: [0.0090294238]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 223 (660) @ Episode 3/10000, loss: [0.0074498388]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 224 (661) @ Episode 3/10000, loss: [0.0071688285]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 225 (662) @ Episode 3/10000, loss: [0.045714021]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 226 (663) @ Episode 3/10000, loss: [0.0074545178]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 227 (664) @ Episode 3/10000, loss: [0.038810946]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 228 (665) @ Episode 3/10000, loss: [0.0075970748]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 229 (666) @ Episode 3/10000, loss: [0.0077143898]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 230 (667) @ Episode 3/10000, loss: [0.0066401321]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 231 (668) @ Episode 3/10000, loss: [0.039405312]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 232 (669) @ Episode 3/10000, loss: [0.0084143709]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 233 (670) @ Episode 3/10000, loss: [0.0076600644]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 234 (671) @ Episode 3/10000, loss: [0.0072360206]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 235 (672) @ Episode 3/10000, loss: [0.0066052368]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 236 (673) @ Episode 3/10000, loss: [0.008417665]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 237 (674) @ Episode 3/10000, loss: [0.0079732994]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 238 (675) @ Episode 3/10000, loss: [0.045766931]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 239 (676) @ Episode 3/10000, loss: [0.0071568219]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 240 (677) @ Episode 3/10000, loss: [0.0074419877]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 241 (678) @ Episode 3/10000, loss: [0.0082613956]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 242 (679) @ Episode 3/10000, loss: [0.007393166]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 243 (680) @ Episode 3/10000, loss: [0.0081448844]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 244 (681) @ Episode 3/10000, loss: [0.045731347]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 245 (682) @ Episode 3/10000, loss: [0.0079676332]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 246 (683) @ Episode 3/10000, loss: [0.00780572]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 247 (684) @ Episode 3/10000, loss: [0.0070466874]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 248 (685) @ Episode 3/10000, loss: [0.0095330216]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 249 (686) @ Episode 3/10000, loss: [0.0074433805]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 250 (687) @ Episode 3/10000, loss: [0.045972727]reward: 1.0\n",
      "reward_total: 3.0\n",
      "Step 251 (688) @ Episode 3/10000, loss: [0.0386087]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 252 (689) @ Episode 3/10000, loss: [0.0090035973]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 253 (690) @ Episode 3/10000, loss: [0.0079667624]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 254 (691) @ Episode 3/10000, loss: [0.0084834341]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 255 (692) @ Episode 3/10000, loss: [0.0088573154]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 256 (693) @ Episode 3/10000, loss: [0.0082799708]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 257 (694) @ Episode 3/10000, loss: [0.044357251]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 258 (695) @ Episode 3/10000, loss: [0.0082491171]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 259 (696) @ Episode 3/10000, loss: [0.0078433473]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 260 (697) @ Episode 3/10000, loss: [0.0079218484]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 261 (698) @ Episode 3/10000, loss: [0.0084576504]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 262 (699) @ Episode 3/10000, loss: [0.0094761755]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 263 (700) @ Episode 3/10000, loss: [0.0074015949]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 264 (701) @ Episode 3/10000, loss: [0.0076945261]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 265 (702) @ Episode 3/10000, loss: [0.0084982291]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 266 (703) @ Episode 3/10000, loss: [0.044994514]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 267 (704) @ Episode 3/10000, loss: [0.044443395]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 268 (705) @ Episode 3/10000, loss: [0.0085191373]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 269 (706) @ Episode 3/10000, loss: [0.0071289791]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 270 (707) @ Episode 3/10000, loss: [0.043361355]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 271 (708) @ Episode 3/10000, loss: [0.008280687]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 272 (709) @ Episode 3/10000, loss: [0.0074479403]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 273 (710) @ Episode 3/10000, loss: [0.0079003479]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 274 (711) @ Episode 3/10000, loss: [0.0089182789]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 275 (712) @ Episode 3/10000, loss: [0.0078433529]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 276 (713) @ Episode 3/10000, loss: [0.0077222446]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 277 (714) @ Episode 3/10000, loss: [0.044431474]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 278 (715) @ Episode 3/10000, loss: [0.007180837]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 279 (716) @ Episode 3/10000, loss: [0.0085374983]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 280 (717) @ Episode 3/10000, loss: [0.0091509316]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 281 (718) @ Episode 3/10000, loss: [0.0093332957]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 282 (719) @ Episode 3/10000, loss: [0.0073385602]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 283 (720) @ Episode 3/10000, loss: [0.0089908019]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 284 (721) @ Episode 3/10000, loss: [0.0073728058]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 285 (722) @ Episode 3/10000, loss: [0.0084479023]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 286 (723) @ Episode 3/10000, loss: [0.007063054]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 287 (724) @ Episode 3/10000, loss: [0.0078677256]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 288 (725) @ Episode 3/10000, loss: [0.0081060287]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 289 (726) @ Episode 3/10000, loss: [0.0089319553]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 290 (727) @ Episode 3/10000, loss: [0.0073491773]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 291 (728) @ Episode 3/10000, loss: [0.008409678]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 292 (729) @ Episode 3/10000, loss: [0.0070935367]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 293 (730) @ Episode 3/10000, loss: [0.008866719]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 294 (731) @ Episode 3/10000, loss: [0.0084605087]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 295 (732) @ Episode 3/10000, loss: [0.044822793]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 296 (733) @ Episode 3/10000, loss: [0.0074539441]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 297 (734) @ Episode 3/10000, loss: [0.0077992603]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 298 (735) @ Episode 3/10000, loss: [0.0087679187]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 299 (736) @ Episode 3/10000, loss: [0.0086357575]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 300 (737) @ Episode 3/10000, loss: [0.0089076236]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 301 (738) @ Episode 3/10000, loss: [0.0070984853]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 302 (739) @ Episode 3/10000, loss: [0.007189767]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 303 (740) @ Episode 3/10000, loss: [0.0081320507]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 304 (741) @ Episode 3/10000, loss: [0.0076088365]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 305 (742) @ Episode 3/10000, loss: [0.0072689895]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 306 (743) @ Episode 3/10000, loss: [0.0088290125]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 307 (744) @ Episode 3/10000, loss: [0.0079258615]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 308 (745) @ Episode 3/10000, loss: [0.0085625369]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 309 (746) @ Episode 3/10000, loss: [0.0080368556]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 310 (747) @ Episode 3/10000, loss: [0.0077694142]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 311 (748) @ Episode 3/10000, loss: [0.0080018509]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 312 (749) @ Episode 3/10000, loss: [0.0080095688]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 313 (750) @ Episode 3/10000, loss: [0.0083948057]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 314 (751) @ Episode 3/10000, loss: [0.0081036203]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 315 (752) @ Episode 3/10000, loss: [0.0083316248]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 316 (753) @ Episode 3/10000, loss: [0.084763914]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 317 (754) @ Episode 3/10000, loss: [0.0086993575]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 318 (755) @ Episode 3/10000, loss: [0.0083584366]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 319 (756) @ Episode 3/10000, loss: [0.0089653749]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 320 (757) @ Episode 3/10000, loss: [0.008703677]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 321 (758) @ Episode 3/10000, loss: [0.045630593]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 322 (759) @ Episode 3/10000, loss: [0.007738892]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 323 (760) @ Episode 3/10000, loss: [0.0072315242]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 324 (761) @ Episode 3/10000, loss: [0.0077742385]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 325 (762) @ Episode 3/10000, loss: [0.0073182918]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 326 (763) @ Episode 3/10000, loss: [0.0075426651]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 327 (764) @ Episode 3/10000, loss: [0.0086347302]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 328 (765) @ Episode 3/10000, loss: [0.0080963671]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 329 (766) @ Episode 3/10000, loss: [0.0092106117]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 330 (767) @ Episode 3/10000, loss: [0.0076785199]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 331 (768) @ Episode 3/10000, loss: [0.0069065848]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 332 (769) @ Episode 3/10000, loss: [0.0077836253]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 333 (770) @ Episode 3/10000, loss: [0.0078374697]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 334 (771) @ Episode 3/10000, loss: [0.007270447]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 335 (772) @ Episode 3/10000, loss: [0.0099576125]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 336 (773) @ Episode 3/10000, loss: [0.0070013525]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 337 (774) @ Episode 3/10000, loss: [0.0085337479]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 338 (775) @ Episode 3/10000, loss: [0.0077131856]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 339 (776) @ Episode 3/10000, loss: [0.044812862]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 340 (777) @ Episode 3/10000, loss: [0.0085058529]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 341 (778) @ Episode 3/10000, loss: [0.045200855]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 342 (779) @ Episode 3/10000, loss: [0.044468671]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 343 (780) @ Episode 3/10000, loss: [0.046197534]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 344 (781) @ Episode 3/10000, loss: [0.0077906847]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 345 (782) @ Episode 3/10000, loss: [0.0073639606]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 346 (783) @ Episode 3/10000, loss: [0.0085791163]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 347 (784) @ Episode 3/10000, loss: [0.0078126043]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 348 (785) @ Episode 3/10000, loss: [0.0087994654]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 349 (786) @ Episode 3/10000, loss: [0.0078500099]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 350 (787) @ Episode 3/10000, loss: [0.0089120436]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 351 (788) @ Episode 3/10000, loss: [0.047047019]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 352 (789) @ Episode 3/10000, loss: [0.0072063473]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 353 (790) @ Episode 3/10000, loss: [0.0066559268]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 354 (791) @ Episode 3/10000, loss: [0.037290685]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 355 (792) @ Episode 3/10000, loss: [0.0076182662]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 356 (793) @ Episode 3/10000, loss: [0.0074292002]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 357 (794) @ Episode 3/10000, loss: [0.045805961]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 358 (795) @ Episode 3/10000, loss: [0.0080272239]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 359 (796) @ Episode 3/10000, loss: [0.046202876]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 360 (797) @ Episode 3/10000, loss: [0.0069910563]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 361 (798) @ Episode 3/10000, loss: [0.045882072]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 362 (799) @ Episode 3/10000, loss: [0.0070387246]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 363 (800) @ Episode 3/10000, loss: [0.0082000243]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 364 (801) @ Episode 3/10000, loss: [0.006525374]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 365 (802) @ Episode 3/10000, loss: [0.0074128276]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 366 (803) @ Episode 3/10000, loss: [0.0081990091]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 367 (804) @ Episode 3/10000, loss: [0.0082193296]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 368 (805) @ Episode 3/10000, loss: [0.0065619312]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 369 (806) @ Episode 3/10000, loss: [0.0078159245]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 370 (807) @ Episode 3/10000, loss: [0.0082679978]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 371 (808) @ Episode 3/10000, loss: [0.0068335407]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 372 (809) @ Episode 3/10000, loss: [0.0086942855]reward: 0.0\n",
      "reward_total: 3.0\n",
      "done: True\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 0 (809) @ Episode 4/10000, loss: Nonereward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 1 (810) @ Episode 4/10000, loss: [0.0065279608]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 2 (811) @ Episode 4/10000, loss: [0.0075233853]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 3 (812) @ Episode 4/10000, loss: [0.0074341716]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 4 (813) @ Episode 4/10000, loss: [0.0083675608]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 5 (814) @ Episode 4/10000, loss: [0.039280385]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 6 (815) @ Episode 4/10000, loss: [0.0079084057]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 7 (816) @ Episode 4/10000, loss: [0.0079215551]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 8 (817) @ Episode 4/10000, loss: [0.0071713836]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 9 (818) @ Episode 4/10000, loss: [0.040554985]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 10 (819) @ Episode 4/10000, loss: [0.0097146034]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 11 (820) @ Episode 4/10000, loss: [0.044490654]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 12 (821) @ Episode 4/10000, loss: [0.0061416412]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 13 (822) @ Episode 4/10000, loss: [0.0093773855]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 14 (823) @ Episode 4/10000, loss: [0.0087613016]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 15 (824) @ Episode 4/10000, loss: [0.0075452928]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 16 (825) @ Episode 4/10000, loss: [0.0096164159]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 17 (826) @ Episode 4/10000, loss: [0.0074227666]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 18 (827) @ Episode 4/10000, loss: [0.0078827254]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 19 (828) @ Episode 4/10000, loss: [0.0086036278]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 20 (829) @ Episode 4/10000, loss: [0.0068298695]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 21 (830) @ Episode 4/10000, loss: [0.0082868319]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 22 (831) @ Episode 4/10000, loss: [0.0089511015]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 23 (832) @ Episode 4/10000, loss: [0.0084850593]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 24 (833) @ Episode 4/10000, loss: [0.00853601]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 25 (834) @ Episode 4/10000, loss: [0.0087886164]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 26 (835) @ Episode 4/10000, loss: [0.0080188867]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 27 (836) @ Episode 4/10000, loss: [0.0087877782]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 28 (837) @ Episode 4/10000, loss: [0.0079770088]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 29 (838) @ Episode 4/10000, loss: [0.0083564287]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 30 (839) @ Episode 4/10000, loss: [0.0085317111]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 31 (840) @ Episode 4/10000, loss: [0.0080385553]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 32 (841) @ Episode 4/10000, loss: [0.0059399018]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 33 (842) @ Episode 4/10000, loss: [0.0077236714]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 34 (843) @ Episode 4/10000, loss: [0.0078246947]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 35 (844) @ Episode 4/10000, loss: [0.0086681303]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 36 (845) @ Episode 4/10000, loss: [0.008834336]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 37 (846) @ Episode 4/10000, loss: [0.0075531504]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 38 (847) @ Episode 4/10000, loss: [0.009083217]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 39 (848) @ Episode 4/10000, loss: [0.0080441646]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 40 (849) @ Episode 4/10000, loss: [0.0075551551]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 41 (850) @ Episode 4/10000, loss: [0.046893302]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 42 (851) @ Episode 4/10000, loss: [0.0079614073]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 43 (852) @ Episode 4/10000, loss: [0.0075659095]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 44 (853) @ Episode 4/10000, loss: [0.0082586147]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 45 (854) @ Episode 4/10000, loss: [0.0080173276]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 46 (855) @ Episode 4/10000, loss: [0.0088617271]reward: 1.0\n",
      "reward_total: 1.0\n",
      "Step 47 (856) @ Episode 4/10000, loss: [0.0096015688]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 48 (857) @ Episode 4/10000, loss: [0.0090475902]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 49 (858) @ Episode 4/10000, loss: [0.0075964546]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 50 (859) @ Episode 4/10000, loss: [0.0076041576]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 51 (860) @ Episode 4/10000, loss: [0.045064043]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 52 (861) @ Episode 4/10000, loss: [0.0085836891]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 53 (862) @ Episode 4/10000, loss: [0.046442833]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 54 (863) @ Episode 4/10000, loss: [0.0087112701]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 55 (864) @ Episode 4/10000, loss: [0.0078310128]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 56 (865) @ Episode 4/10000, loss: [0.045310911]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 57 (866) @ Episode 4/10000, loss: [0.0077828756]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 58 (867) @ Episode 4/10000, loss: [0.0087945703]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 59 (868) @ Episode 4/10000, loss: [0.0077211093]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 60 (869) @ Episode 4/10000, loss: [0.0069127716]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 61 (870) @ Episode 4/10000, loss: [0.0072106374]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 62 (871) @ Episode 4/10000, loss: [0.0080251405]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 63 (872) @ Episode 4/10000, loss: [0.0079542454]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 64 (873) @ Episode 4/10000, loss: [0.047313407]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 65 (874) @ Episode 4/10000, loss: [0.0078681624]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 66 (875) @ Episode 4/10000, loss: [0.045779396]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 67 (876) @ Episode 4/10000, loss: [0.0076488424]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 68 (877) @ Episode 4/10000, loss: [0.038521007]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 69 (878) @ Episode 4/10000, loss: [0.0074047716]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 70 (879) @ Episode 4/10000, loss: [0.0067950194]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 71 (880) @ Episode 4/10000, loss: [0.0086555276]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 72 (881) @ Episode 4/10000, loss: [0.0072879782]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 73 (882) @ Episode 4/10000, loss: [0.0065087564]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 74 (883) @ Episode 4/10000, loss: [0.0065816399]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 75 (884) @ Episode 4/10000, loss: [0.0071713887]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 76 (885) @ Episode 4/10000, loss: [0.045962688]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 77 (886) @ Episode 4/10000, loss: [0.007238368]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 78 (887) @ Episode 4/10000, loss: [0.0081776287]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 79 (888) @ Episode 4/10000, loss: [0.007792891]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 80 (889) @ Episode 4/10000, loss: [0.0075273504]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 81 (890) @ Episode 4/10000, loss: [0.0076153316]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 82 (891) @ Episode 4/10000, loss: [0.0080191353]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 83 (892) @ Episode 4/10000, loss: [0.044532061]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 84 (893) @ Episode 4/10000, loss: [0.0084057823]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 85 (894) @ Episode 4/10000, loss: [0.0081746327]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 86 (895) @ Episode 4/10000, loss: [0.047166783]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 87 (896) @ Episode 4/10000, loss: [0.045115437]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 88 (897) @ Episode 4/10000, loss: [0.0074301641]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 89 (898) @ Episode 4/10000, loss: [0.0079234075]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 90 (899) @ Episode 4/10000, loss: [0.0078542046]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 91 (900) @ Episode 4/10000, loss: [0.0071882606]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 92 (901) @ Episode 4/10000, loss: [0.0075178239]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 93 (902) @ Episode 4/10000, loss: [0.044965055]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 94 (903) @ Episode 4/10000, loss: [0.0082225725]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 95 (904) @ Episode 4/10000, loss: [0.007127855]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 96 (905) @ Episode 4/10000, loss: [0.045463871]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 97 (906) @ Episode 4/10000, loss: [0.0082510691]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 98 (907) @ Episode 4/10000, loss: [0.0084062098]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 99 (908) @ Episode 4/10000, loss: [0.0078544207]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 100 (909) @ Episode 4/10000, loss: [0.0071094721]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 101 (910) @ Episode 4/10000, loss: [0.0082565416]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 102 (911) @ Episode 4/10000, loss: [0.0086552789]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 103 (912) @ Episode 4/10000, loss: [0.0080453092]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 104 (913) @ Episode 4/10000, loss: [0.0083918963]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 105 (914) @ Episode 4/10000, loss: [0.0065358821]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 106 (915) @ Episode 4/10000, loss: [0.0077714538]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 107 (916) @ Episode 4/10000, loss: [0.0098676654]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 108 (917) @ Episode 4/10000, loss: [0.0081970887]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 109 (918) @ Episode 4/10000, loss: [0.0078615714]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 110 (919) @ Episode 4/10000, loss: [0.046067409]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 111 (920) @ Episode 4/10000, loss: [0.044436261]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 112 (921) @ Episode 4/10000, loss: [0.0084544076]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 113 (922) @ Episode 4/10000, loss: [0.0096876379]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 114 (923) @ Episode 4/10000, loss: [0.0074668927]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 115 (924) @ Episode 4/10000, loss: [0.0098004388]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 116 (925) @ Episode 4/10000, loss: [0.0094736479]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 117 (926) @ Episode 4/10000, loss: [0.0076870979]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 118 (927) @ Episode 4/10000, loss: [0.0081217661]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 119 (928) @ Episode 4/10000, loss: [0.0067153247]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 120 (929) @ Episode 4/10000, loss: [0.0086637596]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 121 (930) @ Episode 4/10000, loss: [0.0083659329]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 122 (931) @ Episode 4/10000, loss: [0.0086656027]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 123 (932) @ Episode 4/10000, loss: [0.0078112558]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 124 (933) @ Episode 4/10000, loss: [0.0077119051]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 125 (934) @ Episode 4/10000, loss: [0.008308325]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 126 (935) @ Episode 4/10000, loss: [0.076652363]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 127 (936) @ Episode 4/10000, loss: [0.0097012222]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 128 (937) @ Episode 4/10000, loss: [0.045419805]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 129 (938) @ Episode 4/10000, loss: [0.006938355]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 130 (939) @ Episode 4/10000, loss: [0.0068580853]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 131 (940) @ Episode 4/10000, loss: [0.006682843]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 132 (941) @ Episode 4/10000, loss: [0.0068314569]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 133 (942) @ Episode 4/10000, loss: [0.0065746913]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 134 (943) @ Episode 4/10000, loss: [0.0083891591]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 135 (944) @ Episode 4/10000, loss: [0.0079288948]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 136 (945) @ Episode 4/10000, loss: [0.0076546189]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 137 (946) @ Episode 4/10000, loss: [0.0090484759]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 138 (947) @ Episode 4/10000, loss: [0.0073458776]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 139 (948) @ Episode 4/10000, loss: [0.0084391534]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 140 (949) @ Episode 4/10000, loss: [0.0066350568]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 141 (950) @ Episode 4/10000, loss: [0.0075957091]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 142 (951) @ Episode 4/10000, loss: [0.0088601597]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 143 (952) @ Episode 4/10000, loss: [0.0083651925]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 144 (953) @ Episode 4/10000, loss: [0.0071770148]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 145 (954) @ Episode 4/10000, loss: [0.0080576306]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 146 (955) @ Episode 4/10000, loss: [0.0078593586]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 147 (956) @ Episode 4/10000, loss: [0.0072439192]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 148 (957) @ Episode 4/10000, loss: [0.0060860822]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 149 (958) @ Episode 4/10000, loss: [0.008728195]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 150 (959) @ Episode 4/10000, loss: [0.007447199]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 151 (960) @ Episode 4/10000, loss: [0.0094839018]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 152 (961) @ Episode 4/10000, loss: [0.00877319]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 153 (962) @ Episode 4/10000, loss: [0.0071007111]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 154 (963) @ Episode 4/10000, loss: [0.0076145348]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 155 (964) @ Episode 4/10000, loss: [0.03925737]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 156 (965) @ Episode 4/10000, loss: [0.0086090025]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 157 (966) @ Episode 4/10000, loss: [0.0083950749]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 158 (967) @ Episode 4/10000, loss: [0.0083705895]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 159 (968) @ Episode 4/10000, loss: [0.0078127645]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 160 (969) @ Episode 4/10000, loss: [0.046705153]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 161 (970) @ Episode 4/10000, loss: [0.0072263717]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 162 (971) @ Episode 4/10000, loss: [0.0085889082]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 163 (972) @ Episode 4/10000, loss: [0.008703012]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 164 (973) @ Episode 4/10000, loss: [0.0069369823]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 165 (974) @ Episode 4/10000, loss: [0.037657771]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 166 (975) @ Episode 4/10000, loss: [0.0073460517]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 167 (976) @ Episode 4/10000, loss: [0.0075067254]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 168 (977) @ Episode 4/10000, loss: [0.0074809799]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 169 (978) @ Episode 4/10000, loss: [0.007532143]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 170 (979) @ Episode 4/10000, loss: [0.0071754078]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 171 (980) @ Episode 4/10000, loss: [0.0081224982]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 172 (981) @ Episode 4/10000, loss: [0.0065376316]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 173 (982) @ Episode 4/10000, loss: [0.046291813]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 174 (983) @ Episode 4/10000, loss: [0.0085602915]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 175 (984) @ Episode 4/10000, loss: [0.0066437963]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 176 (985) @ Episode 4/10000, loss: [0.0089887008]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 177 (986) @ Episode 4/10000, loss: [0.0060852249]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 178 (987) @ Episode 4/10000, loss: [0.0081156492]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 179 (988) @ Episode 4/10000, loss: [0.0069454154]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 180 (989) @ Episode 4/10000, loss: [0.0095449099]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 181 (990) @ Episode 4/10000, loss: [0.0083107613]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 182 (991) @ Episode 4/10000, loss: [0.0067054024]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 183 (992) @ Episode 4/10000, loss: [0.0073793577]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 184 (993) @ Episode 4/10000, loss: [0.0083236769]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 185 (994) @ Episode 4/10000, loss: [0.007462739]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 186 (995) @ Episode 4/10000, loss: [0.0076584038]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 187 (996) @ Episode 4/10000, loss: [0.0076584779]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 188 (997) @ Episode 4/10000, loss: [0.0086597428]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 189 (998) @ Episode 4/10000, loss: [0.0061945086]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 190 (999) @ Episode 4/10000, loss: [0.0084791984]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 191 (1000) @ Episode 4/10000, loss: [0.04606314]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 192 (1001) @ Episode 4/10000, loss: [0.0077628698]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 193 (1002) @ Episode 4/10000, loss: [0.008404525]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 194 (1003) @ Episode 4/10000, loss: [0.0081599941]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 195 (1004) @ Episode 4/10000, loss: [0.0079996334]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 196 (1005) @ Episode 4/10000, loss: [0.0082207825]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 197 (1006) @ Episode 4/10000, loss: [0.0078831669]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 198 (1007) @ Episode 4/10000, loss: [0.082697377]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 199 (1008) @ Episode 4/10000, loss: [0.0083701275]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 200 (1009) @ Episode 4/10000, loss: [0.0081106704]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 201 (1010) @ Episode 4/10000, loss: [0.0070576305]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 202 (1011) @ Episode 4/10000, loss: [0.0076636816]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 203 (1012) @ Episode 4/10000, loss: [0.008051645]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 204 (1013) @ Episode 4/10000, loss: [0.0076619769]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 205 (1014) @ Episode 4/10000, loss: [0.0087224729]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 206 (1015) @ Episode 4/10000, loss: [0.0077911979]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 207 (1016) @ Episode 4/10000, loss: [0.0081088245]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 208 (1017) @ Episode 4/10000, loss: [0.0081846397]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 209 (1018) @ Episode 4/10000, loss: [0.0051126364]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 210 (1019) @ Episode 4/10000, loss: [0.038868807]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 211 (1020) @ Episode 4/10000, loss: [0.008328015]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 212 (1021) @ Episode 4/10000, loss: [0.0086260773]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 213 (1022) @ Episode 4/10000, loss: [0.0077045085]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 214 (1023) @ Episode 4/10000, loss: [0.0075648944]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 215 (1024) @ Episode 4/10000, loss: [0.0077172965]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 216 (1025) @ Episode 4/10000, loss: [0.0085149072]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 217 (1026) @ Episode 4/10000, loss: [0.0075518503]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 218 (1027) @ Episode 4/10000, loss: [0.0065349382]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 219 (1028) @ Episode 4/10000, loss: [0.0081138201]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 220 (1029) @ Episode 4/10000, loss: [0.0086432965]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 221 (1030) @ Episode 4/10000, loss: [0.045537543]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 222 (1031) @ Episode 4/10000, loss: [0.0069699232]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 223 (1032) @ Episode 4/10000, loss: [0.0073882332]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 224 (1033) @ Episode 4/10000, loss: [0.0067936061]reward: 0.0\n",
      "reward_total: 1.0\n",
      "Step 225 (1034) @ Episode 4/10000, loss: [0.0073274872]reward: 1.0\n",
      "reward_total: 2.0\n",
      "Step 226 (1035) @ Episode 4/10000, loss: [0.0068527372]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 227 (1036) @ Episode 4/10000, loss: [0.0082461052]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 228 (1037) @ Episode 4/10000, loss: [0.044161912]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 229 (1038) @ Episode 4/10000, loss: [0.045191884]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 230 (1039) @ Episode 4/10000, loss: [0.038950425]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 231 (1040) @ Episode 4/10000, loss: [0.0085003236]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 232 (1041) @ Episode 4/10000, loss: [0.0060906708]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 233 (1042) @ Episode 4/10000, loss: [0.0066585941]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 234 (1043) @ Episode 4/10000, loss: [0.0079786014]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 235 (1044) @ Episode 4/10000, loss: [0.0081565659]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 236 (1045) @ Episode 4/10000, loss: [0.037899055]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 237 (1046) @ Episode 4/10000, loss: [0.0078308471]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 238 (1047) @ Episode 4/10000, loss: [0.0083283782]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 239 (1048) @ Episode 4/10000, loss: [0.0063357512]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 240 (1049) @ Episode 4/10000, loss: [0.007964449]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 241 (1050) @ Episode 4/10000, loss: [0.045313507]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 242 (1051) @ Episode 4/10000, loss: [0.0087225446]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 243 (1052) @ Episode 4/10000, loss: [0.006936837]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 244 (1053) @ Episode 4/10000, loss: [0.0070481542]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 245 (1054) @ Episode 4/10000, loss: [0.008549666]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 246 (1055) @ Episode 4/10000, loss: [0.039223321]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 247 (1056) @ Episode 4/10000, loss: [0.0072304634]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 248 (1057) @ Episode 4/10000, loss: [0.0065891393]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 249 (1058) @ Episode 4/10000, loss: [0.046065833]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 250 (1059) @ Episode 4/10000, loss: [0.0066440403]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 251 (1060) @ Episode 4/10000, loss: [0.0080152825]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 252 (1061) @ Episode 4/10000, loss: [0.0065591615]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 253 (1062) @ Episode 4/10000, loss: [0.0072022835]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 254 (1063) @ Episode 4/10000, loss: [0.0086789429]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 255 (1064) @ Episode 4/10000, loss: [0.0071268398]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 256 (1065) @ Episode 4/10000, loss: [0.0075429822]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 257 (1066) @ Episode 4/10000, loss: [0.0089110676]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 258 (1067) @ Episode 4/10000, loss: [0.0066744047]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 259 (1068) @ Episode 4/10000, loss: [0.008648593]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 260 (1069) @ Episode 4/10000, loss: [0.0091430591]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 261 (1070) @ Episode 4/10000, loss: [0.0081023108]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 262 (1071) @ Episode 4/10000, loss: [0.0084207505]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 263 (1072) @ Episode 4/10000, loss: [0.0081241876]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 264 (1073) @ Episode 4/10000, loss: [0.0078848908]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 265 (1074) @ Episode 4/10000, loss: [0.0064827856]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 266 (1075) @ Episode 4/10000, loss: [0.0076145465]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 267 (1076) @ Episode 4/10000, loss: [0.0061566075]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 268 (1077) @ Episode 4/10000, loss: [0.0067556989]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 269 (1078) @ Episode 4/10000, loss: [0.0087423697]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 270 (1079) @ Episode 4/10000, loss: [0.0078259166]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 271 (1080) @ Episode 4/10000, loss: [0.0067531015]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 272 (1081) @ Episode 4/10000, loss: [0.0077469479]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 273 (1082) @ Episode 4/10000, loss: [0.040582806]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 274 (1083) @ Episode 4/10000, loss: [0.0073904004]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 275 (1084) @ Episode 4/10000, loss: [0.007734146]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 276 (1085) @ Episode 4/10000, loss: [0.0078840628]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 277 (1086) @ Episode 4/10000, loss: [0.0080907168]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 278 (1087) @ Episode 4/10000, loss: [0.038173012]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 279 (1088) @ Episode 4/10000, loss: [0.007459397]reward: 0.0\n",
      "reward_total: 2.0\n",
      "Step 280 (1089) @ Episode 4/10000, loss: [0.0079300869]reward: 1.0\n",
      "reward_total: 3.0\n",
      "Step 281 (1090) @ Episode 4/10000, loss: [0.0064698178]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 282 (1091) @ Episode 4/10000, loss: [0.0071436446]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 283 (1092) @ Episode 4/10000, loss: [0.0067839799]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 284 (1093) @ Episode 4/10000, loss: [0.0067792828]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 285 (1094) @ Episode 4/10000, loss: [0.0076562609]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 286 (1095) @ Episode 4/10000, loss: [0.006927778]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 287 (1096) @ Episode 4/10000, loss: [0.0063488795]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 288 (1097) @ Episode 4/10000, loss: [0.0076732943]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 289 (1098) @ Episode 4/10000, loss: [0.0064318087]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 290 (1099) @ Episode 4/10000, loss: [0.0074266158]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 291 (1100) @ Episode 4/10000, loss: [0.0069154049]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 292 (1101) @ Episode 4/10000, loss: [0.0056765834]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 293 (1102) @ Episode 4/10000, loss: [0.0076203048]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 294 (1103) @ Episode 4/10000, loss: [0.0089622634]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 295 (1104) @ Episode 4/10000, loss: [0.00759868]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 296 (1105) @ Episode 4/10000, loss: [0.006550177]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 297 (1106) @ Episode 4/10000, loss: [0.0076308507]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 298 (1107) @ Episode 4/10000, loss: [0.0073670242]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 299 (1108) @ Episode 4/10000, loss: [0.0071351309]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 300 (1109) @ Episode 4/10000, loss: [0.007039587]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 301 (1110) @ Episode 4/10000, loss: [0.0075204866]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 302 (1111) @ Episode 4/10000, loss: [0.0068271696]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 303 (1112) @ Episode 4/10000, loss: [0.0083686644]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 304 (1113) @ Episode 4/10000, loss: [0.0087726153]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 305 (1114) @ Episode 4/10000, loss: [0.0074252207]reward: 0.0\n",
      "reward_total: 3.0\n",
      "Step 306 (1115) @ Episode 4/10000, loss: [0.0085916771]reward: 0.0\n",
      "reward_total: 3.0\n",
      "done: True\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 0 (1115) @ Episode 5/10000, loss: Nonereward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 1 (1116) @ Episode 5/10000, loss: [0.0083326157]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 2 (1117) @ Episode 5/10000, loss: [0.008316759]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 3 (1118) @ Episode 5/10000, loss: [0.0078804931]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 4 (1119) @ Episode 5/10000, loss: [0.0085748732]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 5 (1120) @ Episode 5/10000, loss: [0.0076028649]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 6 (1121) @ Episode 5/10000, loss: [0.0075493436]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 7 (1122) @ Episode 5/10000, loss: [0.0086465068]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 8 (1123) @ Episode 5/10000, loss: [0.044659276]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 9 (1124) @ Episode 5/10000, loss: [0.044633035]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 10 (1125) @ Episode 5/10000, loss: [0.0088444781]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 11 (1126) @ Episode 5/10000, loss: [0.0084705595]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 12 (1127) @ Episode 5/10000, loss: [0.007706712]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 13 (1128) @ Episode 5/10000, loss: [0.0071144258]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 14 (1129) @ Episode 5/10000, loss: [0.0078667803]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 15 (1130) @ Episode 5/10000, loss: [0.0076836585]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 16 (1131) @ Episode 5/10000, loss: [0.0078931116]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 17 (1132) @ Episode 5/10000, loss: [0.0071123498]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 18 (1133) @ Episode 5/10000, loss: [0.0079712775]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 19 (1134) @ Episode 5/10000, loss: [0.046328675]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 20 (1135) @ Episode 5/10000, loss: [0.0088039581]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 21 (1136) @ Episode 5/10000, loss: [0.0078797862]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 22 (1137) @ Episode 5/10000, loss: [0.0089535955]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 23 (1138) @ Episode 5/10000, loss: [0.0075665042]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 24 (1139) @ Episode 5/10000, loss: [0.0080791079]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 25 (1140) @ Episode 5/10000, loss: [0.0093089473]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 26 (1141) @ Episode 5/10000, loss: [0.0099354163]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 27 (1142) @ Episode 5/10000, loss: [0.0084258504]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 28 (1143) @ Episode 5/10000, loss: [0.044339471]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 29 (1144) @ Episode 5/10000, loss: [0.0062339231]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 30 (1145) @ Episode 5/10000, loss: [0.0083590951]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 31 (1146) @ Episode 5/10000, loss: [0.043177467]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 32 (1147) @ Episode 5/10000, loss: [0.045213845]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 33 (1148) @ Episode 5/10000, loss: [0.0091874152]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 34 (1149) @ Episode 5/10000, loss: [0.0066264085]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 35 (1150) @ Episode 5/10000, loss: [0.0075798947]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 36 (1151) @ Episode 5/10000, loss: [0.0069624847]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 37 (1152) @ Episode 5/10000, loss: [0.0086917765]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 38 (1153) @ Episode 5/10000, loss: [0.0095990999]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 39 (1154) @ Episode 5/10000, loss: [0.007605724]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 40 (1155) @ Episode 5/10000, loss: [0.0078892848]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 41 (1156) @ Episode 5/10000, loss: [0.045169033]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 42 (1157) @ Episode 5/10000, loss: [0.0079243332]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 43 (1158) @ Episode 5/10000, loss: [0.0069984868]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 44 (1159) @ Episode 5/10000, loss: [0.045836233]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 45 (1160) @ Episode 5/10000, loss: [0.007477928]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 46 (1161) @ Episode 5/10000, loss: [0.046373986]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 47 (1162) @ Episode 5/10000, loss: [0.0067166467]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 48 (1163) @ Episode 5/10000, loss: [0.0087599205]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 49 (1164) @ Episode 5/10000, loss: [0.0081542227]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 50 (1165) @ Episode 5/10000, loss: [0.0086093135]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 51 (1166) @ Episode 5/10000, loss: [0.045598302]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 52 (1167) @ Episode 5/10000, loss: [0.0070803473]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 53 (1168) @ Episode 5/10000, loss: [0.0059552616]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 54 (1169) @ Episode 5/10000, loss: [0.045493942]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 55 (1170) @ Episode 5/10000, loss: [0.047377776]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 56 (1171) @ Episode 5/10000, loss: [0.045616616]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 57 (1172) @ Episode 5/10000, loss: [0.0080023259]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 58 (1173) @ Episode 5/10000, loss: [0.044835344]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 59 (1174) @ Episode 5/10000, loss: [0.0074741039]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 60 (1175) @ Episode 5/10000, loss: [0.0077465]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 61 (1176) @ Episode 5/10000, loss: [0.0070470232]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 62 (1177) @ Episode 5/10000, loss: [0.0075114872]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 63 (1178) @ Episode 5/10000, loss: [0.0071933856]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 64 (1179) @ Episode 5/10000, loss: [0.039173391]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 65 (1180) @ Episode 5/10000, loss: [0.0077007785]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 66 (1181) @ Episode 5/10000, loss: [0.0073488653]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 67 (1182) @ Episode 5/10000, loss: [0.0085445587]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 68 (1183) @ Episode 5/10000, loss: [0.0078292973]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 69 (1184) @ Episode 5/10000, loss: [0.0095228693]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 70 (1185) @ Episode 5/10000, loss: [0.0079861321]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 71 (1186) @ Episode 5/10000, loss: [0.044916619]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 72 (1187) @ Episode 5/10000, loss: [0.0083831381]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 73 (1188) @ Episode 5/10000, loss: [0.0076756747]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 74 (1189) @ Episode 5/10000, loss: [0.0072604562]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 75 (1190) @ Episode 5/10000, loss: [0.0070546367]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 76 (1191) @ Episode 5/10000, loss: [0.0071573267]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 77 (1192) @ Episode 5/10000, loss: [0.044977337]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 78 (1193) @ Episode 5/10000, loss: [0.0078968238]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 79 (1194) @ Episode 5/10000, loss: [0.0092276717]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 80 (1195) @ Episode 5/10000, loss: [0.043685798]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 81 (1196) @ Episode 5/10000, loss: [0.0087313261]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 82 (1197) @ Episode 5/10000, loss: [0.0080597643]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 83 (1198) @ Episode 5/10000, loss: [0.039632678]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 84 (1199) @ Episode 5/10000, loss: [0.0064827716]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 85 (1200) @ Episode 5/10000, loss: [0.046280112]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 86 (1201) @ Episode 5/10000, loss: [0.00833405]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 87 (1202) @ Episode 5/10000, loss: [0.0078281797]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 88 (1203) @ Episode 5/10000, loss: [0.007488857]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 89 (1204) @ Episode 5/10000, loss: [0.04472417]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 90 (1205) @ Episode 5/10000, loss: [0.0079217348]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 91 (1206) @ Episode 5/10000, loss: [0.0074764048]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 92 (1207) @ Episode 5/10000, loss: [0.009191528]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 93 (1208) @ Episode 5/10000, loss: [0.0075419997]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 94 (1209) @ Episode 5/10000, loss: [0.0079539288]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 95 (1210) @ Episode 5/10000, loss: [0.0086872075]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 96 (1211) @ Episode 5/10000, loss: [0.04664252]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 97 (1212) @ Episode 5/10000, loss: [0.0073394049]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 98 (1213) @ Episode 5/10000, loss: [0.0072527989]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 99 (1214) @ Episode 5/10000, loss: [0.0088141132]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 100 (1215) @ Episode 5/10000, loss: [0.0079601929]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 101 (1216) @ Episode 5/10000, loss: [0.0074217459]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 102 (1217) @ Episode 5/10000, loss: [0.046248563]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 103 (1218) @ Episode 5/10000, loss: [0.0083131753]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 104 (1219) @ Episode 5/10000, loss: [0.0077239079]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 105 (1220) @ Episode 5/10000, loss: [0.0074284207]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 106 (1221) @ Episode 5/10000, loss: [0.0085892575]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 107 (1222) @ Episode 5/10000, loss: [0.0090634571]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 108 (1223) @ Episode 5/10000, loss: [0.0083044944]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 109 (1224) @ Episode 5/10000, loss: [0.0080686314]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 110 (1225) @ Episode 5/10000, loss: [0.0088180751]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 111 (1226) @ Episode 5/10000, loss: [0.038570635]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 112 (1227) @ Episode 5/10000, loss: [0.0059004445]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 113 (1228) @ Episode 5/10000, loss: [0.0074942117]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 114 (1229) @ Episode 5/10000, loss: [0.006327623]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 115 (1230) @ Episode 5/10000, loss: [0.0085467184]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 116 (1231) @ Episode 5/10000, loss: [0.045871768]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 117 (1232) @ Episode 5/10000, loss: [0.0082847457]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 118 (1233) @ Episode 5/10000, loss: [0.0076957978]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 119 (1234) @ Episode 5/10000, loss: [0.007550722]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 120 (1235) @ Episode 5/10000, loss: [0.0089466125]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 121 (1236) @ Episode 5/10000, loss: [0.0077477288]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 122 (1237) @ Episode 5/10000, loss: [0.0075836689]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 123 (1238) @ Episode 5/10000, loss: [0.0081326179]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 124 (1239) @ Episode 5/10000, loss: [0.043934975]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 125 (1240) @ Episode 5/10000, loss: [0.0088712014]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 126 (1241) @ Episode 5/10000, loss: [0.0073522413]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 127 (1242) @ Episode 5/10000, loss: [0.0067029521]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 128 (1243) @ Episode 5/10000, loss: [0.0090149045]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 129 (1244) @ Episode 5/10000, loss: [0.0089541068]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 130 (1245) @ Episode 5/10000, loss: [0.0088058142]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 131 (1246) @ Episode 5/10000, loss: [0.0090380516]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 132 (1247) @ Episode 5/10000, loss: [0.0055053802]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 133 (1248) @ Episode 5/10000, loss: [0.0078923153]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 134 (1249) @ Episode 5/10000, loss: [0.0075870357]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 135 (1250) @ Episode 5/10000, loss: [0.045488838]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 136 (1251) @ Episode 5/10000, loss: [0.0079670567]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 137 (1252) @ Episode 5/10000, loss: [0.0078682639]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 138 (1253) @ Episode 5/10000, loss: [0.0069464166]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 139 (1254) @ Episode 5/10000, loss: [0.008356425]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 140 (1255) @ Episode 5/10000, loss: [0.039113287]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 141 (1256) @ Episode 5/10000, loss: [0.040649809]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 142 (1257) @ Episode 5/10000, loss: [0.0080215074]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 143 (1258) @ Episode 5/10000, loss: [0.0082483459]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 144 (1259) @ Episode 5/10000, loss: [0.0071690436]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 145 (1260) @ Episode 5/10000, loss: [0.0076834108]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 146 (1261) @ Episode 5/10000, loss: [0.04622383]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 147 (1262) @ Episode 5/10000, loss: [0.0087585114]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 148 (1263) @ Episode 5/10000, loss: [0.0082454327]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 149 (1264) @ Episode 5/10000, loss: [0.0082846237]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 150 (1265) @ Episode 5/10000, loss: [0.0077348826]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 151 (1266) @ Episode 5/10000, loss: [0.0068001216]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 152 (1267) @ Episode 5/10000, loss: [0.037923008]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 153 (1268) @ Episode 5/10000, loss: [0.0080422498]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 154 (1269) @ Episode 5/10000, loss: [0.0079027284]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 155 (1270) @ Episode 5/10000, loss: [0.0080462107]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 156 (1271) @ Episode 5/10000, loss: [0.0094931088]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 157 (1272) @ Episode 5/10000, loss: [0.0085098492]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 158 (1273) @ Episode 5/10000, loss: [0.0079255123]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 159 (1274) @ Episode 5/10000, loss: [0.0082362704]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 160 (1275) @ Episode 5/10000, loss: [0.0078499699]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 161 (1276) @ Episode 5/10000, loss: [0.0075883376]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 162 (1277) @ Episode 5/10000, loss: [0.0083391089]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 163 (1278) @ Episode 5/10000, loss: [0.0088789463]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 164 (1279) @ Episode 5/10000, loss: [0.0064883181]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 165 (1280) @ Episode 5/10000, loss: [0.044450369]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 166 (1281) @ Episode 5/10000, loss: [0.0087548653]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 167 (1282) @ Episode 5/10000, loss: [0.045776121]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 168 (1283) @ Episode 5/10000, loss: [0.0081380662]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 169 (1284) @ Episode 5/10000, loss: [0.0084529333]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 170 (1285) @ Episode 5/10000, loss: [0.0069700638]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 171 (1286) @ Episode 5/10000, loss: [0.0063332748]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 172 (1287) @ Episode 5/10000, loss: [0.0079859402]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 173 (1288) @ Episode 5/10000, loss: [0.0086712092]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 174 (1289) @ Episode 5/10000, loss: [0.0071578128]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 175 (1290) @ Episode 5/10000, loss: [0.0094481725]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 176 (1291) @ Episode 5/10000, loss: [0.0061854059]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 177 (1292) @ Episode 5/10000, loss: [0.0080870725]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 178 (1293) @ Episode 5/10000, loss: [0.046642218]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 179 (1294) @ Episode 5/10000, loss: [0.0082211792]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 180 (1295) @ Episode 5/10000, loss: [0.0072936406]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 181 (1296) @ Episode 5/10000, loss: [0.0080604106]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 182 (1297) @ Episode 5/10000, loss: [0.0078645153]reward: 0.0\n",
      "reward_total: 0.0\n",
      "Step 183 (1298) @ Episode 5/10000, loss: [0.0085064229]reward: 0.0\n",
      "reward_total: 0.0\n",
      "done: True\n",
      "\n",
      "Episode Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "#with g.as_default(), tf.Session(config=config) as sess:\n",
    "with g.as_default(), tf.Session(config=config) as sess, tf.device('/cpu:0'):\n",
    "    # Where we save our checkpoints and graphs\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "    # Create a glboal step variable\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    # Create estimators\n",
    "    q_estimator = Estimator(scope=\"q\")\n",
    "    #q_estimator = Estimator(scope=\"q\")\n",
    "    target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "    # State processor\n",
    "    state_processor = StateProcessor()\n",
    "\n",
    "    # Run it!\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Starts here    \n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    jList = []\n",
    "    rList = []\n",
    "    total_t = 0\n",
    "\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    #checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    #checkpoint_path = os.path.join(experiment_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "\n",
    "    print(\"Import saved model\")\n",
    "    new_saver = tf.train.import_meta_graph('model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "    #for i in range(300):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    " \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        #saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        saver.save(tf.get_default_session(), 'model')\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "    \n",
    "        r_episode = 0\n",
    "        j_episode = 0\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "          \n",
    "            # Add epsilon to Tensorboard\n",
    "            #episode_summary = tf.Summary()\n",
    "            #episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            #q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            r_episode += reward\n",
    "            print('reward:', reward)\n",
    "            print('reward_total:', r_episode)\n",
    "            j_episode += t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "           \n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            #tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "            #print()\n",
    "\n",
    "            #print('reward', reward_batch)\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            \n",
    "            if done:\n",
    "                targets_batch = reward_batch \n",
    "            else: \n",
    "                targets_batch = reward_batch + discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            #print('state batch:', len(states_batch))\n",
    "            states_batch = np.array(states_batch)\n",
    "            #print('np.array:', len(states_batch))\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                print('done:', done)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        #episode_summary = tf.Summary()\n",
    "        #episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        #episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        #q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        #q_estimator.summary_writer.flush()\n",
    "        jList.append(j_episode)\n",
    "        rList.append(r_episode)\n",
    "        print(\"\\nEpisode Reward: {}\".format(r_episode))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
