{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# File: train-atari.py\n",
    "# Author: Yuxin Wu <ppwwyyxxc@gmail.com>\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import threading\n",
    "import cv2\n",
    "from collections import deque\n",
    "import six\n",
    "from six.moves import queue\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorpack import *\n",
    "from tensorpack.utils.concurrency import *\n",
    "from tensorpack.utils.serialize import *\n",
    "from tensorpack.utils.stats import *\n",
    "from tensorpack.tfutils import symbolic_functions as symbf\n",
    "from tensorpack.tfutils.gradproc import MapGradient, SummaryGradient\n",
    "\n",
    "from tensorpack.RL import *\n",
    "from simulator import *\n",
    "import common\n",
    "from common import (play_model, Evaluator, eval_model_multithread)\n",
    "\n",
    "IMAGE_SIZE = (84, 84)\n",
    "FRAME_HISTORY = 4\n",
    "GAMMA = 0.99\n",
    "CHANNEL = FRAME_HISTORY * 3\n",
    "IMAGE_SHAPE3 = IMAGE_SIZE + (CHANNEL,)\n",
    "\n",
    "LOCAL_TIME_MAX = 5\n",
    "STEPS_PER_EPOCH = 6000\n",
    "EVAL_EPISODE = 50\n",
    "BATCH_SIZE = 128\n",
    "SIMULATOR_PROC = 50\n",
    "PREDICTOR_THREAD_PER_GPU = 2\n",
    "PREDICTOR_THREAD = None\n",
    "EVALUATE_PROC = min(multiprocessing.cpu_count() // 2, 20)\n",
    "\n",
    "NUM_ACTIONS = None\n",
    "ENV_NAME = None\n",
    "\n",
    "\n",
    "def get_player(viz=False, train=False, dumpdir=None):\n",
    "    pl = GymEnv(ENV_NAME, dumpdir=dumpdir)\n",
    "\n",
    "    def func(img):\n",
    "        return cv2.resize(img, IMAGE_SIZE[::-1])\n",
    "    pl = MapPlayerState(pl, func)\n",
    "\n",
    "    global NUM_ACTIONS\n",
    "    NUM_ACTIONS = pl.get_action_space().num_actions()\n",
    "\n",
    "    pl = HistoryFramePlayer(pl, FRAME_HISTORY)\n",
    "    if not train:\n",
    "        pl = PreventStuckPlayer(pl, 30, 1)\n",
    "    pl = LimitLengthPlayer(pl, 40000)\n",
    "    return pl\n",
    "\n",
    "\n",
    "common.get_player = get_player\n",
    "\n",
    "\n",
    "class MySimulatorWorker(SimulatorProcess):\n",
    "\n",
    "    def _build_player(self):\n",
    "        return get_player(train=True)\n",
    "\n",
    "\n",
    "class Model(ModelDesc):\n",
    "    def _get_inputs(self):\n",
    "        assert NUM_ACTIONS is not None\n",
    "        return [InputDesc(tf.float32, (None,) + IMAGE_SHAPE3, 'state'),\n",
    "                InputDesc(tf.int64, (None,), 'action'),\n",
    "                InputDesc(tf.float32, (None,), 'futurereward')]\n",
    "\n",
    "    def _get_NN_prediction(self, image):\n",
    "        image = image / 255.0\n",
    "        with argscope(Conv2D, nl=tf.nn.relu):\n",
    "            l = Conv2D('conv0', image, out_channel=32, kernel_shape=5)\n",
    "            l = MaxPooling('pool0', l, 2)\n",
    "            l = Conv2D('conv1', l, out_channel=32, kernel_shape=5)\n",
    "            l = MaxPooling('pool1', l, 2)\n",
    "            l = Conv2D('conv2', l, out_channel=64, kernel_shape=4)\n",
    "            l = MaxPooling('pool2', l, 2)\n",
    "            l = Conv2D('conv3', l, out_channel=64, kernel_shape=3)\n",
    "\n",
    "        l = FullyConnected('fc0', l, 512, nl=tf.identity)\n",
    "        l = PReLU('prelu', l)\n",
    "        logits = FullyConnected('fc-pi', l, out_dim=NUM_ACTIONS, nl=tf.identity)    # unnormalized policy\n",
    "        value = FullyConnected('fc-v', l, 1, nl=tf.identity)\n",
    "        return logits, value\n",
    "\n",
    "    def _build_graph(self, inputs):\n",
    "        state, action, futurereward = inputs\n",
    "        logits, self.value = self._get_NN_prediction(state)\n",
    "        self.value = tf.squeeze(self.value, [1], name='pred_value')  # (B,)\n",
    "        self.policy = tf.nn.softmax(logits, name='policy')\n",
    "\n",
    "        expf = tf.get_variable('explore_factor', shape=[],\n",
    "                               initializer=tf.constant_initializer(1), trainable=False)\n",
    "        policy_explore = tf.nn.softmax(logits * expf, name='policy_explore')\n",
    "        is_training = get_current_tower_context().is_training\n",
    "        if not is_training:\n",
    "            return\n",
    "        log_probs = tf.log(self.policy + 1e-6)\n",
    "\n",
    "        log_pi_a_given_s = tf.reduce_sum(\n",
    "            log_probs * tf.one_hot(action, NUM_ACTIONS), 1)\n",
    "        advantage = tf.subtract(tf.stop_gradient(self.value), futurereward, name='advantage')\n",
    "        policy_loss = tf.reduce_sum(log_pi_a_given_s * advantage, name='policy_loss')\n",
    "        xentropy_loss = tf.reduce_sum(\n",
    "            self.policy * log_probs, name='xentropy_loss')\n",
    "        value_loss = tf.nn.l2_loss(self.value - futurereward, name='value_loss')\n",
    "\n",
    "        pred_reward = tf.reduce_mean(self.value, name='predict_reward')\n",
    "        advantage = symbf.rms(advantage, name='rms_advantage')\n",
    "        entropy_beta = tf.get_variable('entropy_beta', shape=[],\n",
    "                                       initializer=tf.constant_initializer(0.01), trainable=False)\n",
    "        self.cost = tf.add_n([policy_loss, xentropy_loss * entropy_beta, value_loss])\n",
    "        self.cost = tf.truediv(self.cost,\n",
    "                               tf.cast(tf.shape(futurereward)[0], tf.float32),\n",
    "                               name='cost')\n",
    "        summary.add_moving_summary(policy_loss, xentropy_loss,\n",
    "                                   value_loss, pred_reward, advantage, self.cost)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        lr = symbf.get_scalar_var('learning_rate', 0.001, summary=True)\n",
    "        opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)\n",
    "\n",
    "        gradprocs = [MapGradient(lambda grad: tf.clip_by_average_norm(grad, 0.1)),\n",
    "                     SummaryGradient()]\n",
    "        opt = optimizer.apply_grad_processors(opt, gradprocs)\n",
    "        return opt\n",
    "\n",
    "\n",
    "class MySimulatorMaster(SimulatorMaster, Callback):\n",
    "    def __init__(self, pipe_c2s, pipe_s2c, model):\n",
    "        super(MySimulatorMaster, self).__init__(pipe_c2s, pipe_s2c)\n",
    "        self.M = model\n",
    "        self.queue = queue.Queue(maxsize=BATCH_SIZE * 8 * 2)\n",
    "\n",
    "    def _setup_graph(self):\n",
    "        self.async_predictor = MultiThreadAsyncPredictor(\n",
    "            self.trainer.get_predictors(['state'], ['policy_explore', 'pred_value'],\n",
    "                                        PREDICTOR_THREAD), batch_size=15)\n",
    "\n",
    "    def _before_train(self):\n",
    "        self.async_predictor.start()\n",
    "\n",
    "    def _on_state(self, state, ident):\n",
    "        def cb(outputs):\n",
    "            distrib, value = outputs.result()\n",
    "            assert np.all(np.isfinite(distrib)), distrib\n",
    "            action = np.random.choice(len(distrib), p=distrib)\n",
    "            client = self.clients[ident]\n",
    "            client.memory.append(TransitionExperience(state, action, None, value=value))\n",
    "            self.send_queue.put([ident, dumps(action)])\n",
    "        self.async_predictor.put_task([state], cb)\n",
    "\n",
    "    def _on_episode_over(self, ident):\n",
    "        self._parse_memory(0, ident, True)\n",
    "\n",
    "    def _on_datapoint(self, ident):\n",
    "        client = self.clients[ident]\n",
    "        if len(client.memory) == LOCAL_TIME_MAX + 1:\n",
    "            R = client.memory[-1].value\n",
    "            self._parse_memory(R, ident, False)\n",
    "\n",
    "    def _parse_memory(self, init_r, ident, isOver):\n",
    "        client = self.clients[ident]\n",
    "        mem = client.memory\n",
    "        if not isOver:\n",
    "            last = mem[-1]\n",
    "            mem = mem[:-1]\n",
    "\n",
    "        mem.reverse()\n",
    "        R = float(init_r)\n",
    "        for idx, k in enumerate(mem):\n",
    "            R = np.clip(k.reward, -1, 1) + GAMMA * R\n",
    "            self.queue.put([k.state, k.action, R])\n",
    "\n",
    "        if not isOver:\n",
    "            client.memory = [last]\n",
    "        else:\n",
    "            client.memory = []\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    dirname = os.path.join('train_log', 'train-atari-{}'.format(ENV_NAME))\n",
    "    logger.set_logger_dir(dirname)\n",
    "    M = Model()\n",
    "\n",
    "    name_base = str(uuid.uuid1())[:6]\n",
    "    PIPE_DIR = os.environ.get('TENSORPACK_PIPEDIR', '.').rstrip('/')\n",
    "    namec2s = 'ipc://{}/sim-c2s-{}'.format(PIPE_DIR, name_base)\n",
    "    names2c = 'ipc://{}/sim-s2c-{}'.format(PIPE_DIR, name_base)\n",
    "    procs = [MySimulatorWorker(k, namec2s, names2c) for k in range(SIMULATOR_PROC)]\n",
    "    ensure_proc_terminate(procs)\n",
    "    start_proc_mask_signal(procs)\n",
    "\n",
    "    master = MySimulatorMaster(namec2s, names2c, M)\n",
    "    dataflow = BatchData(DataFromQueue(master.queue), BATCH_SIZE)\n",
    "    return TrainConfig(\n",
    "        dataflow=dataflow,\n",
    "        callbacks=[\n",
    "            ModelSaver(),\n",
    "            ScheduledHyperParamSetter('learning_rate', [(80, 0.0003), (120, 0.0001)]),\n",
    "            ScheduledHyperParamSetter('entropy_beta', [(80, 0.005)]),\n",
    "            ScheduledHyperParamSetter('explore_factor',\n",
    "                                      [(80, 2), (100, 3), (120, 4), (140, 5)]),\n",
    "            master,\n",
    "            StartProcOrThread(master),\n",
    "            PeriodicTrigger(Evaluator(EVAL_EPISODE, ['state'], ['policy']), every_k_epochs=2),\n",
    "        ],\n",
    "        session_creator=sesscreate.NewSessionCreator(\n",
    "            config=get_default_sess_config(0.5)),\n",
    "        model=M,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        max_epoch=1000,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.')\n",
    "    parser.add_argument('--load', help='load model')\n",
    "    parser.add_argument('--env', help='env', required=True)\n",
    "    parser.add_argument('--task', help='task to perform',\n",
    "                        choices=['play', 'eval', 'train'], default='train')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ENV_NAME = args.env\n",
    "    assert ENV_NAME\n",
    "    p = get_player()\n",
    "    del p    # set NUM_ACTIONS\n",
    "\n",
    "    if args.gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    if args.task != 'train':\n",
    "        assert args.load is not None\n",
    "\n",
    "    if args.task != 'train':\n",
    "        cfg = PredictConfig(\n",
    "            model=Model(),\n",
    "            session_init=SaverRestore(args.load),\n",
    "            input_names=['state'],\n",
    "            output_names=['policy'])\n",
    "        if args.task == 'play':\n",
    "            play_model(cfg)\n",
    "        elif args.task == 'eval':\n",
    "            eval_model_multithread(cfg, EVAL_EPISODE)\n",
    "    else:\n",
    "        nr_gpu = get_nr_gpu()\n",
    "        if nr_gpu > 0:\n",
    "            if nr_gpu > 1:\n",
    "                predict_tower = list(range(nr_gpu))[-nr_gpu // 2:]\n",
    "            else:\n",
    "                predict_tower = [0]\n",
    "            PREDICTOR_THREAD = len(predict_tower) * PREDICTOR_THREAD_PER_GPU\n",
    "            train_tower = list(range(nr_gpu))[:-nr_gpu // 2] or [0]\n",
    "            logger.info(\"[BA3C] Train on gpu {} and infer on gpu {}\".format(\n",
    "                ','.join(map(str, train_tower)), ','.join(map(str, predict_tower))))\n",
    "            trainer = AsyncMultiGPUTrainer\n",
    "        else:\n",
    "            logger.warn(\"Without GPU this model will never learn! CPU is only useful for debug.\")\n",
    "            nr_gpu = 0\n",
    "            PREDICTOR_THREAD = 1\n",
    "            predict_tower, train_tower = [0], [0]\n",
    "            trainer = QueueInputTrainer\n",
    "        config = get_config()\n",
    "        if args.load:\n",
    "            config.session_init = SaverRestore(args.load)\n",
    "        config.tower = train_tower\n",
    "        config.predict_tower = predict_tower\n",
    "        trainer(config).train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
