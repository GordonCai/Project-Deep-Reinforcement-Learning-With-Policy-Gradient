{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class PolicyGradientREINFORCE(object):\n",
    "\n",
    "    def __init__(self, session,\n",
    "                     optimizer,\n",
    "                     policy_network,\n",
    "                     state_dim,\n",
    "                     num_actions,\n",
    "                     init_exp=0.5,         # initial exploration prob\n",
    "                     final_exp=0.0,        # final exploration prob\n",
    "                     anneal_steps=10000,   # N steps for annealing exploration\n",
    "                     discount_factor=0.99, # discount future rewards\n",
    "                     reg_param=0.001,      # regularization constants\n",
    "                     max_gradient=5,       # max gradient norms\n",
    "                     summary_writer=None,\n",
    "                     summary_every=100):\n",
    "\n",
    "        # tensorflow machinery\n",
    "        self.session        = session\n",
    "        self.optimizer      = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # model components\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "        # training parameters\n",
    "        self.state_dim       = state_dim\n",
    "        self.num_actions     = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_gradient    = max_gradient\n",
    "        self.reg_param       = reg_param\n",
    "\n",
    "        # exploration parameters\n",
    "        self.exploration  = init_exp\n",
    "        self.init_exp     = init_exp\n",
    "        self.final_exp    = final_exp\n",
    "        self.anneal_steps = anneal_steps\n",
    "\n",
    "        # counters\n",
    "        self.train_iteration = 0\n",
    "\n",
    "        # rollout buffer\n",
    "        self.state_buffer  = []\n",
    "        self.reward_buffer = []\n",
    "        self.action_buffer = []\n",
    "\n",
    "        # record reward history for normalization\n",
    "        self.all_rewards = []\n",
    "        self.max_reward_length = 1000000\n",
    "\n",
    "        # create and initialize variables\n",
    "        self.create_variables()\n",
    "        var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "        # make sure all variables are initialized\n",
    "        self.session.run(tf.assert_variables_initialized())\n",
    "\n",
    "        if self.summary_writer is not None:\n",
    "            # graph was not available when journalist was created\n",
    "            self.summary_writer.add_graph(self.session.graph)\n",
    "            self.summary_every = summary_every\n",
    "\n",
    "    def resetModel(self):\n",
    "        self.cleanUp()\n",
    "        self.train_iteration = 0\n",
    "        self.exploration     = self.init_exp\n",
    "        var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "        self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "    def create_variables(self):\n",
    "\n",
    "        with tf.name_scope(\"model_inputs\"):\n",
    "            # raw state representation\n",
    "            self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"states\")\n",
    "\n",
    "        # rollout action based on current policy\n",
    "        with tf.name_scope(\"predict_actions\"):\n",
    "            # initialize policy network\n",
    "            with tf.variable_scope(\"policy_network\"):\n",
    "                self.policy_outputs = self.policy_network(self.states)\n",
    "\n",
    "            # predict actions from policy network\n",
    "            self.action_scores = tf.identity(self.policy_outputs, name=\"action_scores\")\n",
    "            # Note 1: tf.multinomial is not good enough to use yet\n",
    "            # so we don't use self.predicted_actions for now\n",
    "            self.predicted_actions = tf.multinomial(self.action_scores, 1)\n",
    "\n",
    "        # regularization loss\n",
    "        policy_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"policy_network\")\n",
    "\n",
    "        # compute loss and gradients\n",
    "        with tf.name_scope(\"compute_pg_gradients\"):\n",
    "            # gradients for selecting action from policy network\n",
    "            self.taken_actions = tf.placeholder(tf.int32, (None,), name=\"taken_actions\")\n",
    "            self.discounted_rewards = tf.placeholder(tf.float32, (None,), name=\"discounted_rewards\")\n",
    "\n",
    "            with tf.variable_scope(\"policy_network\", reuse=True):\n",
    "                self.logprobs = self.policy_network(self.states)\n",
    "\n",
    "            # compute policy loss and regularization loss\n",
    "            self.cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(self.logprobs, self.taken_actions)\n",
    "            self.pg_loss            = tf.reduce_mean(self.cross_entropy_loss)\n",
    "            self.reg_loss           = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_network_variables])\n",
    "            self.loss               = self.pg_loss + self.reg_param * self.reg_loss\n",
    "\n",
    "            # compute gradients\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss)\n",
    "\n",
    "            # compute policy gradients\n",
    "            for i, (grad, var) in enumerate(self.gradients):\n",
    "                if grad is not None:\n",
    "                    self.gradients[i] = (grad * self.discounted_rewards, var)\n",
    "\n",
    "            for grad, var in self.gradients:\n",
    "                tf.histogram_summary(var.name, var)\n",
    "            if grad is not None:\n",
    "                tf.histogram_summary(var.name + '/gradients', grad)\n",
    "\n",
    "            # emit summaries\n",
    "            tf.scalar_summary(\"policy_loss\", self.pg_loss)\n",
    "            tf.scalar_summary(\"reg_loss\", self.reg_loss)\n",
    "            tf.scalar_summary(\"total_loss\", self.loss)\n",
    "\n",
    "        # training update\n",
    "        with tf.name_scope(\"train_policy_network\"):\n",
    "            # apply gradients to update policy network\n",
    "            self.train_op = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "        self.summarize = tf.merge_all_summaries()\n",
    "        self.no_op = tf.no_op()\n",
    "\n",
    "    def sampleAction(self, states):\n",
    "        # TODO: use this code piece when tf.multinomial gets better\n",
    "        # sample action from current policy\n",
    "        # actions = self.session.run(self.predicted_actions, {self.states: states})[0]\n",
    "        # return actions[0]\n",
    "\n",
    "        # temporary workaround\n",
    "        def softmax(y):\n",
    "            \"\"\" simple helper function here that takes unnormalized logprobs \"\"\"\n",
    "            maxy = np.amax(y)\n",
    "            e = np.exp(y - maxy)\n",
    "            return e / np.sum(e)\n",
    "\n",
    "        # epsilon-greedy exploration strategy\n",
    "        if random.random() < self.exploration:\n",
    "            return random.randint(0, self.num_actions-1)\n",
    "        else:\n",
    "            action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n",
    "            action_probs  = softmax(action_scores) - 1e-5\n",
    "            action = np.argmax(np.random.multinomial(1, action_probs))\n",
    "            return action\n",
    "\n",
    "    def updateModel(self):\n",
    "\n",
    "        N = len(self.reward_buffer)\n",
    "        r = 0 # use discounted reward to approximate Q value\n",
    "\n",
    "        # compute discounted future rewards\n",
    "        discounted_rewards = np.zeros(N)\n",
    "        for t in reversed(xrange(N)):\n",
    "            # future discounted reward from now on\n",
    "            r = self.reward_buffer[t] + self.discount_factor * r\n",
    "            discounted_rewards[t] = r\n",
    "\n",
    "        # reduce gradient variance by normalization\n",
    "        self.all_rewards += discounted_rewards.tolist()\n",
    "        self.all_rewards = self.all_rewards[:self.max_reward_length]\n",
    "        discounted_rewards -= np.mean(self.all_rewards)\n",
    "        discounted_rewards /= np.std(self.all_rewards)\n",
    "\n",
    "        # whether to calculate summaries\n",
    "        calculate_summaries = self.train_iteration % self.summary_every == 0 and self.summary_writer is not None\n",
    "\n",
    "        # update policy network with the rollout in batches\n",
    "        for t in xrange(N-1):\n",
    "\n",
    "            # prepare inputs\n",
    "            states  = self.state_buffer[t][np.newaxis, :]\n",
    "            actions = np.array([self.action_buffer[t]])\n",
    "            rewards = np.array([discounted_rewards[t]])\n",
    "\n",
    "            # evaluate gradients\n",
    "            grad_evals = [grad for grad, var in self.gradients]\n",
    "\n",
    "            # perform one update of training\n",
    "            _, summary_str = self.session.run([\n",
    "                self.train_op,\n",
    "                self.summarize if calculate_summaries else self.no_op\n",
    "              ], {\n",
    "                self.states:             states,\n",
    "                self.taken_actions:      actions,\n",
    "                self.discounted_rewards: rewards\n",
    "              })\n",
    "\n",
    "            # emit summaries\n",
    "            if calculate_summaries:\n",
    "                self.summary_writer.add_summary(summary_str, self.train_iteration)\n",
    "\n",
    "        self.annealExploration()\n",
    "        self.train_iteration += 1\n",
    "\n",
    "        # clean up\n",
    "        self.cleanUp()\n",
    "\n",
    "    def annealExploration(self, stategy='linear'):\n",
    "        ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n",
    "        self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n",
    "\n",
    "    def storeRollout(self, state, action, reward):\n",
    "        self.action_buffer.append(action)\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.state_buffer.append(state)\n",
    "\n",
    "    def cleanUp(self):\n",
    "        self.state_buffer  = []\n",
    "        self.reward_buffer = []\n",
    "        self.action_buffer = []"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
