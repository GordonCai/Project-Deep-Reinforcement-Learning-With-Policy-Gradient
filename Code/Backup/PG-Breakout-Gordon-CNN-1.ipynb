{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-24 17:39:59,526] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue from last saved episode\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-94b56e499824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mfeed_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mimage_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0maprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_aprob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0maprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Initialize OpenAI for Breakout\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "# Hyper parameters for policy gradient model\n",
    "global num_actions, discount, batch_size\n",
    "num_actions = 3\n",
    "discount = 0.99\n",
    "save_every = 20\n",
    "train_every = 1\n",
    "reward_factor = 1\n",
    "max_episode = 10000 # When to terminate traininig\n",
    "\n",
    "# Hyper parameters for Network\n",
    "learning_rate=0.0001\n",
    "CNN_param = {'conv_1': [32,12,2],'conv2':[48,8,2]}\n",
    "FC_param = {'FC_1':256}\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize parameter for policy gradient model \n",
    "observation = env.reset()\n",
    "image_old  = None\n",
    "# image_current, image_last_1, image_last_2, image_last_3, image_last_4 = None,None,None,None,None\n",
    "images, fake_labels, rewards_std, action_hist, reward_hist, reward_runn = [], [], [], [], [], []\n",
    "reward_episode = 0\n",
    "\n",
    "# Build network\n",
    "def Network(image_, FC_param, CNN_param=None):\n",
    "    \n",
    "    if CNN_param!=None:\n",
    "        conv_tmp = tf.layers.conv2d(inputs=image_,\n",
    "                                    filters=CNN_param['conv_1'][0],\n",
    "                                    kernel_size=[CNN_param['conv_1'][1], CNN_param['conv_1'][1]],\n",
    "                                    strides = CNN_param['conv_1'][2],\n",
    "                                    padding=\"same\",\n",
    "                                    activation=tf.nn.relu,\n",
    "                                    use_bias=False,\n",
    "                                    name='conv_1')\n",
    "        conv_tmp = tf.layers.max_pooling2d(inputs=conv_tmp, \n",
    "                                            pool_size=[2, 2], \n",
    "                                            strides=2)\n",
    "\n",
    "        for key,value in CNN_param.items():\n",
    "            if key != 'conv_1':\n",
    "                conv_tmp = tf.layers.conv2d(inputs=conv_tmp,\n",
    "                                            filters=value[0],\n",
    "                                            kernel_size=[value[1], value[1]],\n",
    "                                            strides = value[2],\n",
    "                                            padding=\"same\",\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            use_bias=False,\n",
    "                                            name=key)\n",
    "                conv_tmp = tf.layers.max_pooling2d(inputs=conv_tmp, \n",
    "                                                       pool_size=[2,2], \n",
    "                                                       strides=2)\n",
    "    \n",
    "    if CNN_param==None:\n",
    "        FC_tmp = tf.contrib.layers.flatten(image_, scope='Flatten')\n",
    "    else:\n",
    "        FC_tmp = tf.contrib.layers.flatten(conv_tmp, scope='Flatten')\n",
    "    \n",
    "    for key,value in FC_param.items():\n",
    "        FC_tmp = tf.layers.dense(inputs=FC_tmp, \n",
    "                                 units=value,\n",
    "                                 kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                     stddev=1./np.sqrt(5000), \n",
    "                                                                                     dtype=tf.float32),\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 use_bias=False,\n",
    "                                 name=key)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=FC_tmp, \n",
    "                             units=num_actions, \n",
    "                             kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                 stddev=1./np.sqrt(500), \n",
    "                                                                                 dtype=tf.float32),\n",
    "                             use_bias=False,\n",
    "                             name='Logits')\n",
    "    \n",
    "    action_probs = tf.nn.softmax(logits, name='SoftMax')\n",
    "    \n",
    "    return action_probs\n",
    "\n",
    "# Get discounted reward and normalize it\n",
    "def discount_norm(rew):\n",
    "    \n",
    "    rew_func = lambda a, v: a*discount + v # Reward function\n",
    "    \n",
    "    rew_reverse = tf.scan(rew_func, tf.reverse(rew,[True, False]))\n",
    "    discounted_rew = tf.reverse(rew_reverse,[True, False])\n",
    "    \n",
    "    mean, variance= tf.nn.moments(discounted_rew, [0])\n",
    "    discounted_rew -= mean\n",
    "    discounted_rew /= tf.sqrt(variance + 1e-6)\n",
    "    \n",
    "    return discounted_rew\n",
    "\n",
    "# Process image by cropping and binarizing\n",
    "def process_obs(obs):\n",
    "    \n",
    "    obs = obs[35:195]\n",
    "    obs = obs[::2,::2,0]\n",
    "    obs[obs == 144] = 0\n",
    "    obs[obs == 109] = 0\n",
    "    obs[obs != 0] = 1 \n",
    "    \n",
    "    return obs.astype(np.float)\n",
    "\n",
    "def argmax_frame(image,image_old):\n",
    "    \n",
    "    if image_old!=None and image!=None:\n",
    "        tmp = np.maximum.reduce([image,image_old]).reshape(80,80,1)\n",
    "    elif image==None:\n",
    "        tmp = np.zeros((80,80,1))\n",
    "    else:\n",
    "        tmp = image.reshape(80,80,1)\n",
    "        \n",
    "    return tmp\n",
    "\n",
    "def form_sequence(image_current, image_last_1, image_last_2, image_last_3, image_last_4):\n",
    "    \n",
    "    images_out = np.concatenate((argmax_frame(image_current, image_last_1),\n",
    "                                 argmax_frame(image_last_1, image_last_2),\n",
    "                                 argmax_frame(image_last_2, image_last_3),\n",
    "                                 argmax_frame(image_last_2, image_last_4)), axis=2)\n",
    "    \n",
    "    return images_out, image_current, image_last_1, image_last_2, image_last_3\n",
    "    \n",
    "\n",
    "# Break down into batch jobs to reduce GPU/CPU load\n",
    "def gen_batch_index(num_of_steps):\n",
    "    \n",
    "    batch_index = []\n",
    "    start = 0\n",
    "    for index in np.array(range(0,num_of_steps,batch_size)):\n",
    "        end=index\n",
    "        if end > 0:\n",
    "            batch_index.append([start,end])\n",
    "        start=index\n",
    "    end = num_of_steps\n",
    "    batch_index.append([start,end])\n",
    "    \n",
    "    return batch_index\n",
    "\n",
    "def get_running_mean(reward_hist):\n",
    "    \n",
    "    running_mean=[]\n",
    "    mean=0\n",
    "    for i in range(len(reward_hist)):\n",
    "        mean = 0.99*mean + 0.01*reward_hist[i]\n",
    "        running_mean.append(mean)\n",
    "        \n",
    "    return running_mean\n",
    "\n",
    "# Build model\n",
    "with tf.Graph().as_default() as g:\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        image_ = tf.placeholder(dtype=tf.float32, shape=[None, 80,80,1],name=\"image\")\n",
    "        fake_label_ = tf.placeholder(dtype=tf.float32, shape=[None, num_actions],name=\"fake_label\")\n",
    "        reward_ = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"reward\")\n",
    "\n",
    "        discounted_epr = discount_norm(reward_) # Get policy gradient\n",
    "\n",
    "        tf_aprob = Network(image_,FC_param,CNN_param)\n",
    "        loss = tf.nn.l2_loss(fake_label_-tf_aprob) # Define loss\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#         optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay_, epsilon=epsilon_)\n",
    "        grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=discounted_epr)\n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "# Main program\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "#     writer = tf.summary.FileWriter('./graph/', graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # Load from last session or start over\n",
    "    try:\n",
    "        episode_number = pickle.load(open('PG-Breakout-ckpt-1-CNN/last-episode.p','rb'))\n",
    "        saver.restore(sess, 'PG-Breakout-ckpt-1-CNN\\\\Pong_PG.ckpt-'+str(episode_number))\n",
    "        reward_hist = pickle.load(open('PG-Breakout-ckpt-1-CNN/all_reward.p','rb'))\n",
    "        print('Continue from last saved episode')\n",
    "    except:\n",
    "        print('Training from beginning')\n",
    "        episode_number = 0\n",
    "        \n",
    "    # Starting playing\n",
    "    while episode_number<=10001:\n",
    "        \n",
    "        image_proc = process_obs(observation)\n",
    "        image_diff = image_proc - image_old if image_old is not None else np.zeros((80,80))\n",
    "        image_old = image_proc\n",
    "        \n",
    "#         images_out, image_last_1, image_last_2, image_last_3, image_last_4 = \\\n",
    "#             form_sequence(process_obs(observation), image_last_1, image_last_2, image_last_3, image_last_4)\n",
    "            \n",
    "        feed_step = {image_: np.reshape(image_diff, (1,80,80,1))}\n",
    "        aprob = sess.run(tf_aprob,feed_step); aprob = aprob[0,:]\n",
    "        action = np.random.choice(num_actions,p=aprob)\n",
    "        label = np.zeros_like(aprob); label[action] = 1 \n",
    "\n",
    "        # Input action to OpenAI and get feedback\n",
    "        observation, reward, done, info = env.step(action+1)\n",
    "        reward_episode += reward\n",
    "\n",
    "        # Record for training\n",
    "        images.append(image_diff); fake_labels.append(label); rewards_std.append(reward)\n",
    "\n",
    "        # Training\n",
    "        if done:\n",
    "            \n",
    "            if episode_number % train_every == 0:\n",
    "                \n",
    "                # Use stochastic gradient decent if using CNN\n",
    "                batch_index = gen_batch_index(len(images))\n",
    "                for item in batch_index:\n",
    "                    feed_episode = {image_: np.array(images[item[0]:item[1]]).reshape(-1, 80, 80, 1), \n",
    "                            fake_label_: np.array(fake_labels[item[0]:item[1]]).reshape(-1,num_actions), \n",
    "                            reward_: np.array(rewards_std[item[0]:item[1]]).reshape(-1,1)}\n",
    "                    sess.run(train_op,feed_episode)\n",
    "                    \n",
    "                # Clear memory\n",
    "                images, fake_labels, rewards_std, action_hist = [], [], [], []\n",
    "            \n",
    "            # Bookkeeping\n",
    "            reward_hist.append(reward_episode)\n",
    "            reward_runn.append(reward_episode)\n",
    "            episode_number += 1\n",
    "            observation = env.reset()\n",
    "            print('\\tep {}: reward: {}'.format(episode_number, reward_episode))\n",
    "            reward_episode = 0\n",
    "            \n",
    "            # Save model\n",
    "            if episode_number % save_every == 0:\n",
    "                \n",
    "                saver.save(sess, 'PG-Breakout-ckpt-1-CNN/Pong_PG.ckpt', global_step=episode_number)\n",
    "                print('Model saved, mean reward is',np.mean(reward_runn))\n",
    "                pickle.dump(episode_number,open('PG-Breakout-ckpt-1-CNN/last-episode.p','wb'))\n",
    "                pickle.dump(reward_hist,open('PG-Breakout-ckpt-1-CNN/all_reward.p','wb'))\n",
    "                \n",
    "                plt.figure(figsize=(18,3))\n",
    "                plt.plot([2]*len(reward_hist),'r--')\n",
    "                plt.plot(np.array(get_running_mean(reward_hist)))\n",
    "                plt.show()\n",
    "                \n",
    "                reward_runn = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "interval = 2\n",
    "running_mean=[]; running_var=[]\n",
    "for i in range(np.floor(len(reward_hist)/interval).astype('int32')):\n",
    "    running_mean.extend([np.mean(reward_hist[i*interval:i*interval+interval+1])]*interval)\n",
    "    running_var.extend([np.var(reward_hist[i*interval:i*interval+interval+1])]*interval)\n",
    "\n",
    "fig,axe = plt.subplots(3,1,figsize=(18,15))\n",
    "axe[0].plot(np.array(reward_hist))\n",
    "axe[0].set_title('Cummulative reward')\n",
    "axe[1].plot(np.array(running_mean))\n",
    "axe[1].set_title('Running mean')\n",
    "axe[2].plot(np.array(running_var))\n",
    "axe[2].set_title('Running variance')\n",
    "plt.show()\n",
    "fig.savefig('PG-Breakout-ckpt-1-CNN/CNN1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADBFJREFUeJzt3V+MXOV9xvHv0zUWTVJkTAAZ7NQgUQIXxaQWBVFVLa4L\nSRH0IlSgtEIREjdpBWqq1OSuUiuRm4RcVJEQkHJBA9QBxUIRjuUQtZUqBxPcJGAcE0rxygQT/og0\nSGnt/Hoxx82KruOzuzOzPvt+P9Jo5n3nzJ736OiZ82fOnl+qCklt+ZXlHoCk6TP4UoMMvtQggy81\nyOBLDTL4UoMMvtSgJQU/yXVJDiR5Mcm2cQ1K0mRlsRfwJJkBfgBsBWaBp4Fbqur58Q1P0iSsWsJn\nrwBerKqXAJI8DNwInDD4H1w7Uxs3nNbrj//gu+9bwtCkleU3fvPdXtO9fOh/+PGbx3Ky6ZYS/POB\nQ3Pas8Bv/7IPbNxwGt/euaHXH7/2vE2LH5m0wuzcua/XdFdce+jkE7G0Y/z5vlX+33FDktuT7E2y\n9/U3ji1hdpLGZSnBnwXmbr7XA4ffO1FV3VtVm6tq89lnzSxhdpLGZSnBfxq4KMkFSVYDNwM7xjMs\nSZO06GP8qjqa5M+BncAM8EBVPTe2kUmamKWc3KOqvg58fUxjkTQlXrknNcjgSw0y+FKDDL7UIIMv\nNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw06afCTPJDk\nSJLvz+lbm2RXkoPd85mTHaakceqzxf8H4Lr39G0DdlfVRcDuri1pIE4a/Kr6Z+DN93TfCDzYvX4Q\n+OMxj0vSBC32GP/cqnoVoHs+Z3xDkjRpEz+5Zwkt6dSz2Pvqv5ZkXVW9mmQdcOREE1bVvcC9AJsv\nO713Te6dh/sVCZS0cIvd4u8Abu1e3wp8bTzDkTQNfX7O+wrwb8DFSWaT3AbcDWxNchDY2rUlDcRJ\nd/Wr6pYTvLVlzGORNCVeuSc1yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMM\nvtQggy81yOBLDTL4UoMMvtQggy81yOBLDepzz70NSZ5Ksj/Jc0nu6PotoyUNVJ8t/lHg01V1CXAl\n8Kkkl2IZLWmw+pTQerWqvtO9/gmwHzgfy2hJg7WgY/wkG4HLgT1YRksarN7BT/IB4KvAnVX1zgI+\nZwkt6RTTK/hJTmMU+oeq6rGu+7WufBa/rIxWVd1bVZuravPZZ82MY8ySlqjPWf0A9wP7q+rzc96y\njJY0UH2KZl4N/BnwvSTHK1l+llHZrEe7klqvADdNZoiSxq1PCa1/BXKCty2jJQ2QV+5JDTL4UoMM\nvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4\nUoP63HNvWVx73qblHoJ0yth5eN/JJ1qAPnfZPT3Jt5P8e1c772+6/guS7Olq5z2SZPVYRyZpYvrs\n6v8MuKaqLgM2AdcluRL4HPCFrnbeW8BtkxumpHHqUzuvquq/uuZp3aOAa4DtXb+186QB6VtJZ6a7\np/4RYBfwQ+DtqjraTTLLqJDmfJ+1hJZ0iukV/Ko6VlWbgPXAFcAl8012gs9aQks6xSzo57yqehv4\nFnAlsCbJ8V8F1gOHxzs0SZPS56z+2UnWdK9/FfgDYD/wFPDxbjJr50kD0ud3/HXAg0lmGH1RPFpV\nTyR5Hng4yd8CzzIqrClpAPrUzvsucPk8/S8xOt6XNDBesis1yOBLDTL4UoMMvtQggy81yOBLDTL4\nUoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDeod/O7e+s8meaJrW0JL\nGqiFbPHvYHR33eMsoSUNVN9KOuuBPwLu69rBElrSYPXd4t8DfAb4edc+C0toSYPVp6DG9cCRqnpm\nbvc8k1pCSxqIPgU1rgZuSPIx4HTgDEZ7AGuSrOq2+pbQkgakT5nsu6pqfVVtBG4GvllVn8ASWtJg\nLeV3/L8G/jLJi4yO+S2hJQ1En139/1NV32JULdcSWtKAeeWe1CCDLzXI4EsNMvhSgwy+1CCDLzXI\n4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzWo1623krwM/AQ4Bhyt\nqs1J1gKPABuBl4E/qaq3JjNMSeO0kC3+71fVpqra3LW3Abu7Elq7u7akAVjKrv6NjEpngSW0pEHp\nG/wCvpHkmSS3d33nVtWrAN3zOZMYoKTx63t77aur6nCSc4BdSV7oO4Pui+J2gA+dv6C7eUuakF5b\n/Ko63D0fAR5ndD/915KsA+iej5zgs9bOk04xfYpmvj/Jrx1/Dfwh8H1gB6PSWWAJLWlQ+ux7nws8\nnuT49P9YVU8meRp4NMltwCvATZMbpqRxOmnwu1JZl83T/wawZRKDkjRZXrknNcjgSw0y+FKDDL7U\nIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw3qFfwk\na5JsT/JCkv1JrkqyNsmuJAe75zMnPVhJ49F3i/9F4Mmq+jCj++/txxJa0mD1ub32GcDvAvcDVNV/\nV9XbWEJLGqw+W/wLgdeBLyd5Nsl93f31LaElDVSf4K8CPgJ8qaouB37KAnbrk9yeZG+Sva+/cWyR\nw5Q0Tn2CPwvMVtWerr2d0ReBJbSkgTpp8KvqR8ChJBd3XVuA57GEljRYfcvX/gXwUJLVwEvAJxl9\naVhCSxqgXsGvqn3A5nnesoSWNEBeuSc1yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBL\nDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDepTUOPiJPvmPN5JcqcltKTh6nOX3QNVtamq\nNgG/BbwLPI4ltKTBWuiu/hbgh1X1n1hCSxqshQb/ZuAr3WtLaEkD1Tv43T31bwD+aSEzsISWdOpZ\nyBb/o8B3quq1rm0JLWmgFhL8W/jFbj5YQksarF7BT/I+YCvw2Jzuu4GtSQ527909/uFJmoS+JbTe\nBc56T98bWEJLGiSv3JMaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQ\nwZca1Ou/8ySN387D+5Zt3m7xpQYZfKlBBl9qkMGXGuTJPWmZXHvept7TjvtEoFt8qUEGX2pQqmp6\nM0teB34K/HhqM52uD7Iyl83lGo5fr6qzTzbRVIMPkGRvVW2e6kynZKUum8u18rirLzXI4EsNWo7g\n37sM85yWlbpsLtcKM/VjfEnLz119qUFTDX6S65IcSPJikm3TnPc4JdmQ5Kkk+5M8l+SOrn9tkl1J\nDnbPZy73WBcjyUySZ5M80bUvSLKnW65Hkqxe7jEuRpI1SbYneaFbd1etlHW2UFMLfpIZ4O+BjwKX\nArckuXRa8x+zo8Cnq+oS4ErgU92ybAN2V9VFwO6uPUR3APvntD8HfKFbrreA25ZlVEv3ReDJqvow\ncBmjZVwp62xhqmoqD+AqYOec9l3AXdOa/4SX7WvAVuAAsK7rWwccWO6xLWJZ1jMKwDXAE0AYXeSy\nar71OJQHcAbwH3Tnteb0D36dLeYxzV3984FDc9qzXd+gJdkIXA7sAc6tqlcBuudzlm9ki3YP8Bng\n5137LODtqjratYe63i4EXge+3B3G3Jfk/ayMdbZg0wx+5ukb9E8KST4AfBW4s6reWe7xLFWS64Ej\nVfXM3O55Jh3ielsFfAT4UlVdzujS8TZ26+cxzeDPAhvmtNcDh6c4/7FKchqj0D9UVY913a8lWde9\nvw44slzjW6SrgRuSvAw8zGh3/x5gTZLj/8I91PU2C8xW1Z6uvZ3RF8HQ19miTDP4TwMXdWeIVwM3\nAzumOP+xSRLgfmB/VX1+zls7gFu717cyOvYfjKq6q6rWV9VGRuvnm1X1CeAp4OPdZINbLoCq+hFw\nKMnFXdcW4HkGvs4Wa9r/nfcxRluQGeCBqvq7qc18jJL8DvAvwPf4xbHwZxkd5z8KfAh4Bbipqt5c\nlkEuUZLfA/6qqq5PciGjPYC1wLPAn1bVz5ZzfIuRZBNwH7AaeAn4JKON34pYZwvhlXtSg7xyT2qQ\nwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUH/C5oeEYch+P74AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ccc1ae8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_proc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3585,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fake_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
