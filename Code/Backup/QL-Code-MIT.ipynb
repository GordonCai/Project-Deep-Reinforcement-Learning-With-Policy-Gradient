{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "class NeuralQLearner(object):\n",
    "\n",
    "  def __init__(self, session,\n",
    "                     optimizer,\n",
    "                     q_network,\n",
    "                     state_dim,\n",
    "                     num_actions,\n",
    "                     batch_size=32,\n",
    "                     init_exp=0.5,       # initial exploration prob\n",
    "                     final_exp=0.1,      # final exploration prob\n",
    "                     anneal_steps=10000, # N steps for annealing exploration \n",
    "                     replay_buffer_size=10000,\n",
    "                     store_replay_every=5, # how frequent to store experience\n",
    "                     discount_factor=0.9, # discount future rewards\n",
    "                     target_update_rate=0.01,\n",
    "                     reg_param=0.01, # regularization constants\n",
    "                     max_gradient=5, # max gradient norms\n",
    "                     double_q_learning=False,\n",
    "                     summary_writer=None,\n",
    "                     summary_every=100):\n",
    "\n",
    "    # tensorflow machinery\n",
    "    self.session        = session\n",
    "    self.optimizer      = optimizer\n",
    "    self.summary_writer = summary_writer\n",
    "\n",
    "    # model components\n",
    "    self.q_network     = q_network\n",
    "    self.replay_buffer = ReplayBuffer(buffer_size=replay_buffer_size)\n",
    "\n",
    "    # Q learning parameters\n",
    "    self.batch_size      = batch_size\n",
    "    self.state_dim       = state_dim\n",
    "    self.num_actions     = num_actions\n",
    "    self.exploration     = init_exp\n",
    "    self.init_exp        = init_exp\n",
    "    self.final_exp       = final_exp\n",
    "    self.anneal_steps    = anneal_steps\n",
    "    self.discount_factor = discount_factor\n",
    "    self.target_update_rate = target_update_rate\n",
    "    self.double_q_learning = double_q_learning\n",
    "\n",
    "    # training parameters\n",
    "    self.max_gradient = max_gradient\n",
    "    self.reg_param    = reg_param\n",
    "\n",
    "    # counters\n",
    "    self.store_replay_every   = store_replay_every\n",
    "    self.store_experience_cnt = 0\n",
    "    self.train_iteration      = 0\n",
    "\n",
    "    # create and initialize variables\n",
    "    self.create_variables()\n",
    "    var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "    self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "    # make sure all variables are initialized\n",
    "    self.session.run(tf.assert_variables_initialized())\n",
    "\n",
    "    if self.summary_writer is not None:\n",
    "      # graph was not available when journalist was created\n",
    "      self.summary_writer.add_graph(self.session.graph)\n",
    "      self.summary_every = summary_every\n",
    "\n",
    "  def create_variables(self):\n",
    "    # compute action from a state: a* = argmax_a Q(s_t,a)\n",
    "    with tf.name_scope(\"predict_actions\"):\n",
    "      # raw state representation\n",
    "      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"states\")\n",
    "      # initialize Q network\n",
    "      with tf.variable_scope(\"q_network\"):\n",
    "        self.q_outputs = self.q_network(self.states)\n",
    "      # predict actions from Q network\n",
    "      self.action_scores = tf.identity(self.q_outputs, name=\"action_scores\")\n",
    "      tf.histogram_summary(\"action_scores\", self.action_scores)\n",
    "      self.predicted_actions = tf.argmax(self.action_scores, dimension=1, name=\"predicted_actions\")\n",
    "\n",
    "    # estimate rewards using the next state: r(s_t,a_t) + argmax_a Q(s_{t+1}, a)\n",
    "    with tf.name_scope(\"estimate_future_rewards\"):\n",
    "      self.next_states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"next_states\")\n",
    "      self.next_state_mask = tf.placeholder(tf.float32, (None,), name=\"next_state_masks\")\n",
    "\n",
    "      if self.double_q_learning:\n",
    "        # reuse Q network for action selection\n",
    "        with tf.variable_scope(\"q_network\", reuse=True):\n",
    "          self.q_next_outputs = self.q_network(self.next_states)\n",
    "        self.action_selection = tf.argmax(tf.stop_gradient(self.q_next_outputs), 1, name=\"action_selection\")\n",
    "        tf.histogram_summary(\"action_selection\", self.action_selection)\n",
    "        self.action_selection_mask = tf.one_hot(self.action_selection, self.num_actions, 1, 0)\n",
    "        # use target network for action evaluation\n",
    "        with tf.variable_scope(\"target_network\"):\n",
    "          self.target_outputs = self.q_network(self.next_states) * tf.cast(self.action_selection_mask, tf.float32)\n",
    "        self.action_evaluation = tf.reduce_sum(self.target_outputs, reduction_indices=[1,])\n",
    "        tf.histogram_summary(\"action_evaluation\", self.action_evaluation)\n",
    "        self.target_values = self.action_evaluation * self.next_state_mask\n",
    "      else:\n",
    "        # initialize target network\n",
    "        with tf.variable_scope(\"target_network\"):\n",
    "          self.target_outputs = self.q_network(self.next_states)\n",
    "        # compute future rewards\n",
    "        self.next_action_scores = tf.stop_gradient(self.target_outputs)\n",
    "        self.target_values = tf.reduce_max(self.next_action_scores, reduction_indices=[1,]) * self.next_state_mask\n",
    "        tf.histogram_summary(\"next_action_scores\", self.next_action_scores)\n",
    "\n",
    "      self.rewards = tf.placeholder(tf.float32, (None,), name=\"rewards\")\n",
    "      self.future_rewards = self.rewards + self.discount_factor * self.target_values\n",
    "\n",
    "    # compute loss and gradients\n",
    "    with tf.name_scope(\"compute_temporal_differences\"):\n",
    "      # compute temporal difference loss\n",
    "      self.action_mask = tf.placeholder(tf.float32, (None, self.num_actions), name=\"action_mask\")\n",
    "      self.masked_action_scores = tf.reduce_sum(self.action_scores * self.action_mask, reduction_indices=[1,])\n",
    "      self.temp_diff = self.masked_action_scores - self.future_rewards\n",
    "      self.td_loss = tf.reduce_mean(tf.square(self.temp_diff))\n",
    "      # regularization loss\n",
    "      q_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"q_network\")\n",
    "      self.reg_loss = self.reg_param * tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in q_network_variables])\n",
    "      # compute total loss and gradients\n",
    "      self.loss = self.td_loss + self.reg_loss\n",
    "      gradients = self.optimizer.compute_gradients(self.loss)\n",
    "      # clip gradients by norm\n",
    "      for i, (grad, var) in enumerate(gradients):\n",
    "        if grad is not None:\n",
    "          gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n",
    "      # add histograms for gradients.\n",
    "      for grad, var in gradients:\n",
    "        tf.histogram_summary(var.name, var)\n",
    "        if grad is not None:\n",
    "          tf.histogram_summary(var.name + '/gradients', grad)\n",
    "      self.train_op = self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "    # update target network with Q network\n",
    "    with tf.name_scope(\"update_target_network\"):\n",
    "      self.target_network_update = []\n",
    "      # slowly update target network parameters with Q network parameters\n",
    "      q_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"q_network\")\n",
    "      target_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_network\")\n",
    "      for v_source, v_target in zip(q_network_variables, target_network_variables):\n",
    "        # this is equivalent to target = (1-alpha) * target + alpha * source\n",
    "        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n",
    "        self.target_network_update.append(update_op)\n",
    "      self.target_network_update = tf.group(*self.target_network_update)\n",
    "\n",
    "    # scalar summaries\n",
    "    tf.scalar_summary(\"td_loss\", self.td_loss)\n",
    "    tf.scalar_summary(\"reg_loss\", self.reg_loss)\n",
    "    tf.scalar_summary(\"total_loss\", self.loss)\n",
    "    tf.scalar_summary(\"exploration\", self.exploration)\n",
    "\n",
    "    self.summarize = tf.merge_all_summaries()\n",
    "    self.no_op = tf.no_op()\n",
    "\n",
    "  def storeExperience(self, state, action, reward, next_state, done):\n",
    "    # always store end states\n",
    "    if self.store_experience_cnt % self.store_replay_every == 0 or done:\n",
    "      self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "    self.store_experience_cnt += 1\n",
    "\n",
    "  def eGreedyAction(self, states, explore=True):\n",
    "    if explore and self.exploration > random.random():\n",
    "      return random.randint(0, self.num_actions-1)\n",
    "    else:\n",
    "      return self.session.run(self.predicted_actions, {self.states: states})[0]\n",
    "\n",
    "  def annealExploration(self, stategy='linear'):\n",
    "    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n",
    "    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n",
    "\n",
    "  def updateModel(self):\n",
    "    # not enough experiences yet\n",
    "    if self.replay_buffer.count() < self.batch_size:\n",
    "      return\n",
    "\n",
    "    batch           = self.replay_buffer.getBatch(self.batch_size)\n",
    "    states          = np.zeros((self.batch_size, self.state_dim))\n",
    "    rewards         = np.zeros((self.batch_size,))\n",
    "    action_mask     = np.zeros((self.batch_size, self.num_actions))\n",
    "    next_states     = np.zeros((self.batch_size, self.state_dim))\n",
    "    next_state_mask = np.zeros((self.batch_size,))\n",
    "\n",
    "    for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "      states[k] = s0\n",
    "      rewards[k] = r\n",
    "      action_mask[k][a] = 1\n",
    "      # check terminal state\n",
    "      if not done:\n",
    "        next_states[k] = s1\n",
    "        next_state_mask[k] = 1\n",
    "\n",
    "    # whether to calculate summaries\n",
    "    calculate_summaries = self.train_iteration % self.summary_every == 0 and self.summary_writer is not None\n",
    "\n",
    "    # perform one update of training\n",
    "    cost, _, summary_str = self.session.run([\n",
    "      self.loss,\n",
    "      self.train_op,\n",
    "      self.summarize if calculate_summaries else self.no_op\n",
    "    ], {\n",
    "      self.states:          states,\n",
    "      self.next_states:     next_states,\n",
    "      self.next_state_mask: next_state_mask,\n",
    "      self.action_mask:     action_mask,\n",
    "      self.rewards:         rewards\n",
    "    })\n",
    "\n",
    "    # update target network using Q-network\n",
    "    self.session.run(self.target_network_update)\n",
    "\n",
    "    # emit summaries\n",
    "    if calculate_summaries:\n",
    "      self.summary_writer.add_summary(summary_str, self.train_iteration)\n",
    "\n",
    "    self.annealExploration()\n",
    "    self.train_iteration += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
