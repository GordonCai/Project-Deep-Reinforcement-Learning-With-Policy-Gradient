{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "class DeepDeterministicPolicyGradient(object):\n",
    "\n",
    "  def __init__(self, session,\n",
    "                     optimizer,\n",
    "                     actor_network,\n",
    "                     critic_network,\n",
    "                     state_dim,\n",
    "                     action_dim,\n",
    "                     batch_size=32,\n",
    "                     replay_buffer_size=1000000, # size of replay buffer\n",
    "                     store_replay_every=1,       # how frequent to store experience\n",
    "                     discount_factor=0.99,       # discount future rewards\n",
    "                     target_update_rate=0.01,\n",
    "                     reg_param=0.01,             # regularization constants\n",
    "                     max_gradient=5,             # max gradient norms\n",
    "                     noise_sigma=0.20,\n",
    "                     noise_theta=0.15,\n",
    "                     summary_writer=None,\n",
    "                     summary_every=100):\n",
    "\n",
    "    # tensorflow machinery\n",
    "    self.session        = session\n",
    "    self.optimizer      = optimizer\n",
    "    self.summary_writer = summary_writer\n",
    "\n",
    "    # model components\n",
    "    self.actor_network  = actor_network\n",
    "    self.critic_network = critic_network\n",
    "    self.replay_buffer  = ReplayBuffer(buffer_size=replay_buffer_size)\n",
    "\n",
    "    # training parameters\n",
    "    self.batch_size         = batch_size\n",
    "    self.state_dim          = state_dim\n",
    "    self.action_dim         = action_dim\n",
    "    self.discount_factor    = discount_factor\n",
    "    self.target_update_rate = target_update_rate\n",
    "    self.max_gradient       = max_gradient\n",
    "    self.reg_param          = reg_param\n",
    "\n",
    "    # Ornstein-Uhlenbeck noise for exploration\n",
    "    self.noise_var = tf.Variable(tf.zeros([1, action_dim]))\n",
    "    noise_random = tf.random_normal([1, action_dim], stddev=noise_sigma)\n",
    "    self.noise = self.noise_var.assign_sub((noise_theta) * self.noise_var - noise_random)\n",
    "\n",
    "    # counters\n",
    "    self.store_replay_every   = store_replay_every\n",
    "    self.store_experience_cnt = 0\n",
    "    self.train_iteration      = 0\n",
    "\n",
    "    # create and initialize variables\n",
    "    self.create_variables()\n",
    "    var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "    self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "    # make sure all variables are initialized\n",
    "    self.session.run(tf.assert_variables_initialized())\n",
    "\n",
    "    if self.summary_writer is not None:\n",
    "      # graph was not available when journalist was created\n",
    "      self.summary_writer.add_graph(self.session.graph)\n",
    "      self.summary_every = summary_every\n",
    "\n",
    "  def create_variables(self):\n",
    "    \n",
    "    with tf.name_scope(\"model_inputs\"):\n",
    "      # raw state representation\n",
    "      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"states\")\n",
    "      # action input used by critic network\n",
    "      self.action = tf.placeholder(tf.float32, (None, self.action_dim), name=\"action\")\n",
    "\n",
    "    # define outputs from the actor and the critic\n",
    "    with tf.name_scope(\"predict_actions\"):\n",
    "      # initialize actor-critic network\n",
    "      with tf.variable_scope(\"actor_network\"):\n",
    "        self.policy_outputs = self.actor_network(self.states)\n",
    "      with tf.variable_scope(\"critic_network\"):\n",
    "        self.value_outputs    = self.critic_network(self.states, self.action)\n",
    "        self.action_gradients = tf.gradients(self.value_outputs, self.action)[0]\n",
    "\n",
    "      # predict actions from policy network\n",
    "      self.predicted_actions = tf.identity(self.policy_outputs, name=\"predicted_actions\")\n",
    "      tf.histogram_summary(\"predicted_actions\", self.predicted_actions)\n",
    "      tf.histogram_summary(\"action_scores\", self.value_outputs)\n",
    "\n",
    "    # get variable list\n",
    "    actor_network_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"actor_network\")\n",
    "    critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"critic_network\")\n",
    "\n",
    "    # estimate rewards using the next state: r + argmax_a Q'(s_{t+1}, u'(a))\n",
    "    with tf.name_scope(\"estimate_future_rewards\"):\n",
    "      self.next_states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"next_states\")\n",
    "      self.next_state_mask = tf.placeholder(tf.float32, (None,), name=\"next_state_masks\")\n",
    "      self.rewards = tf.placeholder(tf.float32, (None,), name=\"rewards\")\n",
    "\n",
    "      # initialize target network\n",
    "      with tf.variable_scope(\"target_actor_network\"):\n",
    "        self.target_actor_outputs = self.actor_network(self.next_states)\n",
    "      with tf.variable_scope(\"target_critic_network\"):\n",
    "        self.target_critic_outputs = self.critic_network(self.next_states, self.target_actor_outputs)\n",
    "\n",
    "      # compute future rewards\n",
    "      self.next_action_scores = tf.stop_gradient(self.target_critic_outputs)[:,0] * self.next_state_mask\n",
    "      tf.histogram_summary(\"next_action_scores\", self.next_action_scores)\n",
    "      self.future_rewards = self.rewards + self.discount_factor * self.next_action_scores\n",
    "\n",
    "    # compute loss and gradients\n",
    "    with tf.name_scope(\"compute_pg_gradients\"):\n",
    "\n",
    "      # compute gradients for critic network\n",
    "      self.temp_diff        = self.value_outputs[:,0] - self.future_rewards\n",
    "      self.mean_square_loss = tf.reduce_mean(tf.square(self.temp_diff))\n",
    "      self.critic_reg_loss  = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in critic_network_variables])\n",
    "      self.critic_loss      = self.mean_square_loss + self.reg_param * self.critic_reg_loss\n",
    "      self.critic_gradients = self.optimizer.compute_gradients(self.critic_loss, critic_network_variables)\n",
    "\n",
    "      # compute actor gradients (we don't do weight decay for actor network)\n",
    "      self.q_action_grad = tf.placeholder(tf.float32, (None, self.action_dim), name=\"q_action_grad\")\n",
    "      actor_policy_gradients = tf.gradients(self.policy_outputs, actor_network_variables, -self.q_action_grad)\n",
    "      self.actor_gradients = zip(actor_policy_gradients, actor_network_variables)\n",
    "\n",
    "      # collect all gradients\n",
    "      self.gradients = self.actor_gradients + self.critic_gradients\n",
    "\n",
    "      # clip gradients\n",
    "      for i, (grad, var) in enumerate(self.gradients):\n",
    "        # clip gradients by norm\n",
    "        if grad is not None:\n",
    "          self.gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n",
    "\n",
    "      # summarize gradients\n",
    "      for grad, var in self.gradients:\n",
    "        tf.histogram_summary(var.name, var)\n",
    "        if grad is not None:\n",
    "          tf.histogram_summary(var.name + '/gradients', grad)\n",
    "\n",
    "      # emit summaries\n",
    "      tf.scalar_summary(\"critic_loss\", self.critic_loss)\n",
    "      tf.scalar_summary(\"critic_td_loss\", self.mean_square_loss)\n",
    "      tf.scalar_summary(\"critic_reg_loss\", self.critic_reg_loss)\n",
    "\n",
    "      # apply gradients to update actor network\n",
    "      self.train_op = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "    # update target network with Q network\n",
    "    with tf.name_scope(\"update_target_network\"):\n",
    "      self.target_network_update = []\n",
    "\n",
    "      # slowly update target network parameters with the actor network parameters\n",
    "      actor_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"actor_network\")\n",
    "      target_actor_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_actor_network\")\n",
    "      for v_source, v_target in zip(actor_network_variables, target_actor_network_variables):\n",
    "        # this is equivalent to target = (1-alpha) * target + alpha * source\n",
    "        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n",
    "        self.target_network_update.append(update_op)\n",
    "\n",
    "      # same for the critic network\n",
    "      critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"critic_network\")\n",
    "      target_critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_critic_network\")\n",
    "      for v_source, v_target in zip(critic_network_variables, target_critic_network_variables):\n",
    "        # this is equivalent to target = (1-alpha) * target + alpha * source\n",
    "        update_op = v_target.assign_sub(self.target_update_rate * (v_target - v_source))\n",
    "        self.target_network_update.append(update_op)\n",
    "\n",
    "      # group all assignment operations together\n",
    "      self.target_network_update = tf.group(*self.target_network_update)\n",
    "\n",
    "    self.summarize = tf.merge_all_summaries()\n",
    "    self.no_op = tf.no_op()\n",
    "\n",
    "  def sampleAction(self, states, exploration=True):\n",
    "    policy_outs, ou_noise = self.session.run([\n",
    "      self.policy_outputs,\n",
    "      self.noise\n",
    "    ], {\n",
    "      self.states: states\n",
    "    })\n",
    "    # add OU noise for exploration\n",
    "    policy_outs = policy_outs + ou_noise if exploration else policy_outs\n",
    "    return policy_outs\n",
    "\n",
    "  def updateModel(self):\n",
    "\n",
    "    # not enough experiences yet\n",
    "    if self.replay_buffer.count() < self.batch_size:\n",
    "      return\n",
    "\n",
    "    batch           = self.replay_buffer.getBatch(self.batch_size)\n",
    "    states          = np.zeros((self.batch_size, self.state_dim))\n",
    "    rewards         = np.zeros((self.batch_size,))\n",
    "    actions         = np.zeros((self.batch_size, self.action_dim))\n",
    "    next_states     = np.zeros((self.batch_size, self.state_dim))\n",
    "    next_state_mask = np.zeros((self.batch_size,))\n",
    "\n",
    "    for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "      states[k]  = s0\n",
    "      rewards[k] = r\n",
    "      actions[k] = a\n",
    "      if not done:\n",
    "        next_states[k] = s1\n",
    "        next_state_mask[k] = 1\n",
    "\n",
    "    # whether to calculate summaries\n",
    "    calculate_summaries = self.train_iteration % self.summary_every == 0 and self.summary_writer is not None\n",
    "\n",
    "    # compute a = u(s)\n",
    "    policy_outs = self.session.run(self.policy_outputs, {\n",
    "      self.states: states\n",
    "    })\n",
    "\n",
    "    # compute d_a Q(s,a) where s=s_i, a=u(s)\n",
    "    action_grads = self.session.run(self.action_gradients, {\n",
    "      self.states: states,\n",
    "      self.action: policy_outs\n",
    "    })\n",
    "\n",
    "    critic_loss, _, summary_str = self.session.run([\n",
    "      self.critic_loss,\n",
    "      self.train_op,\n",
    "      self.summarize if calculate_summaries else self.no_op\n",
    "    ], {\n",
    "      self.states:          states,\n",
    "      self.next_states:     next_states,\n",
    "      self.next_state_mask: next_state_mask,\n",
    "      self.action:          actions,\n",
    "      self.rewards:         rewards,\n",
    "      self.q_action_grad:   action_grads\n",
    "    })\n",
    "\n",
    "    # update target network using Q-network\n",
    "    self.session.run(self.target_network_update)\n",
    "\n",
    "    # emit summaries\n",
    "    if calculate_summaries:\n",
    "      self.summary_writer.add_summary(summary_str, self.train_iteration)\n",
    "\n",
    "    self.train_iteration += 1\n",
    "\n",
    "  def storeExperience(self, state, action, reward, next_state, done):\n",
    "    # always store end states\n",
    "    if self.store_experience_cnt % self.store_replay_every == 0 or done:\n",
    "      self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "    self.store_experience_cnt += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
