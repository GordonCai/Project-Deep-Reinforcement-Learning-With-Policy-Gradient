{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class PolicyGradientActorCritic(object):\n",
    "\n",
    "  def __init__(self, session,\n",
    "                     optimizer,\n",
    "                     actor_network,\n",
    "                     critic_network,\n",
    "                     state_dim,\n",
    "                     num_actions,\n",
    "                     init_exp=0.1,         # initial exploration prob\n",
    "                     final_exp=0.0,        # final exploration prob\n",
    "                     anneal_steps=1000,    # N steps for annealing exploration \n",
    "                     discount_factor=0.99, # discount future rewards\n",
    "                     reg_param=0.001,      # regularization constants\n",
    "                     max_gradient=5,       # max gradient norms\n",
    "                     summary_writer=None,\n",
    "                     summary_every=100):\n",
    "\n",
    "    # tensorflow machinery\n",
    "    self.session        = session\n",
    "    self.optimizer      = optimizer\n",
    "    self.summary_writer = summary_writer\n",
    "\n",
    "    # model components\n",
    "    self.actor_network  = actor_network\n",
    "    self.critic_network = critic_network\n",
    "\n",
    "    # training parameters\n",
    "    self.state_dim       = state_dim\n",
    "    self.num_actions     = num_actions\n",
    "    self.discount_factor = discount_factor\n",
    "    self.max_gradient    = max_gradient\n",
    "    self.reg_param       = reg_param\n",
    "\n",
    "    # exploration parameters\n",
    "    self.exploration  = init_exp\n",
    "    self.init_exp     = init_exp\n",
    "    self.final_exp    = final_exp\n",
    "    self.anneal_steps = anneal_steps\n",
    "\n",
    "    # counters\n",
    "    self.train_iteration = 0\n",
    "\n",
    "    # rollout buffer\n",
    "    self.state_buffer  = []\n",
    "    self.reward_buffer = []\n",
    "    self.action_buffer = []\n",
    "\n",
    "    # create and initialize variables\n",
    "    self.create_variables()\n",
    "    var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "    self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "    # make sure all variables are initialized\n",
    "    self.session.run(tf.assert_variables_initialized())\n",
    "\n",
    "    if self.summary_writer is not None:\n",
    "      # graph was not available when journalist was created\n",
    "      self.summary_writer.add_graph(self.session.graph)\n",
    "      self.summary_every = summary_every\n",
    "\n",
    "  def resetModel(self):\n",
    "    self.cleanUp()\n",
    "    self.train_iteration = 0\n",
    "    self.exploration     = self.init_exp\n",
    "    var_lists = tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "    self.session.run(tf.initialize_variables(var_lists))\n",
    "\n",
    "  def create_variables(self):\n",
    "    \n",
    "    with tf.name_scope(\"model_inputs\"):\n",
    "      # raw state representation\n",
    "      self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"states\")\n",
    "\n",
    "    # rollout action based on current policy\n",
    "    with tf.name_scope(\"predict_actions\"):\n",
    "      # initialize actor-critic network\n",
    "      with tf.variable_scope(\"actor_network\"):\n",
    "        self.policy_outputs = self.actor_network(self.states)\n",
    "      with tf.variable_scope(\"critic_network\"):\n",
    "        self.value_outputs = self.critic_network(self.states)\n",
    "\n",
    "      # predict actions from policy network\n",
    "      self.action_scores = tf.identity(self.policy_outputs, name=\"action_scores\")\n",
    "      # Note 1: tf.multinomial is not good enough to use yet\n",
    "      # so we don't use self.predicted_actions for now\n",
    "      self.predicted_actions = tf.multinomial(self.action_scores, 1)\n",
    "\n",
    "    # get variable list\n",
    "    actor_network_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"actor_network\")\n",
    "    critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"critic_network\")\n",
    "\n",
    "    # compute loss and gradients\n",
    "    with tf.name_scope(\"compute_pg_gradients\"):\n",
    "      # gradients for selecting action from policy network\n",
    "      self.taken_actions = tf.placeholder(tf.int32, (None,), name=\"taken_actions\")\n",
    "      self.discounted_rewards = tf.placeholder(tf.float32, (None,), name=\"discounted_rewards\")\n",
    "\n",
    "      with tf.variable_scope(\"actor_network\", reuse=True):\n",
    "        self.logprobs = self.actor_network(self.states)\n",
    "\n",
    "      with tf.variable_scope(\"critic_network\", reuse=True):\n",
    "        self.estimated_values = self.critic_network(self.states)\n",
    "\n",
    "      # compute policy loss and regularization loss\n",
    "      self.cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(self.logprobs, self.taken_actions)\n",
    "      self.pg_loss            = tf.reduce_mean(self.cross_entropy_loss)\n",
    "      self.actor_reg_loss     = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in actor_network_variables])\n",
    "      self.actor_loss         = self.pg_loss + self.reg_param * self.actor_reg_loss\n",
    "\n",
    "      # compute actor gradients\n",
    "      self.actor_gradients = self.optimizer.compute_gradients(self.actor_loss, actor_network_variables)\n",
    "      # compute advantages A(s) = R - V(s)\n",
    "      self.advantages = tf.reduce_sum(self.discounted_rewards - self.estimated_values)\n",
    "      # compute policy gradients\n",
    "      for i, (grad, var) in enumerate(self.actor_gradients):\n",
    "        if grad is not None:\n",
    "          self.actor_gradients[i] = (grad * self.advantages, var)\n",
    "\n",
    "      # compute critic gradients\n",
    "      self.mean_square_loss = tf.reduce_mean(tf.square(self.discounted_rewards - self.estimated_values))\n",
    "      self.critic_reg_loss  = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in critic_network_variables])\n",
    "      self.critic_loss      = self.mean_square_loss + self.reg_param * self.critic_reg_loss\n",
    "      self.critic_gradients = self.optimizer.compute_gradients(self.critic_loss, critic_network_variables)\n",
    "\n",
    "      # collect all gradients\n",
    "      self.gradients = self.actor_gradients + self.critic_gradients\n",
    "\n",
    "      # clip gradients\n",
    "      for i, (grad, var) in enumerate(self.gradients):\n",
    "        # clip gradients by norm\n",
    "        if grad is not None:\n",
    "          self.gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n",
    "\n",
    "      # summarize gradients\n",
    "      for grad, var in self.gradients:\n",
    "        tf.histogram_summary(var.name, var)\n",
    "        if grad is not None:\n",
    "          tf.histogram_summary(var.name + '/gradients', grad)\n",
    "\n",
    "      # emit summaries\n",
    "      tf.histogram_summary(\"estimated_values\", self.estimated_values)\n",
    "      tf.scalar_summary(\"actor_loss\", self.actor_loss)\n",
    "      tf.scalar_summary(\"critic_loss\", self.critic_loss)\n",
    "      tf.scalar_summary(\"reg_loss\", self.actor_reg_loss + self.critic_reg_loss)\n",
    "\n",
    "    # training update\n",
    "    with tf.name_scope(\"train_actor_critic\"):\n",
    "      # apply gradients to update actor network\n",
    "      self.train_op = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "    self.summarize = tf.merge_all_summaries()\n",
    "    self.no_op = tf.no_op()\n",
    "\n",
    "  def sampleAction(self, states):\n",
    "    # TODO: use this code piece when tf.multinomial gets better\n",
    "    # sample action from current policy\n",
    "    # actions = self.session.run(self.predicted_actions, {self.states: states})[0]\n",
    "    # return actions[0]\n",
    "\n",
    "    # temporary workaround\n",
    "    def softmax(y):\n",
    "      \"\"\" simple helper function here that takes unnormalized logprobs \"\"\"\n",
    "      maxy = np.amax(y)\n",
    "      e = np.exp(y - maxy)\n",
    "      return e / np.sum(e)\n",
    "\n",
    "    # epsilon-greedy exploration strategy\n",
    "    if random.random() < self.exploration:\n",
    "      return random.randint(0, self.num_actions-1)\n",
    "    else:\n",
    "      action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n",
    "      action_probs  = softmax(action_scores) - 1e-5\n",
    "      action = np.argmax(np.random.multinomial(1, action_probs))\n",
    "      return action\n",
    "\n",
    "  def updateModel(self):\n",
    "\n",
    "    N = len(self.reward_buffer)\n",
    "    r = 0 # use discounted reward to approximate Q value\n",
    "\n",
    "    # compute discounted future rewards\n",
    "    discounted_rewards = np.zeros(N)\n",
    "    for t in reversed(xrange(N)):\n",
    "      # future discounted reward from now on\n",
    "      r = self.reward_buffer[t] + self.discount_factor * r\n",
    "      discounted_rewards[t] = r\n",
    "\n",
    "    # whether to calculate summaries\n",
    "    calculate_summaries = self.train_iteration % self.summary_every == 0 and self.summary_writer is not None\n",
    "\n",
    "    # update policy network with the rollout in batches\n",
    "    for t in xrange(N-1):\n",
    "\n",
    "      # prepare inputs\n",
    "      states  = self.state_buffer[t][np.newaxis, :]\n",
    "      actions = np.array([self.action_buffer[t]])\n",
    "      rewards = np.array([discounted_rewards[t]])\n",
    "\n",
    "      # perform one update of training\n",
    "      _, summary_str = self.session.run([\n",
    "        self.train_op,\n",
    "        self.summarize if calculate_summaries else self.no_op\n",
    "      ], {\n",
    "        self.states:             states,\n",
    "        self.taken_actions:      actions,\n",
    "        self.discounted_rewards: rewards\n",
    "      })\n",
    "\n",
    "      # emit summaries\n",
    "      if calculate_summaries:\n",
    "        self.summary_writer.add_summary(summary_str, self.train_iteration)\n",
    "\n",
    "    self.annealExploration()\n",
    "    self.train_iteration += 1\n",
    "\n",
    "    # clean up\n",
    "    self.cleanUp()\n",
    "\n",
    "  def annealExploration(self, stategy='linear'):\n",
    "    ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n",
    "    self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n",
    "\n",
    "  def storeRollout(self, state, action, reward):\n",
    "    self.action_buffer.append(action)\n",
    "    self.reward_buffer.append(reward)\n",
    "    self.state_buffer.append(state)\n",
    "\n",
    "  def cleanUp(self):\n",
    "    self.state_buffer  = []\n",
    "    self.reward_buffer = []\n",
    "    self.action_buffer = []"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
