{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-21 13:17:41,363] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model to load. starting new session\n",
      "ep 0: reward: -20.0, mean reward: -20.000000\n",
      "\tep 1: reward: -20.0\n",
      "\tep 2: reward: -20.0\n",
      "\tep 3: reward: -21.0\n",
      "\tep 4: reward: -20.0\n",
      "\tep 5: reward: -21.0\n",
      "\tep 6: reward: -20.0\n",
      "\tep 7: reward: -21.0\n",
      "\tep 8: reward: -20.0\n",
      "\tep 9: reward: -20.0\n",
      "ep 10: reward: -20.0, mean reward: -20.028534\n",
      "\tep 11: reward: -20.0\n",
      "\tep 12: reward: -21.0\n",
      "\tep 13: reward: -19.0\n",
      "\tep 14: reward: -20.0\n",
      "\tep 15: reward: -20.0\n",
      "\tep 16: reward: -21.0\n",
      "\tep 17: reward: -19.0\n",
      "\tep 18: reward: -18.0\n",
      "\tep 19: reward: -21.0\n",
      "ep 20: reward: -21.0, mean reward: -20.025913\n",
      "\tep 21: reward: -21.0\n",
      "\tep 22: reward: -19.0\n",
      "\tep 23: reward: -19.0\n",
      "\tep 24: reward: -20.0\n",
      "\tep 25: reward: -21.0\n",
      "\tep 26: reward: -20.0\n",
      "\tep 27: reward: -20.0\n",
      "\tep 28: reward: -20.0\n",
      "\tep 29: reward: -19.0\n",
      "ep 30: reward: -21.0, mean reward: -20.023632\n",
      "\tep 31: reward: -20.0\n",
      "\tep 32: reward: -21.0\n",
      "\tep 33: reward: -21.0\n",
      "\tep 34: reward: -19.0\n",
      "\tep 35: reward: -20.0\n",
      "\tep 36: reward: -19.0\n",
      "\tep 37: reward: -21.0\n",
      "\tep 38: reward: -21.0\n",
      "\tep 39: reward: -21.0\n",
      "ep 40: reward: -20.0, mean reward: -20.050304\n",
      "\tep 41: reward: -20.0\n",
      "\tep 42: reward: -21.0\n",
      "\tep 43: reward: -20.0\n",
      "\tep 44: reward: -20.0\n",
      "\tep 45: reward: -21.0\n",
      "\tep 46: reward: -19.0\n",
      "\tep 47: reward: -21.0\n",
      "\tep 48: reward: -21.0\n",
      "\tep 49: reward: -19.0\n",
      "SAVED MODEL #50\n",
      "ep 50: reward: -21.0, mean reward: -20.074229\n",
      "\tep 51: reward: -21.0\n",
      "\tep 52: reward: -20.0\n",
      "\tep 53: reward: -20.0\n",
      "\tep 54: reward: -19.0\n",
      "\tep 55: reward: -19.0\n",
      "\tep 56: reward: -21.0\n",
      "\tep 57: reward: -21.0\n",
      "\tep 58: reward: -19.0\n",
      "\tep 59: reward: -21.0\n",
      "ep 60: reward: -20.0, mean reward: -20.076750\n",
      "\tep 61: reward: -21.0\n",
      "\tep 62: reward: -21.0\n",
      "\tep 63: reward: -21.0\n",
      "\tep 64: reward: -20.0\n",
      "\tep 65: reward: -21.0\n",
      "\tep 66: reward: -20.0\n",
      "\tep 67: reward: -21.0\n",
      "\tep 68: reward: -21.0\n",
      "\tep 69: reward: -21.0\n",
      "ep 70: reward: -20.0, mean reward: -20.136009\n",
      "\tep 71: reward: -19.0\n",
      "\tep 72: reward: -21.0\n",
      "\tep 73: reward: -21.0\n",
      "\tep 74: reward: -21.0\n",
      "\tep 75: reward: -20.0\n",
      "\tep 76: reward: -19.0\n",
      "\tep 77: reward: -19.0\n",
      "\tep 78: reward: -21.0\n",
      "\tep 79: reward: -21.0\n",
      "ep 80: reward: -20.0, mean reward: -20.142223\n",
      "\tep 81: reward: -20.0\n",
      "\tep 82: reward: -21.0\n",
      "\tep 83: reward: -20.0\n",
      "\tep 84: reward: -20.0\n",
      "\tep 85: reward: -19.0\n",
      "\tep 86: reward: -19.0\n",
      "\tep 87: reward: -21.0\n",
      "\tep 88: reward: -21.0\n",
      "\tep 89: reward: -20.0\n",
      "ep 90: reward: -21.0, mean reward: -20.148240\n",
      "\tep 91: reward: -20.0\n",
      "\tep 92: reward: -20.0\n",
      "\tep 93: reward: -21.0\n",
      "\tep 94: reward: -21.0\n",
      "\tep 95: reward: -21.0\n",
      "\tep 96: reward: -21.0\n",
      "\tep 97: reward: -21.0\n",
      "\tep 98: reward: -21.0\n",
      "\tep 99: reward: -20.0\n",
      "SAVED MODEL #100\n",
      "ep 100: reward: -20.0, mean reward: -20.191421\n",
      "\tep 101: reward: -21.0\n",
      "\tep 102: reward: -20.0\n",
      "\tep 103: reward: -20.0\n",
      "\tep 104: reward: -18.0\n",
      "\tep 105: reward: -21.0\n",
      "\tep 106: reward: -20.0\n",
      "\tep 107: reward: -20.0\n",
      "\tep 108: reward: -19.0\n",
      "\tep 109: reward: -21.0\n",
      "ep 110: reward: -20.0, mean reward: -20.173032\n",
      "\tep 111: reward: -21.0\n",
      "\tep 112: reward: -20.0\n",
      "\tep 113: reward: -21.0\n",
      "\tep 114: reward: -21.0\n",
      "\tep 115: reward: -21.0\n",
      "\tep 116: reward: -21.0\n",
      "\tep 117: reward: -21.0\n",
      "\tep 118: reward: -18.0\n",
      "\tep 119: reward: -20.0\n",
      "ep 120: reward: -20.0, mean reward: -20.193575\n",
      "\tep 121: reward: -20.0\n",
      "\tep 122: reward: -20.0\n",
      "\tep 123: reward: -20.0\n",
      "\tep 124: reward: -21.0\n",
      "\tep 125: reward: -21.0\n",
      "\tep 126: reward: -20.0\n",
      "\tep 127: reward: -20.0\n",
      "\tep 128: reward: -21.0\n",
      "\tep 129: reward: -21.0\n",
      "ep 130: reward: -21.0, mean reward: -20.223691\n",
      "\tep 131: reward: -21.0\n",
      "\tep 132: reward: -19.0\n",
      "\tep 133: reward: -21.0\n",
      "\tep 134: reward: -19.0\n",
      "\tep 135: reward: -20.0\n",
      "\tep 136: reward: -20.0\n",
      "\tep 137: reward: -20.0\n",
      "\tep 138: reward: -21.0\n",
      "\tep 139: reward: -21.0\n",
      "ep 140: reward: -20.0, mean reward: -20.221817\n",
      "\tep 141: reward: -21.0\n",
      "\tep 142: reward: -21.0\n",
      "\tep 143: reward: -21.0\n",
      "\tep 144: reward: -19.0\n",
      "\tep 145: reward: -20.0\n",
      "\tep 146: reward: -21.0\n",
      "\tep 147: reward: -21.0\n",
      "\tep 148: reward: -21.0\n",
      "\tep 149: reward: -19.0\n",
      "SAVED MODEL #150\n",
      "ep 150: reward: -21.0, mean reward: -20.248086\n",
      "\tep 151: reward: -21.0\n",
      "\tep 152: reward: -20.0\n",
      "\tep 153: reward: -21.0\n",
      "\tep 154: reward: -20.0\n",
      "\tep 155: reward: -21.0\n",
      "\tep 156: reward: -20.0\n",
      "\tep 157: reward: -21.0\n",
      "\tep 158: reward: -20.0\n",
      "\tep 159: reward: -21.0\n",
      "ep 160: reward: -21.0, mean reward: -20.281933\n",
      "\tep 161: reward: -19.0\n",
      "\tep 162: reward: -20.0\n",
      "\tep 163: reward: -21.0\n",
      "\tep 164: reward: -21.0\n",
      "\tep 165: reward: -20.0\n",
      "\tep 166: reward: -19.0\n",
      "\tep 167: reward: -21.0\n",
      "\tep 168: reward: -20.0\n",
      "\tep 169: reward: -21.0\n",
      "ep 170: reward: -20.0, mean reward: -20.274572\n",
      "\tep 171: reward: -20.0\n",
      "\tep 172: reward: -20.0\n",
      "\tep 173: reward: -20.0\n",
      "\tep 174: reward: -20.0\n",
      "\tep 175: reward: -20.0\n",
      "\tep 176: reward: -20.0\n",
      "\tep 177: reward: -20.0\n",
      "\tep 178: reward: -20.0\n",
      "\tep 179: reward: -19.0\n",
      "ep 180: reward: -20.0, mean reward: -20.238418\n",
      "\tep 181: reward: -21.0\n",
      "\tep 182: reward: -21.0\n",
      "\tep 183: reward: -21.0\n",
      "\tep 184: reward: -21.0\n",
      "\tep 185: reward: -20.0\n",
      "\tep 186: reward: -21.0\n",
      "\tep 187: reward: -20.0\n",
      "\tep 188: reward: -20.0\n",
      "\tep 189: reward: -20.0\n",
      "ep 190: reward: -19.0, mean reward: -20.252325\n",
      "\tep 191: reward: -21.0\n",
      "\tep 192: reward: -20.0\n",
      "\tep 193: reward: -21.0\n",
      "\tep 194: reward: -20.0\n",
      "\tep 195: reward: -21.0\n",
      "\tep 196: reward: -21.0\n",
      "\tep 197: reward: -21.0\n",
      "\tep 198: reward: -21.0\n",
      "\tep 199: reward: -21.0\n",
      "SAVED MODEL #200\n",
      "ep 200: reward: -21.0, mean reward: -20.305174\n",
      "\tep 201: reward: -20.0\n",
      "\tep 202: reward: -21.0\n",
      "\tep 203: reward: -19.0\n",
      "\tep 204: reward: -20.0\n",
      "\tep 205: reward: -18.0\n",
      "\tep 206: reward: -21.0\n",
      "\tep 207: reward: -20.0\n",
      "\tep 208: reward: -21.0\n",
      "\tep 209: reward: -20.0\n",
      "ep 210: reward: -18.0, mean reward: -20.256288\n",
      "\tep 211: reward: -20.0\n",
      "\tep 212: reward: -21.0\n",
      "\tep 213: reward: -20.0\n",
      "\tep 214: reward: -20.0\n",
      "\tep 215: reward: -20.0\n",
      "\tep 216: reward: -20.0\n",
      "\tep 217: reward: -21.0\n",
      "\tep 218: reward: -19.0\n",
      "\tep 219: reward: -21.0\n",
      "ep 220: reward: -19.0, mean reward: -20.240812\n",
      "\tep 221: reward: -21.0\n",
      "\tep 222: reward: -21.0\n",
      "\tep 223: reward: -21.0\n",
      "\tep 224: reward: -20.0\n",
      "\tep 225: reward: -21.0\n",
      "\tep 226: reward: -21.0\n",
      "\tep 227: reward: -20.0\n",
      "\tep 228: reward: -21.0\n",
      "\tep 229: reward: -20.0\n",
      "ep 230: reward: -20.0, mean reward: -20.274386\n",
      "\tep 231: reward: -19.0\n",
      "\tep 232: reward: -21.0\n",
      "\tep 233: reward: -21.0\n",
      "\tep 234: reward: -21.0\n",
      "\tep 235: reward: -20.0\n",
      "\tep 236: reward: -21.0\n",
      "\tep 237: reward: -20.0\n",
      "\tep 238: reward: -21.0\n",
      "\tep 239: reward: -21.0\n",
      "ep 240: reward: -20.0, mean reward: -20.296284\n",
      "\tep 241: reward: -21.0\n",
      "\tep 242: reward: -21.0\n",
      "\tep 243: reward: -21.0\n",
      "\tep 244: reward: -20.0\n",
      "\tep 245: reward: -21.0\n",
      "\tep 246: reward: -20.0\n",
      "\tep 247: reward: -21.0\n",
      "\tep 248: reward: -20.0\n",
      "\tep 249: reward: -20.0\n",
      "SAVED MODEL #250\n",
      "ep 250: reward: -18.0, mean reward: -20.294850\n",
      "\tep 251: reward: -21.0\n",
      "\tep 252: reward: -21.0\n",
      "\tep 253: reward: -20.0\n",
      "\tep 254: reward: -20.0\n",
      "\tep 255: reward: -21.0\n",
      "\tep 256: reward: -20.0\n",
      "\tep 257: reward: -19.0\n",
      "\tep 258: reward: -19.0\n",
      "\tep 259: reward: -21.0\n",
      "ep 260: reward: -20.0, mean reward: -20.284926\n",
      "\tep 261: reward: -20.0\n",
      "\tep 262: reward: -20.0\n",
      "\tep 263: reward: -21.0\n",
      "\tep 264: reward: -18.0\n",
      "\tep 265: reward: -21.0\n",
      "\tep 266: reward: -20.0\n",
      "\tep 267: reward: -20.0\n",
      "\tep 268: reward: -21.0\n",
      "\tep 269: reward: -21.0\n",
      "ep 270: reward: -21.0, mean reward: -20.287384\n",
      "\tep 271: reward: -21.0\n",
      "\tep 272: reward: -20.0\n",
      "\tep 273: reward: -20.0\n",
      "\tep 274: reward: -21.0\n",
      "\tep 275: reward: -21.0\n",
      "\tep 276: reward: -19.0\n",
      "\tep 277: reward: -20.0\n",
      "\tep 278: reward: -21.0\n",
      "\tep 279: reward: -20.0\n",
      "ep 280: reward: -19.0, mean reward: -20.278160\n",
      "\tep 281: reward: -21.0\n",
      "\tep 282: reward: -20.0\n",
      "\tep 283: reward: -21.0\n",
      "\tep 284: reward: -21.0\n",
      "\tep 285: reward: -21.0\n",
      "\tep 286: reward: -21.0\n",
      "\tep 287: reward: -21.0\n",
      "\tep 288: reward: -19.0\n",
      "\tep 289: reward: -20.0\n",
      "ep 290: reward: -21.0, mean reward: -20.308451\n",
      "\tep 291: reward: -20.0\n",
      "\tep 292: reward: -20.0\n",
      "\tep 293: reward: -19.0\n",
      "\tep 294: reward: -18.0\n",
      "\tep 295: reward: -21.0\n",
      "\tep 296: reward: -19.0\n",
      "\tep 297: reward: -19.0\n",
      "\tep 298: reward: -21.0\n",
      "\tep 299: reward: -20.0\n",
      "SAVED MODEL #300\n",
      "ep 300: reward: -21.0, mean reward: -20.260809\n",
      "\tep 301: reward: -21.0\n",
      "\tep 302: reward: -21.0\n",
      "\tep 303: reward: -20.0\n",
      "\tep 304: reward: -21.0\n",
      "\tep 305: reward: -21.0\n",
      "\tep 306: reward: -21.0\n",
      "\tep 307: reward: -21.0\n",
      "\tep 308: reward: -21.0\n",
      "\tep 309: reward: -20.0\n",
      "ep 310: reward: -20.0, mean reward: -20.302269\n",
      "\tep 311: reward: -21.0\n",
      "\tep 312: reward: -20.0\n",
      "\tep 313: reward: -19.0\n",
      "\tep 314: reward: -21.0\n",
      "\tep 315: reward: -20.0\n",
      "\tep 316: reward: -21.0\n",
      "\tep 317: reward: -21.0\n",
      "\tep 318: reward: -20.0\n",
      "\tep 319: reward: -21.0\n",
      "ep 320: reward: -21.0, mean reward: -20.321805\n",
      "\tep 321: reward: -20.0\n",
      "\tep 322: reward: -20.0\n",
      "\tep 323: reward: -21.0\n",
      "\tep 324: reward: -21.0\n",
      "\tep 325: reward: -20.0\n",
      "\tep 326: reward: -21.0\n",
      "\tep 327: reward: -20.0\n",
      "\tep 328: reward: -21.0\n",
      "\tep 329: reward: -21.0\n",
      "ep 330: reward: -20.0, mean reward: -20.339077\n",
      "\tep 331: reward: -19.0\n",
      "\tep 332: reward: -20.0\n",
      "\tep 333: reward: -20.0\n",
      "\tep 334: reward: -18.0\n",
      "\tep 335: reward: -21.0\n",
      "\tep 336: reward: -20.0\n",
      "\tep 337: reward: -20.0\n",
      "\tep 338: reward: -21.0\n",
      "\tep 339: reward: -20.0\n",
      "ep 340: reward: -21.0, mean reward: -20.308001\n",
      "\tep 341: reward: -19.0\n",
      "\tep 342: reward: -20.0\n",
      "\tep 343: reward: -21.0\n",
      "\tep 344: reward: -20.0\n",
      "\tep 345: reward: -19.0\n",
      "\tep 346: reward: -20.0\n",
      "\tep 347: reward: -21.0\n",
      "\tep 348: reward: -21.0\n",
      "\tep 349: reward: -21.0\n",
      "SAVED MODEL #350\n",
      "ep 350: reward: -21.0, mean reward: -20.308630\n",
      "\tep 351: reward: -18.0\n",
      "\tep 352: reward: -20.0\n",
      "\tep 353: reward: -21.0\n",
      "\tep 354: reward: -19.0\n",
      "\tep 355: reward: -20.0\n",
      "\tep 356: reward: -20.0\n",
      "\tep 357: reward: -18.0\n",
      "\tep 358: reward: -20.0\n",
      "\tep 359: reward: -20.0\n",
      "ep 360: reward: -20.0, mean reward: -20.241349\n",
      "\tep 361: reward: -20.0\n",
      "\tep 362: reward: -19.0\n",
      "\tep 363: reward: -20.0\n",
      "\tep 364: reward: -21.0\n",
      "\tep 365: reward: -21.0\n",
      "\tep 366: reward: -19.0\n",
      "\tep 367: reward: -21.0\n",
      "\tep 368: reward: -21.0\n",
      "\tep 369: reward: -20.0\n",
      "ep 370: reward: -20.0, mean reward: -20.237867\n",
      "\tep 371: reward: -20.0\n",
      "\tep 372: reward: -21.0\n",
      "\tep 373: reward: -18.0\n",
      "\tep 374: reward: -20.0\n",
      "\tep 375: reward: -21.0\n",
      "\tep 376: reward: -18.0\n",
      "\tep 377: reward: -21.0\n",
      "\tep 378: reward: -21.0\n",
      "\tep 379: reward: -20.0\n",
      "ep 380: reward: -20.0, mean reward: -20.215511\n",
      "\tep 381: reward: -20.0\n",
      "\tep 382: reward: -20.0\n",
      "\tep 383: reward: -21.0\n",
      "\tep 384: reward: -20.0\n",
      "\tep 385: reward: -21.0\n",
      "\tep 386: reward: -20.0\n",
      "\tep 387: reward: -21.0\n",
      "\tep 388: reward: -20.0\n",
      "\tep 389: reward: -20.0\n",
      "ep 390: reward: -21.0, mean reward: -20.233438\n",
      "\tep 391: reward: -19.0\n",
      "\tep 392: reward: -21.0\n",
      "\tep 393: reward: -21.0\n",
      "\tep 394: reward: -21.0\n",
      "\tep 395: reward: -21.0\n",
      "\tep 396: reward: -20.0\n",
      "\tep 397: reward: -21.0\n",
      "\tep 398: reward: -21.0\n",
      "\tep 399: reward: -21.0\n",
      "SAVED MODEL #400\n",
      "ep 400: reward: -21.0, mean reward: -20.278859\n",
      "\tep 401: reward: -20.0\n",
      "\tep 402: reward: -21.0\n",
      "\tep 403: reward: -19.0\n",
      "\tep 404: reward: -21.0\n",
      "\tep 405: reward: -21.0\n",
      "\tep 406: reward: -20.0\n",
      "\tep 407: reward: -21.0\n",
      "\tep 408: reward: -21.0\n",
      "\tep 409: reward: -21.0\n",
      "ep 410: reward: -20.0, mean reward: -20.300430\n",
      "\tep 411: reward: -18.0\n",
      "\tep 412: reward: -17.0\n",
      "\tep 413: reward: -21.0\n",
      "\tep 414: reward: -21.0\n",
      "\tep 415: reward: -21.0\n",
      "\tep 416: reward: -20.0\n",
      "\tep 417: reward: -21.0\n",
      "\tep 418: reward: -20.0\n",
      "\tep 419: reward: -20.0\n",
      "ep 420: reward: -21.0, mean reward: -20.273699\n",
      "\tep 421: reward: -20.0\n",
      "\tep 422: reward: -19.0\n",
      "\tep 423: reward: -20.0\n",
      "\tep 424: reward: -20.0\n",
      "\tep 425: reward: -19.0\n",
      "\tep 426: reward: -21.0\n",
      "\tep 427: reward: -20.0\n",
      "\tep 428: reward: -21.0\n",
      "\tep 429: reward: -20.0\n",
      "ep 430: reward: -19.0, mean reward: -20.238198\n",
      "\tep 431: reward: -21.0\n",
      "\tep 432: reward: -21.0\n",
      "\tep 433: reward: -21.0\n",
      "\tep 434: reward: -21.0\n",
      "\tep 435: reward: -20.0\n",
      "\tep 436: reward: -21.0\n",
      "\tep 437: reward: -21.0\n",
      "\tep 438: reward: -20.0\n",
      "\tep 439: reward: -21.0\n",
      "ep 440: reward: -21.0, mean reward: -20.291729\n",
      "\tep 441: reward: -20.0\n",
      "\tep 442: reward: -20.0\n",
      "\tep 443: reward: -21.0\n",
      "\tep 444: reward: -21.0\n",
      "\tep 445: reward: -21.0\n",
      "\tep 446: reward: -21.0\n",
      "\tep 447: reward: -20.0\n",
      "\tep 448: reward: -20.0\n",
      "\tep 449: reward: -21.0\n",
      "SAVED MODEL #450\n",
      "ep 450: reward: -21.0, mean reward: -20.321586\n",
      "\tep 451: reward: -19.0\n",
      "\tep 452: reward: -20.0\n",
      "\tep 453: reward: -19.0\n",
      "\tep 454: reward: -19.0\n",
      "\tep 455: reward: -20.0\n",
      "\tep 456: reward: -21.0\n",
      "\tep 457: reward: -21.0\n",
      "\tep 458: reward: -20.0\n",
      "\tep 459: reward: -19.0\n",
      "ep 460: reward: -19.0, mean reward: -20.262375\n",
      "\tep 461: reward: -21.0\n",
      "\tep 462: reward: -20.0\n",
      "\tep 463: reward: -21.0\n",
      "\tep 464: reward: -21.0\n",
      "\tep 465: reward: -21.0\n",
      "\tep 466: reward: -21.0\n",
      "\tep 467: reward: -21.0\n",
      "\tep 468: reward: -20.0\n",
      "\tep 469: reward: -20.0\n",
      "ep 470: reward: -19.0, mean reward: -20.283977\n",
      "\tep 471: reward: -21.0\n",
      "\tep 472: reward: -19.0\n",
      "\tep 473: reward: -20.0\n",
      "\tep 474: reward: -19.0\n",
      "\tep 475: reward: -21.0\n",
      "\tep 476: reward: -21.0\n",
      "\tep 477: reward: -21.0\n",
      "\tep 478: reward: -21.0\n",
      "\tep 479: reward: -20.0\n",
      "ep 480: reward: -18.0, mean reward: -20.265936\n",
      "\tep 481: reward: -21.0\n",
      "\tep 482: reward: -20.0\n",
      "\tep 483: reward: -18.0\n",
      "\tep 484: reward: -20.0\n",
      "\tep 485: reward: -19.0\n",
      "\tep 486: reward: -21.0\n",
      "\tep 487: reward: -20.0\n",
      "\tep 488: reward: -19.0\n",
      "\tep 489: reward: -18.0\n",
      "ep 490: reward: -19.0, mean reward: -20.191497\n",
      "\tep 491: reward: -21.0\n",
      "\tep 492: reward: -21.0\n",
      "\tep 493: reward: -20.0\n",
      "\tep 494: reward: -20.0\n",
      "\tep 495: reward: -20.0\n",
      "\tep 496: reward: -19.0\n",
      "\tep 497: reward: -20.0\n",
      "\tep 498: reward: -20.0\n",
      "\tep 499: reward: -21.0\n",
      "SAVED MODEL #500\n",
      "ep 500: reward: -21.0, mean reward: -20.201843\n",
      "\tep 501: reward: -19.0\n",
      "\tep 502: reward: -21.0\n",
      "\tep 503: reward: -21.0\n",
      "\tep 504: reward: -20.0\n",
      "\tep 505: reward: -21.0\n",
      "\tep 506: reward: -21.0\n",
      "\tep 507: reward: -21.0\n",
      "\tep 508: reward: -21.0\n",
      "\tep 509: reward: -20.0\n",
      "ep 510: reward: -21.0, mean reward: -20.240576\n",
      "\tep 511: reward: -21.0\n",
      "\tep 512: reward: -21.0\n",
      "\tep 513: reward: -18.0\n",
      "\tep 514: reward: -19.0\n",
      "\tep 515: reward: -19.0\n",
      "\tep 516: reward: -21.0\n",
      "\tep 517: reward: -20.0\n",
      "\tep 518: reward: -20.0\n",
      "\tep 519: reward: -19.0\n",
      "ep 520: reward: -20.0, mean reward: -20.198075\n",
      "\tep 521: reward: -20.0\n",
      "\tep 522: reward: -20.0\n",
      "\tep 523: reward: -21.0\n",
      "\tep 524: reward: -19.0\n",
      "\tep 525: reward: -20.0\n",
      "\tep 526: reward: -20.0\n",
      "\tep 527: reward: -20.0\n",
      "\tep 528: reward: -21.0\n",
      "\tep 529: reward: -19.0\n",
      "ep 530: reward: -19.0, mean reward: -20.168942\n",
      "\tep 531: reward: -18.0\n",
      "\tep 532: reward: -20.0\n",
      "\tep 533: reward: -20.0\n",
      "\tep 534: reward: -21.0\n",
      "\tep 535: reward: -20.0\n",
      "\tep 536: reward: -20.0\n",
      "\tep 537: reward: -20.0\n",
      "\tep 538: reward: -21.0\n",
      "\tep 539: reward: -20.0\n",
      "ep 540: reward: -20.0, mean reward: -20.153734\n",
      "\tep 541: reward: -19.0\n",
      "\tep 542: reward: -20.0\n",
      "\tep 543: reward: -19.0\n",
      "\tep 544: reward: -21.0\n",
      "\tep 545: reward: -21.0\n",
      "\tep 546: reward: -21.0\n",
      "\tep 547: reward: -21.0\n",
      "\tep 548: reward: -21.0\n",
      "\tep 549: reward: -21.0\n",
      "SAVED MODEL #550\n",
      "ep 550: reward: -20.0, mean reward: -20.178513\n",
      "\tep 551: reward: -20.0\n",
      "\tep 552: reward: -21.0\n",
      "\tep 553: reward: -20.0\n",
      "\tep 554: reward: -20.0\n",
      "\tep 555: reward: -21.0\n",
      "\tep 556: reward: -20.0\n",
      "\tep 557: reward: -18.0\n",
      "\tep 558: reward: -21.0\n",
      "\tep 559: reward: -20.0\n",
      "ep 560: reward: -20.0, mean reward: -20.170576\n",
      "\tep 561: reward: -19.0\n",
      "\tep 562: reward: -20.0\n",
      "\tep 563: reward: -20.0\n",
      "\tep 564: reward: -18.0\n",
      "\tep 565: reward: -18.0\n",
      "\tep 566: reward: -19.0\n",
      "\tep 567: reward: -20.0\n",
      "\tep 568: reward: -21.0\n",
      "\tep 569: reward: -19.0\n",
      "ep 570: reward: -19.0, mean reward: -20.087577\n",
      "\tep 571: reward: -20.0\n",
      "\tep 572: reward: -19.0\n",
      "\tep 573: reward: -21.0\n",
      "\tep 574: reward: -18.0\n",
      "\tep 575: reward: -19.0\n",
      "\tep 576: reward: -21.0\n",
      "\tep 577: reward: -21.0\n",
      "\tep 578: reward: -20.0\n",
      "\tep 579: reward: -21.0\n",
      "ep 580: reward: -20.0, mean reward: -20.080165\n",
      "\tep 581: reward: -21.0\n",
      "\tep 582: reward: -21.0\n",
      "\tep 583: reward: -20.0\n",
      "\tep 584: reward: -18.0\n",
      "\tep 585: reward: -19.0\n",
      "\tep 586: reward: -20.0\n",
      "\tep 587: reward: -20.0\n",
      "\tep 588: reward: -18.0\n",
      "\tep 589: reward: -20.0\n",
      "ep 590: reward: -19.0, mean reward: -20.032921\n",
      "\tep 591: reward: -18.0\n",
      "\tep 592: reward: -20.0\n",
      "\tep 593: reward: -20.0\n",
      "\tep 594: reward: -20.0\n",
      "\tep 595: reward: -21.0\n",
      "\tep 596: reward: -21.0\n",
      "\tep 597: reward: -21.0\n",
      "\tep 598: reward: -20.0\n",
      "\tep 599: reward: -19.0\n",
      "SAVED MODEL #600\n",
      "ep 600: reward: -18.0, mean reward: -20.010422\n",
      "\tep 601: reward: -20.0\n",
      "\tep 602: reward: -20.0\n",
      "\tep 603: reward: -21.0\n",
      "\tep 604: reward: -21.0\n",
      "\tep 605: reward: -21.0\n",
      "\tep 606: reward: -21.0\n",
      "\tep 607: reward: -21.0\n",
      "\tep 608: reward: -20.0\n",
      "\tep 609: reward: -19.0\n",
      "ep 610: reward: -17.0, mean reward: -20.017080\n",
      "\tep 611: reward: -19.0\n",
      "\tep 612: reward: -21.0\n",
      "\tep 613: reward: -20.0\n",
      "\tep 614: reward: -20.0\n",
      "\tep 615: reward: -21.0\n",
      "\tep 616: reward: -19.0\n",
      "\tep 617: reward: -20.0\n",
      "\tep 618: reward: -21.0\n",
      "\tep 619: reward: -19.0\n",
      "ep 620: reward: -20.0, mean reward: -20.015344\n",
      "\tep 621: reward: -20.0\n",
      "\tep 622: reward: -20.0\n",
      "\tep 623: reward: -19.0\n",
      "\tep 624: reward: -20.0\n",
      "\tep 625: reward: -17.0\n",
      "\tep 626: reward: -19.0\n",
      "\tep 627: reward: -19.0\n",
      "\tep 628: reward: -18.0\n",
      "\tep 629: reward: -19.0\n",
      "ep 630: reward: -20.0, mean reward: -19.927215\n",
      "\tep 631: reward: -19.0\n",
      "\tep 632: reward: -19.0\n",
      "\tep 633: reward: -20.0\n",
      "\tep 634: reward: -19.0\n",
      "\tep 635: reward: -21.0\n",
      "\tep 636: reward: -20.0\n",
      "\tep 637: reward: -19.0\n",
      "\tep 638: reward: -20.0\n",
      "\tep 639: reward: -19.0\n",
      "ep 640: reward: -17.0, mean reward: -19.866304\n",
      "\tep 641: reward: -20.0\n",
      "\tep 642: reward: -20.0\n",
      "\tep 643: reward: -18.0\n",
      "\tep 644: reward: -21.0\n",
      "\tep 645: reward: -21.0\n",
      "\tep 646: reward: -21.0\n",
      "\tep 647: reward: -21.0\n",
      "\tep 648: reward: -20.0\n",
      "\tep 649: reward: -20.0\n",
      "SAVED MODEL #650\n",
      "ep 650: reward: -21.0, mean reward: -19.908680\n",
      "\tep 651: reward: -19.0\n",
      "\tep 652: reward: -18.0\n",
      "\tep 653: reward: -19.0\n",
      "\tep 654: reward: -20.0\n",
      "\tep 655: reward: -21.0\n",
      "\tep 656: reward: -19.0\n",
      "\tep 657: reward: -20.0\n",
      "\tep 658: reward: -19.0\n",
      "\tep 659: reward: -20.0\n",
      "ep 660: reward: -21.0, mean reward: -19.880604\n",
      "\tep 661: reward: -20.0\n",
      "\tep 662: reward: -20.0\n",
      "\tep 663: reward: -19.0\n",
      "\tep 664: reward: -20.0\n",
      "\tep 665: reward: -20.0\n",
      "\tep 666: reward: -19.0\n",
      "\tep 667: reward: -20.0\n",
      "\tep 668: reward: -17.0\n",
      "\tep 669: reward: -21.0\n",
      "ep 670: reward: -21.0, mean reward: -19.863591\n",
      "\tep 671: reward: -21.0\n",
      "\tep 672: reward: -20.0\n",
      "\tep 673: reward: -21.0\n",
      "\tep 674: reward: -19.0\n",
      "\tep 675: reward: -20.0\n",
      "\tep 676: reward: -20.0\n",
      "\tep 677: reward: -19.0\n",
      "\tep 678: reward: -20.0\n",
      "\tep 679: reward: -21.0\n",
      "ep 680: reward: -21.0, mean reward: -19.895872\n",
      "\tep 681: reward: -21.0\n",
      "\tep 682: reward: -19.0\n",
      "\tep 683: reward: -20.0\n",
      "\tep 684: reward: -21.0\n",
      "\tep 685: reward: -18.0\n",
      "\tep 686: reward: -20.0\n",
      "\tep 687: reward: -18.0\n",
      "\tep 688: reward: -17.0\n",
      "\tep 689: reward: -21.0\n",
      "ep 690: reward: -21.0, mean reward: -19.867223\n",
      "\tep 691: reward: -21.0\n",
      "\tep 692: reward: -20.0\n",
      "\tep 693: reward: -20.0\n",
      "\tep 694: reward: -20.0\n",
      "\tep 695: reward: -20.0\n",
      "\tep 696: reward: -20.0\n",
      "\tep 697: reward: -21.0\n",
      "\tep 698: reward: -20.0\n",
      "\tep 699: reward: -21.0\n",
      "SAVED MODEL #700\n",
      "ep 700: reward: -19.0, mean reward: -19.898657\n",
      "\tep 701: reward: -21.0\n",
      "\tep 702: reward: -20.0\n",
      "\tep 703: reward: -21.0\n",
      "\tep 704: reward: -20.0\n",
      "\tep 705: reward: -15.0\n",
      "\tep 706: reward: -19.0\n",
      "\tep 707: reward: -18.0\n",
      "\tep 708: reward: -20.0\n",
      "\tep 709: reward: -20.0\n",
      "ep 710: reward: -16.0, mean reward: -19.810241\n",
      "\tep 711: reward: -18.0\n",
      "\tep 712: reward: -20.0\n",
      "\tep 713: reward: -20.0\n",
      "\tep 714: reward: -20.0\n",
      "\tep 715: reward: -20.0\n",
      "\tep 716: reward: -21.0\n",
      "\tep 717: reward: -19.0\n",
      "\tep 718: reward: -21.0\n",
      "\tep 719: reward: -19.0\n",
      "ep 720: reward: -19.0, mean reward: -19.799919\n",
      "\tep 721: reward: -20.0\n",
      "\tep 722: reward: -19.0\n",
      "\tep 723: reward: -19.0\n",
      "\tep 724: reward: -19.0\n",
      "\tep 725: reward: -20.0\n",
      "\tep 726: reward: -18.0\n",
      "\tep 727: reward: -20.0\n",
      "\tep 728: reward: -18.0\n",
      "\tep 729: reward: -20.0\n",
      "ep 730: reward: -18.0, mean reward: -19.732274\n",
      "\tep 731: reward: -17.0\n",
      "\tep 732: reward: -19.0\n",
      "\tep 733: reward: -20.0\n",
      "\tep 734: reward: -19.0\n",
      "\tep 735: reward: -18.0\n",
      "\tep 736: reward: -19.0\n",
      "\tep 737: reward: -18.0\n",
      "\tep 738: reward: -21.0\n",
      "\tep 739: reward: -20.0\n",
      "ep 740: reward: -18.0, mean reward: -19.653595\n",
      "\tep 741: reward: -20.0\n",
      "\tep 742: reward: -19.0\n",
      "\tep 743: reward: -18.0\n",
      "\tep 744: reward: -19.0\n",
      "\tep 745: reward: -18.0\n",
      "\tep 746: reward: -21.0\n",
      "\tep 747: reward: -21.0\n",
      "\tep 748: reward: -19.0\n",
      "\tep 749: reward: -20.0\n",
      "SAVED MODEL #750\n",
      "ep 750: reward: -20.0, mean reward: -19.639922\n",
      "\tep 751: reward: -19.0\n",
      "\tep 752: reward: -18.0\n",
      "\tep 753: reward: -20.0\n",
      "\tep 754: reward: -18.0\n",
      "\tep 755: reward: -18.0\n",
      "\tep 756: reward: -19.0\n",
      "\tep 757: reward: -21.0\n",
      "\tep 758: reward: -18.0\n",
      "\tep 759: reward: -20.0\n",
      "ep 760: reward: -19.0, mean reward: -19.579407\n",
      "\tep 761: reward: -19.0\n",
      "\tep 762: reward: -19.0\n",
      "\tep 763: reward: -21.0\n",
      "\tep 764: reward: -20.0\n",
      "\tep 765: reward: -20.0\n",
      "\tep 766: reward: -20.0\n",
      "\tep 767: reward: -18.0\n",
      "\tep 768: reward: -17.0\n",
      "\tep 769: reward: -18.0\n",
      "ep 770: reward: -21.0, mean reward: -19.551973\n",
      "\tep 771: reward: -19.0\n",
      "\tep 772: reward: -20.0\n",
      "\tep 773: reward: -18.0\n",
      "\tep 774: reward: -19.0\n",
      "\tep 775: reward: -20.0\n",
      "\tep 776: reward: -19.0\n",
      "\tep 777: reward: -18.0\n",
      "\tep 778: reward: -21.0\n",
      "\tep 779: reward: -18.0\n",
      "ep 780: reward: -21.0, mean reward: -19.528610\n",
      "\tep 781: reward: -20.0\n",
      "\tep 782: reward: -18.0\n",
      "\tep 783: reward: -17.0\n",
      "\tep 784: reward: -19.0\n",
      "\tep 785: reward: -20.0\n",
      "\tep 786: reward: -19.0\n",
      "\tep 787: reward: -20.0\n",
      "\tep 788: reward: -19.0\n",
      "\tep 789: reward: -18.0\n",
      "ep 790: reward: -20.0, mean reward: -19.478645\n",
      "\tep 791: reward: -20.0\n",
      "\tep 792: reward: -20.0\n",
      "\tep 793: reward: -20.0\n",
      "\tep 794: reward: -20.0\n",
      "\tep 795: reward: -18.0\n",
      "\tep 796: reward: -17.0\n",
      "\tep 797: reward: -21.0\n",
      "\tep 798: reward: -20.0\n",
      "\tep 799: reward: -19.0\n",
      "SAVED MODEL #800\n",
      "ep 800: reward: -18.0, mean reward: -19.460461\n",
      "\tep 801: reward: -20.0\n",
      "\tep 802: reward: -21.0\n",
      "\tep 803: reward: -18.0\n",
      "\tep 804: reward: -20.0\n",
      "\tep 805: reward: -20.0\n",
      "\tep 806: reward: -19.0\n",
      "\tep 807: reward: -19.0\n",
      "\tep 808: reward: -18.0\n",
      "\tep 809: reward: -19.0\n",
      "ep 810: reward: -20.0, mean reward: -19.453826\n",
      "\tep 811: reward: -18.0\n",
      "\tep 812: reward: -21.0\n",
      "\tep 813: reward: -18.0\n",
      "\tep 814: reward: -19.0\n",
      "\tep 815: reward: -19.0\n",
      "\tep 816: reward: -16.0\n",
      "\tep 817: reward: -20.0\n",
      "\tep 818: reward: -21.0\n",
      "\tep 819: reward: -21.0\n",
      "ep 820: reward: -19.0, mean reward: -19.430718\n",
      "\tep 821: reward: -19.0\n",
      "\tep 822: reward: -20.0\n",
      "\tep 823: reward: -19.0\n",
      "\tep 824: reward: -20.0\n",
      "\tep 825: reward: -20.0\n",
      "\tep 826: reward: -17.0\n",
      "\tep 827: reward: -17.0\n",
      "\tep 828: reward: -16.0\n",
      "\tep 829: reward: -20.0\n",
      "ep 830: reward: -19.0, mean reward: -19.359565\n",
      "\tep 831: reward: -19.0\n",
      "\tep 832: reward: -18.0\n",
      "\tep 833: reward: -20.0\n",
      "\tep 834: reward: -20.0\n",
      "\tep 835: reward: -18.0\n",
      "\tep 836: reward: -18.0\n",
      "\tep 837: reward: -19.0\n",
      "\tep 838: reward: -21.0\n",
      "\tep 839: reward: -17.0\n",
      "ep 840: reward: -17.0, mean reward: -19.295378\n",
      "\tep 841: reward: -17.0\n",
      "\tep 842: reward: -19.0\n",
      "\tep 843: reward: -18.0\n",
      "\tep 844: reward: -18.0\n",
      "\tep 845: reward: -19.0\n",
      "\tep 846: reward: -17.0\n",
      "\tep 847: reward: -20.0\n",
      "\tep 848: reward: -17.0\n",
      "\tep 849: reward: -18.0\n",
      "SAVED MODEL #850\n",
      "ep 850: reward: -18.0, mean reward: -19.181118\n",
      "\tep 851: reward: -19.0\n",
      "\tep 852: reward: -20.0\n",
      "\tep 853: reward: -19.0\n",
      "\tep 854: reward: -21.0\n",
      "\tep 855: reward: -19.0\n",
      "\tep 856: reward: -20.0\n",
      "\tep 857: reward: -21.0\n",
      "\tep 858: reward: -19.0\n",
      "\tep 859: reward: -18.0\n",
      "ep 860: reward: -16.0, mean reward: -19.180969\n",
      "\tep 861: reward: -19.0\n",
      "\tep 862: reward: -20.0\n",
      "\tep 863: reward: -20.0\n",
      "\tep 864: reward: -18.0\n",
      "\tep 865: reward: -17.0\n",
      "\tep 866: reward: -21.0\n",
      "\tep 867: reward: -19.0\n",
      "\tep 868: reward: -18.0\n",
      "\tep 869: reward: -16.0\n",
      "ep 870: reward: -20.0, mean reward: -19.143489\n",
      "\tep 871: reward: -18.0\n",
      "\tep 872: reward: -17.0\n",
      "\tep 873: reward: -18.0\n",
      "\tep 874: reward: -15.0\n",
      "\tep 875: reward: -18.0\n",
      "\tep 876: reward: -21.0\n",
      "\tep 877: reward: -18.0\n",
      "\tep 878: reward: -16.0\n",
      "\tep 879: reward: -19.0\n",
      "ep 880: reward: -21.0, mean reward: -19.045795\n",
      "\tep 881: reward: -17.0\n",
      "\tep 882: reward: -17.0\n",
      "\tep 883: reward: -19.0\n",
      "\tep 884: reward: -21.0\n",
      "\tep 885: reward: -21.0\n",
      "\tep 886: reward: -19.0\n",
      "\tep 887: reward: -17.0\n",
      "\tep 888: reward: -16.0\n",
      "\tep 889: reward: -19.0\n",
      "ep 890: reward: -20.0, mean reward: -19.003732\n",
      "\tep 891: reward: -19.0\n",
      "\tep 892: reward: -21.0\n",
      "\tep 893: reward: -21.0\n",
      "\tep 894: reward: -20.0\n",
      "\tep 895: reward: -21.0\n",
      "\tep 896: reward: -18.0\n",
      "\tep 897: reward: -19.0\n",
      "\tep 898: reward: -20.0\n",
      "\tep 899: reward: -20.0\n",
      "SAVED MODEL #900\n",
      "ep 900: reward: -18.0, mean reward: -19.069001\n",
      "\tep 901: reward: -18.0\n",
      "\tep 902: reward: -20.0\n",
      "\tep 903: reward: -19.0\n",
      "\tep 904: reward: -20.0\n",
      "\tep 905: reward: -16.0\n",
      "\tep 906: reward: -18.0\n",
      "\tep 907: reward: -19.0\n",
      "\tep 908: reward: -18.0\n",
      "\tep 909: reward: -19.0\n",
      "ep 910: reward: -20.0, mean reward: -19.033973\n",
      "\tep 911: reward: -15.0\n",
      "\tep 912: reward: -21.0\n",
      "\tep 913: reward: -19.0\n",
      "\tep 914: reward: -20.0\n",
      "\tep 915: reward: -17.0\n",
      "\tep 916: reward: -17.0\n",
      "\tep 917: reward: -16.0\n",
      "\tep 918: reward: -19.0\n",
      "\tep 919: reward: -19.0\n",
      "ep 920: reward: -19.0, mean reward: -18.954713\n",
      "\tep 921: reward: -19.0\n",
      "\tep 922: reward: -19.0\n",
      "\tep 923: reward: -18.0\n",
      "\tep 924: reward: -20.0\n",
      "\tep 925: reward: -17.0\n",
      "\tep 926: reward: -18.0\n",
      "\tep 927: reward: -17.0\n",
      "\tep 928: reward: -18.0\n",
      "\tep 929: reward: -20.0\n",
      "ep 930: reward: -20.0, mean reward: -18.921205\n",
      "\tep 931: reward: -18.0\n",
      "\tep 932: reward: -19.0\n",
      "\tep 933: reward: -20.0\n",
      "\tep 934: reward: -19.0\n",
      "\tep 935: reward: -19.0\n",
      "\tep 936: reward: -19.0\n",
      "\tep 937: reward: -17.0\n",
      "\tep 938: reward: -19.0\n",
      "\tep 939: reward: -19.0\n",
      "ep 940: reward: -17.0, mean reward: -18.889519\n",
      "\tep 941: reward: -20.0\n",
      "\tep 942: reward: -17.0\n",
      "\tep 943: reward: -20.0\n",
      "\tep 944: reward: -16.0\n",
      "\tep 945: reward: -19.0\n",
      "\tep 946: reward: -20.0\n",
      "\tep 947: reward: -18.0\n",
      "\tep 948: reward: -20.0\n",
      "\tep 949: reward: -18.0\n",
      "SAVED MODEL #950\n",
      "ep 950: reward: -18.0, mean reward: -18.861643\n",
      "\tep 951: reward: -18.0\n",
      "\tep 952: reward: -20.0\n",
      "\tep 953: reward: -19.0\n",
      "\tep 954: reward: -17.0\n",
      "\tep 955: reward: -17.0\n",
      "\tep 956: reward: -20.0\n",
      "\tep 957: reward: -18.0\n",
      "\tep 958: reward: -19.0\n",
      "\tep 959: reward: -18.0\n",
      "ep 960: reward: -19.0, mean reward: -18.827118\n",
      "\tep 961: reward: -20.0\n",
      "\tep 962: reward: -21.0\n",
      "\tep 963: reward: -19.0\n",
      "\tep 964: reward: -19.0\n",
      "\tep 965: reward: -18.0\n",
      "\tep 966: reward: -21.0\n",
      "\tep 967: reward: -17.0\n",
      "\tep 968: reward: -20.0\n",
      "\tep 969: reward: -14.0\n",
      "ep 970: reward: -18.0, mean reward: -18.811836\n",
      "\tep 971: reward: -17.0\n",
      "\tep 972: reward: -17.0\n",
      "\tep 973: reward: -17.0\n",
      "\tep 974: reward: -19.0\n",
      "\tep 975: reward: -20.0\n",
      "\tep 976: reward: -20.0\n",
      "\tep 977: reward: -20.0\n",
      "\tep 978: reward: -18.0\n",
      "\tep 979: reward: -19.0\n",
      "ep 980: reward: -15.0, mean reward: -18.753479\n",
      "\tep 981: reward: -18.0\n",
      "\tep 982: reward: -18.0\n",
      "\tep 983: reward: -19.0\n",
      "\tep 984: reward: -16.0\n",
      "\tep 985: reward: -19.0\n",
      "\tep 986: reward: -19.0\n",
      "\tep 987: reward: -18.0\n",
      "\tep 988: reward: -21.0\n",
      "\tep 989: reward: -20.0\n",
      "ep 990: reward: -20.0, mean reward: -18.760243\n",
      "\tep 991: reward: -18.0\n",
      "\tep 992: reward: -17.0\n",
      "\tep 993: reward: -17.0\n",
      "\tep 994: reward: -18.0\n",
      "\tep 995: reward: -19.0\n",
      "\tep 996: reward: -18.0\n",
      "\tep 997: reward: -18.0\n",
      "\tep 998: reward: -19.0\n",
      "\tep 999: reward: -15.0\n",
      "SAVED MODEL #1000\n",
      "ep 1000: reward: -16.0, mean reward: -18.638613\n",
      "\tep 1001: reward: -18.0\n",
      "\tep 1002: reward: -21.0\n",
      "\tep 1003: reward: -17.0\n",
      "\tep 1004: reward: -20.0\n",
      "\tep 1005: reward: -18.0\n",
      "\tep 1006: reward: -19.0\n",
      "\tep 1007: reward: -17.0\n",
      "\tep 1008: reward: -20.0\n",
      "\tep 1009: reward: -17.0\n",
      "ep 1010: reward: -18.0, mean reward: -18.624346\n",
      "\tep 1011: reward: -20.0\n",
      "\tep 1012: reward: -19.0\n",
      "\tep 1013: reward: -19.0\n",
      "\tep 1014: reward: -19.0\n",
      "\tep 1015: reward: -17.0\n",
      "\tep 1016: reward: -21.0\n",
      "\tep 1017: reward: -18.0\n",
      "\tep 1018: reward: -18.0\n",
      "\tep 1019: reward: -21.0\n",
      "ep 1020: reward: -15.0, mean reward: -18.629889\n",
      "\tep 1021: reward: -18.0\n",
      "\tep 1022: reward: -17.0\n",
      "\tep 1023: reward: -19.0\n",
      "\tep 1024: reward: -17.0\n",
      "\tep 1025: reward: -19.0\n",
      "\tep 1026: reward: -19.0\n",
      "\tep 1027: reward: -19.0\n",
      "\tep 1028: reward: -15.0\n",
      "\tep 1029: reward: -18.0\n",
      "ep 1030: reward: -16.0, mean reward: -18.539754\n",
      "\tep 1031: reward: -15.0\n",
      "\tep 1032: reward: -21.0\n",
      "\tep 1033: reward: -18.0\n",
      "\tep 1034: reward: -19.0\n",
      "\tep 1035: reward: -18.0\n",
      "\tep 1036: reward: -15.0\n",
      "\tep 1037: reward: -19.0\n",
      "\tep 1038: reward: -19.0\n",
      "\tep 1039: reward: -21.0\n",
      "ep 1040: reward: -17.0, mean reward: -18.508222\n",
      "\tep 1041: reward: -18.0\n",
      "\tep 1042: reward: -18.0\n",
      "\tep 1043: reward: -16.0\n",
      "\tep 1044: reward: -18.0\n",
      "\tep 1045: reward: -17.0\n",
      "\tep 1046: reward: -16.0\n",
      "\tep 1047: reward: -20.0\n",
      "\tep 1048: reward: -18.0\n",
      "\tep 1049: reward: -16.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-746251faf2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SAVED MODEL #{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1384\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1385\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mas_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         clear_devices=clear_devices)\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m   1639\u001b[0m       \u001b[0mexport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1642\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1034\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1035\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0mentry_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m       \u001b[0mencode_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mG:\\Program Files\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    694\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Solves Pong with Policy Gradients in Tensorflow.'''\n",
    "# written October 2016 by Sam Greydanus\n",
    "# inspired by karpathy's gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "n_obs = 80 * 80           # dimensionality of observations\n",
    "h = 200                   # number of hidden layer neurons\n",
    "n_actions = 3             # number of available actions\n",
    "learning_rate = 1e-3\n",
    "gamma = .99               # discount factor for reward\n",
    "decay = 0.99              # decay rate for RMSProp gradients\n",
    "save_path='models/pong.ckpt'\n",
    "\n",
    "# gamespace \n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs,rs,ys,ac = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "# initialize model\n",
    "tf_model = {}\n",
    "with tf.variable_scope('layer_one',reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_obs), dtype=tf.float32)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_obs, h], initializer=xavier_l1)\n",
    "with tf.variable_scope('layer_two',reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(h), dtype=tf.float32)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [h,n_actions], initializer=xavier_l2)\n",
    "\n",
    "# tf operations\n",
    "def tf_discount_rewards(tf_r): #tf_r ~ [game_steps,1]\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r\n",
    "\n",
    "def tf_policy_forward(x): #x ~ [1,D]\n",
    "    h = tf.matmul(x, tf_model['W1'])\n",
    "    h = tf.nn.relu(h)\n",
    "    logp = tf.matmul(h, tf_model['W2'])\n",
    "    p = tf.nn.softmax(logp)\n",
    "    return p\n",
    "\n",
    "# downsampling\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1    # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# tf placeholders\n",
    "tf_x = tf.placeholder(dtype=tf.float32, shape=[None, n_obs],name=\"tf_x\")\n",
    "tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"tf_y\")\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "\n",
    "# tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "# tf optimizer op\n",
    "tf_aprob = tf_policy_forward(tf_x)\n",
    "loss = tf.nn.l2_loss(tf_y-tf_aprob)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay)\n",
    "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_epr)\n",
    "train_op = optimizer.apply_gradients(tf_grads)\n",
    "\n",
    "# tf graph initialization\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "writer = tf.summary.FileWriter('./graph/', graph=tf.get_default_graph())\n",
    "\n",
    "# try load saved model\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print(\"no saved model to load. starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"loaded model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])\n",
    "\n",
    "\n",
    "# training loop\n",
    "while episode_number<=1000:\n",
    "#     if True: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(n_obs)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    feed = {tf_x: np.reshape(x, (1,-1))}\n",
    "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p=aprob)\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action+1)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # record game history\n",
    "    xs.append(x) ; ys.append(label) ; rs.append(reward); ac.append(action+1)\n",
    "    \n",
    "    if done:\n",
    "        # update running reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        # parameter update\n",
    "        feed = {tf_x: np.vstack(xs), tf_epr: np.vstack(rs), tf_y: np.vstack(ys)}\n",
    "        _ = sess.run(train_op,feed)\n",
    "#         print(sess.run(tf_aprob,feed))\n",
    "        \n",
    "#         plt.hist(np.array(ac))\n",
    "#         plt.show()\n",
    "        \n",
    "        # print progress console\n",
    "        if episode_number % 10 == 0:\n",
    "            print('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward))\n",
    "        else:\n",
    "            print('\\tep {}: reward: {}'.format(episode_number, reward_sum))\n",
    "        \n",
    "        # bookkeeping\n",
    "        xs,rs,ys,ac = [],[],[],[] # reset game history\n",
    "        episode_number += 1 # the Next Episode\n",
    "        observation = env.reset() # reset env\n",
    "        reward_sum = 0\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print(\"SAVED MODEL #{}\".format(episode_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
